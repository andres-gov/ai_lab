[
  {
    "id": "HHS-ONC-2026-0001-0002",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Onboard AI",
    "submitterType": "Organization",
    "date": "2026-01-08T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Introduction\n  - Onboard AI works with health systems and AI developers to evaluate, govern, and monitor AI tools in clinical settings\n  - Experience spans enterprise AI governance committees, clinical operations, quality and safety, legal and compliance, and information security teams\n  - Comments informed by direct operational experience with how AI tools are reviewed, approved, and monitored in healthcare organizations today\n  - Central observation: AI adoption is constrained less by technical capability and more by uncertainty about what constitutes reasonable governance, evaluation, and oversight\n    - Health systems are willing to adopt AI but lack confidence their internal processes will be viewed as sufficient and defensible by regulators, accrediting bodies, or courts\n  - HHS can materially accelerate adoption by reducing this uncertainty\n\n- Executive Summary (Summary Recommendation)\n  - HHS can most effectively accelerate AI adoption by clarifying a minimal, risk-proportionate governance baseline aligned with existing healthcare quality and safety practices\n  - This enables health systems to demonstrate reasonable evaluation, accountability, and monitoring of AI tools\n  - This approach would:\n    - Reduce uncertainty for providers and developers\n    - Improve defensibility of organizational decision-making\n    - Support consistent oversight without creating new certification regimes\n    - Require no new statutory authority or prescriptive regulation\n  - The goal is not to define best-in-class practices but to articulate the least the industry needs to demonstrate responsible AI use\n\n- General Comment on Regulation, Reimbursement, and Research & Development\n  - HHS's most effective near-term role is not to introduce new compliance frameworks but to clarify expectations that already exist implicitly across healthcare quality, safety, and accreditation paradigms\n  - Healthcare organizations are deeply familiar with demonstrating:\n    - Defined scope of use\n    - Fit-for-purpose evaluation\n    - Clear accountability\n    - Continuous monitoring and re-review\n  - Aligning AI expectations with these existing practices would enable faster adoption while preserving patient safety and public trust\n  - This approach would also benefit HHS by:\n    - Reducing variability in oversight\n    - Limiting hindsight-driven enforcement\n    - Enabling more consistent interpretation across federal and state actors without expanding regulatory burden\n\n- Responses to Specific Questions\n  - Question 1: Biggest barriers to private sector innovation and adoption of AI in clinical care\n    - Unclear governance expectations for non-medical device AI used in clinical workflows\n    - Inconsistent evaluation requirements across health systems\n    - Lack of a shared reference point for \"reasonable\" oversight\n    - Limited post-deployment monitoring infrastructure in many provider organizations\n    - These factors lead to prolonged review cycles, duplicative effort, and risk-averse decisions even for low-risk, high-value AI use cases\n    - AI developers are increasingly stagnated by 12+ month AI Committee review cycles added onto existing onerous AI procurement processes\n  - Question 2: Regulatory, payment policy, or programmatic changes HHS should prioritize\n    - HHS should prioritize clarifying a minimal, risk-proportionate governance baseline for AI used in clinical care, particularly for non-medical device AI\n    - Could be accomplished through guidance rather than rulemaking, requiring no new statutory authority or CFR changes\n    - At minimum, the baseline would clarify that responsible AI adoption includes four demonstrable elements:\n      - Documented intended use and clinical scope\n      - Fit-for-purpose pre-deployment evaluation\n      - Defined governance and accountability assignment\n      - Post-deployment monitoring and re-review triggers\n    - This baseline represents the least the industry needs to reduce uncertainty—not a comprehensive framework or certification\n    - Purpose is to provide a common reference point for reasonableness, not to impose uniform technical standards\n    - Clear federal articulation would help align state-level oversight and reduce fragmentation across jurisdictions\n  - Question 3: Novel legal and implementation issues for non-medical devices and HHS's role\n    - Key issues include:\n      - Ambiguity around accountability when AI influences but does not automate clinical decisions\n      - Inconsistent approaches to indemnification and responsibility allocation\n      - Unclear expectations for oversight of adaptive or evolving AI systems\n    - HHS can play a constructive role by clarifying expectations for reasonable governance\n      - Enables courts, regulators, and organizations to assess whether appropriate care was exercised without adjudicating technical model details\n  - Question 4: Most promising evaluation methods for non-medical devices and whether HHS should support them\n    - Pre-deployment approaches:\n      - Use-case-specific risk assessment\n      - Contextual performance and bias evaluation\n      - Documentation of known limitations tied to intended use\n    - Post-deployment approaches:\n      - Monitoring for performance drift, bias signals, and safety events\n        - Need definition of what minimum \"monitoring\" means (e.g., quarterly testing vs. continuous real-time monitoring)\n      - Defined triggers for reassessment (e.g., new version releases)\n      - Periodic governance review\n    - HHS support would be most impactful if focused on shared evaluation infrastructure, implementation science, and real-world monitoring approaches rather than prescriptive testing requirements\n  - Question 5: How HHS can best support private-sector accreditation, certification, and testing activities\n    - Encourage alignment with existing healthcare quality and safety governance models\n    - Avoid creation of a single federal AI certification\n    - Recognize transparent, risk-based third-party assurance activities\n    - This approach mirrors successful models in health IT and patient safety and preserves flexibility for innovation\n  - Question 6: Where AI tools have met or fallen short of expectations\n    - AI tools have met or exceeded expectations when:\n      - Use cases are narrowly defined\n      - Human oversight is preserved\n      - Performance is monitored over time\n    - AI tools have fallen short when:\n      - Deployed without local validation\n      - Treated as static products rather than managed systems\n      - Introduced without clear accountability structures\n    - High-value opportunities remain in workflow optimization, documentation support, triage assistance, and operational efficiency\n  - Question 7: Roles or governing bodies that influence AI adoption and main hurdles\n    - AI adoption decisions typically influenced by:\n      - Enterprise AI or clinical governance committees\n      - Clinical champions\n      - IT and security leadership\n      - Legal and risk management functions\n    - Primary hurdles include manual review processes, lack of institutional memory, and absence of standardized governance\n  - Question 8: Where enhanced interoperability would accelerate AI development\n    - Interoperability would be most impactful for:\n      - Standardized AI documentation and governance artifacts\n      - Evaluation and monitoring outputs\n      - Audit logs and review records\n      - Repeatable FHIR-based model testing requirements\n    - Extending interoperability beyond data exchange to include governance and assurance artifacts would meaningfully reduce duplication and speed responsible adoption\n  - Question 9: Challenges and concerns of patients and caregivers\n    - Patients and caregivers seek:\n      - Improved access and quality\n      - Reduced clinician burden\n      - Safer, more reliable care\n    - Primary concerns include opacity, bias, and loss of human oversight\n    - Visible, auditable governance and accountability mechanisms are essential to maintaining trust\n  - Question 10: Specific areas of AI research HHS should prioritize\n    - Research on governance, accountability, and monitoring models\n    - Longitudinal studies of AI performance and safety\n    - Economic analysis of AI-driven productivity and cost impacts\n      - Importantly including pilot/POC success rates, reasons for failures, and associated costs\n\n- Closing\n  - AI adoption in clinical care will accelerate when health systems can demonstrate clearly, consistently, and audibly that their governance, evaluation, and monitoring practices meet a reasonable and recognizable bar\n  - This approach aligns with how healthcare organizations already demonstrate readiness and accountability under established quality and safety oversight models\n    - Including accreditation-based frameworks that emphasize defined scope, organizational accountability, and continuous monitoring rather than one-time certification\n  - Applying these familiar principles to AI governance reduces uncertainty without introducing AI-specific exceptionalism\n  - By clarifying a minimal, accreditation-aligned governance baseline without creating new certification regimes or prescriptive technical standards, HHS can:\n    - Reduce uncertainty for providers and developers\n    - Improve defensibility of organizational decision-making\n    - Enable responsible innovation across the healthcare system while preserving public trust",
      "oneLineSummary": "AI governance company urges HHS to accelerate clinical AI adoption not through new regulations but by simply clarifying a minimal governance baseline that aligns with existing healthcare quality frameworks—giving health systems the confidence that their oversight practices will be viewed as reasonable.",
      "commenterProfile": "- **Name/Organization:** Onboard AI (submitted by Troy Bannister)\n- **Type:** Business\n- **Role/Expertise:** AI governance company that works with health systems and AI developers to evaluate, govern, and monitor AI tools in clinical settings; experience spans enterprise AI governance committees, clinical operations, quality/safety, legal/compliance, and information security\n- **Geographic Scope:** National\n- **Stake in Issue:** Directly involved in helping health systems navigate AI review and approval processes; sees firsthand how governance uncertainty slows adoption and creates duplicative work",
      "corePosition": "AI adoption in clinical care is held back not by technology but by uncertainty—health systems don't know what \"reasonable\" governance looks like, and they're afraid their processes won't hold up to scrutiny from regulators, accreditors, or courts. HHS can unlock adoption by clarifying a minimal, risk-proportionate governance baseline that aligns with existing healthcare quality and safety practices, without creating new certification regimes or prescriptive regulations.",
      "keyRecommendations": "- Clarify a minimal, risk-proportionate governance baseline for clinical AI through guidance (not rulemaking)\n  - Requires no new statutory authority or CFR changes\n  - Four demonstrable elements: documented intended use, fit-for-purpose pre-deployment evaluation, defined governance and accountability, post-deployment monitoring with re-review triggers\n  - Purpose is to establish a floor for reasonableness, not best-in-class practices\n- Align AI governance expectations with existing healthcare quality and safety paradigms\n  - Healthcare organizations already know how to demonstrate defined scope, fit-for-purpose evaluation, clear accountability, and continuous monitoring\n  - Avoid AI-specific exceptionalism\n- Avoid creating a single federal AI certification\n  - Instead, recognize transparent, risk-based third-party assurance activities\n  - Mirror successful models in health IT and patient safety\n- Extend interoperability standards beyond data exchange to include governance artifacts\n  - Standardized AI documentation and governance artifacts\n  - Evaluation and monitoring outputs\n  - Audit logs and review records\n  - Repeatable FHIR-based model testing requirements\n- Prioritize research on governance, accountability, and monitoring models\n  - Longitudinal studies of AI performance and safety\n  - Economic analysis including pilot/POC success rates, failure reasons, and costs\n- Define what minimum \"monitoring\" means in practice\n  - Clarify whether quarterly testing vs. continuous real-time monitoring is expected",
      "mainConcerns": "- Governance uncertainty is the primary barrier to AI adoption\n  - Health systems lack confidence their internal processes will be viewed as sufficient and defensible\n  - No shared reference point for what constitutes \"reasonable\" oversight\n- AI developers face 12+ month AI Committee review cycles on top of existing onerous procurement processes\n- Inconsistent evaluation requirements across health systems create duplicative effort\n- Limited post-deployment monitoring infrastructure in many provider organizations\n- Ambiguity around accountability when AI influences but doesn't automate clinical decisions\n- Inconsistent approaches to indemnification and responsibility allocation\n- Unclear expectations for oversight of adaptive or evolving AI systems\n- AI tools fail when:\n  - Deployed without local validation\n  - Treated as static products rather than managed systems\n  - Introduced without clear accountability structures\n- Patient concerns about opacity, bias, and loss of human oversight threaten trust",
      "notableExperiences": "- Counterintuitive framing: The problem isn't technology or even regulation—it's uncertainty about what's expected, which causes risk-averse paralysis even for low-risk, high-value AI use cases\n- Practical observation from working with health systems: AI developers are increasingly stagnated by 12+ month AI Committee review cycles being added onto existing procurement processes\n- Strategic insight: Healthcare organizations already know how to demonstrate readiness under accreditation frameworks—HHS should leverage this existing muscle memory rather than creating AI-specific requirements\n- Novel interoperability angle: Extending interoperability beyond data exchange to include governance and assurance artifacts would meaningfully reduce duplication across health systems\n- Research gap identified: Economic analysis should include pilot/POC success rates, reasons for failures, and associated costs—not just productivity gains",
      "keyQuotations": "- \"AI adoption in clinical care is constrained less by technical capability and more by uncertainty about what constitutes reasonable governance, evaluation, and oversight.\"\n- \"The goal is not to define best-in-class practices, but to articulate the least the industry needs to demonstrate responsible AI use.\"\n- \"Applying these familiar principles to AI governance reduces uncertainty without introducing AI-specific exceptionalism.\""
    },
    "themeScores": {
      "1": 1,
      "2": 1,
      "3": 1,
      "5": 1,
      "9": 1,
      "1.2": 1,
      "1.6": 1,
      "2.1": 1,
      "9.1": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "AI Governance"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Bias Testing"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Human-in-the-Loop"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Post-deployment Monitoring"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Pre-deployment Evaluation"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Trustworthy AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Algorithmic Bias"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Model Drift"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Accreditation"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Certification"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Triage"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Pilot Program"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Procurement"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "FHIR"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Patient Safety"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Health System"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Indemnification"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Trust"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "Implementation Science"
      }
    ],
    "hasAttachments": true,
    "wordCount": 1324,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0003",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Amir Abrams",
    "submitterType": "Individual",
    "date": "2026-01-08T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Executive Summary\n  - The RFI correctly identifies that clinical AI adoption is constrained less by model capability than by whether care settings can reliably supply the longitudinal, high-fidelity context needed for safe inference\n  - Core Proposal: Enable a one-time patient designation of a certified longitudinal endpoint to which full-fidelity USCDI data is automatically delivered as it is created, across care settings\n    - This is not a consumer portal strategy\n    - It is a convergence layer that makes complete longitudinal records reliably available back to care delivery systems (with consent) where AI actually runs\n  - Why This Matters for AI: Clinical AI fails when deployed into informationally impoverished workflows\n    - Patient-designated routing changes the default so that longitudinal completeness becomes the expected input condition for AI, rather than a special-case integration achievement\n  - Key Recommendations:\n    - Regulation: Define and certify patient-designated longitudinal endpoints with strict security, consent, provenance, and audit requirements\n    - Reimbursement: Pilot payment models tied to verified delivery rather than capability attestations\n    - R&D: Fund evaluations that connect longitudinal completeness improvements to clinical outcomes, workflow impact, and AI safety/bias monitoring\n  - This is a narrow structural change with outsized impact: it improves care coordination immediately, reduces preventable harm, and creates the data conditions necessary for safe, effective, and equitable clinical AI\n\n- Introduction: Why I'm Commenting / Relevant Experience\n  - I am submitting this comment because the RFI correctly identifies a core problem: clinical AI adoption is constrained less by model capability than by whether the care setting can reliably supply the longitudinal, high-fidelity context needed for safe and clinically useful inference\n  - My perspective comes from building and operating clinical data infrastructure in real healthcare delivery environments\n    - Clinical data warehouses, integration pipelines, and point-of-care systems used by clinicians under real time constraints\n  - In those settings, the central failure mode is not \"no standards exist,\" but \"the record is incomplete when and where clinical decisions are made\"\n    - AI embedded into workflows inherits that incompleteness, which directly degrades safety, quality, and equity\n\n- Background: The Problem the RFI is Trying to Solve\n  - Fragmentation, not standards maturity, is the binding constraint\n    - Despite FHIR/USCDI maturity, national exchange efforts, and the shift toward API-based access, longitudinal aggregation remains the exception rather than the norm\n    - The operational reality is that records are frequently split across hospitals, specialist groups, labs, urgent care, imaging centers, behavioral health providers, and post-acute settings\n    - Clinicians often have an incomplete view of diagnoses, medications, prior workups, allergies, procedures, problem lists, and results performed elsewhere\n    - For clinical AI, this is not a \"nice-to-have\"\n    - Longitudinal completeness is the substrate for:\n      - Accurate clinical context and risk stratification\n      - Reduction of false positives/false negatives that arise from missing history\n      - Bias mitigation (because missingness is not random and often correlates with socioeconomic access patterns)\n      - Trustworthy post-deployment monitoring (drift, subgroup performance, safety event correlation)\n  - Why fragmentation persists: routing control is mis-assigned\n    - The underlying structural issue is that routing decisions are typically controlled by institutions and vendors, not by patients\n    - Even when standards and exchange exist, the default routing behavior tends to produce episodic exchange, partial aggregation, and brittle \"islands\" of data that do not converge into a reliable longitudinal record accessible at the point of care\n    - Interoperability capability does not guarantee longitudinal completeness, because the system's routing defaults do not reliably cause data to converge\n\n- Core Proposal: Patient-Designated Longitudinal Routing by Default\n  - One structural change\n    - HHS can unlock meaningful clinical AI adoption by enabling patient-designated routing by default\n    - A one-time patient designation of a certified longitudinal endpoint to which full-fidelity USCDI data is automatically delivered as it is created, across care settings\n    - Key properties:\n      - One-time designation (like choosing a primary pharmacy or selecting a health plan PCP)\n      - Automatic delivery of standardized clinical payloads (USCDI-aligned) as events occur\n      - Persistent routing via a record-locator mechanism so the designation \"follows\" the patient\n      - No patient management burden (no requirement that patients log in, curate, or actively manage data)\n      - Return-to-care enablement: treating systems can retrieve a current longitudinal view with patient consent, supporting clinician workflow and embedded AI\n  - What this is not\n    - This is not \"a consumer portal strategy\" and not a burden-shift to patients\n    - The endpoint is not primarily for human browsing or personal recordkeeping\n    - It is a convergence layer intended to make a complete, current longitudinal record reliably available back to care delivery systems (with consent) where AI actually runs\n  - Why this solves the AI adoption bottleneck\n    - Clinical AI fails in practice when deployed into workflows that are informationally impoverished\n    - Patient-designated routing changes the default so that longitudinal completeness becomes the expected input condition for AI, rather than a special-case integration achievement\n    - This is the smallest structural shift that:\n      - Improves patient safety and care coordination immediately (even without AI)\n      - Creates the precondition for AI systems to be reliable and monitorable\n      - Aligns incentives in a way enforcement alone has not achieved\n\n- Analysis: Impacts, Unintended Consequences, and Why HHS Should Prefer This Approach\n  - Benefits to clinical care (even before AI)\n    - A persistent longitudinal record reduces:\n      - Duplicated labs and imaging ordered because prior results are unavailable\n      - Medication reconciliation errors (a major source of preventable harm)\n      - Delays in diagnosis due to missing prior workup and specialist notes\n      - Avoidable admissions/readmissions driven by incomplete outpatient history\n      - Clinician time lost hunting for records across portals and faxes\n    - These improvements are direct clinical quality benefits that also improve AI readiness\n  - Why incentives matter more than capability attestations\n    - Interoperability has often been framed as \"build capability\" + \"enforce compliance\"\n    - That approach does not reliably create longitudinal completeness because it does not make completeness an operational default that organizations are paid to produce\n    - If reimbursement is conditioned on verified delivery to a patient-designated endpoint (not mere \"capability to deliver\"), interoperability shifts from an enforcement problem to an operational norm\n    - This is a durable behavioral change lever\n  - Distributional impacts (equity)\n    - Fragmentation and missingness are not evenly distributed\n    - People who receive care across multiple systems (often due to access patterns, insurance churn, transportation constraints, or regional provider availability) are disproportionately harmed by non-convergent records\n    - These same populations are at higher risk of AI underperformance when missingness becomes a hidden input variable correlated with disadvantage\n    - Patient-designated routing improves equity by making longitudinal completeness a default expectation across sites rather than a privilege of being \"in the right health system\"\n  - Semantic interoperability is a real risk—and a necessary forcing function\n    - Implementing default longitudinal routing will reveal known weaknesses in semantic consistency: coding variation, normalization drift, incomplete reconciliation of problem lists, and differing local interpretations\n    - These issues exist today; episodic exchange often masks them\n    - This should be treated as a forcing function rather than a reason to avoid reform\n    - AI systems amplify semantic inconsistency risks; therefore, operationalizing longitudinal aggregation is the mechanism that makes semantic quality measurable, improvable, and tied to downstream usability\n  - Privacy, security, and trust: the \"central objection\" and how to mitigate it\n    - A convergence endpoint increases the perceived \"blast radius\" of a breach if designed poorly\n    - That concern is legitimate and should be addressed explicitly in the program design\n    - Mitigations that can be required through certification and policy guardrails:\n      - Consent + revocation: simple, durable patient consent workflows; immediate revocation capability\n      - Strong provenance + auditability: comprehensive audit logs; patient-accessible \"who accessed what and when\"\n      - Minimum necessary retrieval: retrieval into treating systems should be scoped; discourage uncontrolled bulk retrieval except when clinically justified\n      - Segmentation & sensitive domains: phased inclusion for highly sensitive categories; require clear labeling and provenance for sensitive information\n      - Security baseline: modern authentication/authorization, encryption in transit/at rest, continuous monitoring, and incident response controls commensurate with handling longitudinal clinical data\n    - A default routing policy must be paired with a certification program that makes \"safe aggregation\" non-negotiable\n\n- Recommended HHS Actions Using the RFI Levers\n  - Regulation: Define and certify \"longitudinal endpoints\" and routing behavior\n    - HHS/ASTP/ONC should define certification criteria for longitudinal endpoints and for EHR behaviors required to support patient designation and delivery\n    - Certification should require longitudinal endpoints to:\n      - Accept complete USCDI-aligned payloads (full-fidelity, not lossy summaries)\n      - Support standardized query and return to treating systems with patient consent\n      - Provide robust provenance, versioning, and audit logs\n      - Enforce strong security controls and clear operational responsibilities\n      - Implement consent and revocation mechanisms appropriate for clinical workflows\n      - Support resilience and availability expectations appropriate for point-of-care retrieval\n    - Certification should require sending systems to:\n      - Support a patient-designated routing instruction\n      - Automatically deliver standardized payloads to that endpoint (subject to consent and policy)\n      - Support reliable event-based update flows (not just occasional exports)\n      - Provide delivery verification (receipt + integrity checks)\n    - This does not require redefining USCDI or replacing FHIR; it changes the default routing behavior and makes longitudinal convergence reliable\n  - Persistent routing: record locator mechanism\n    - HHS should establish or enable a persistent routing pointer (record-locator-like) so a patient's designation is discoverable across care settings\n    - That pointer should be patient-controlled, privacy-preserving, and designed to avoid enabling unauthorized tracking\n    - Conceptually: the routing pointer says \"where the longitudinal record lives,\" not \"what is in it\"\n    - Access to the content remains consent-governed and auditable\n  - Reimbursement: tie payment to verified delivery and usability (critical)\n    - Regulation alone will not produce durable change\n    - HHS should pilot reimbursement models where the measure is successful delivery (and, over time, usability) rather than capability attestation\n    - Potential models:\n      - Infrastructure payments for certified longitudinal endpoints with pricing guardrails\n      - Quality / value-based measures tied to verified delivery performance (completeness + latency)\n      - Incentives for treating systems that demonstrate retrieval use in workflows (without penalizing patients)\n    - CMS can directly influence Medicare/Medicaid\n    - Commercial alignment can be approached via demonstrations that show ROI and reduced waste through fewer duplicates and improved coordination\n  - R&D and pilots: fund implementation science, not just model research\n    - HHS should sponsor pilots that evaluate:\n      - Completeness: USCDI coverage over time vs expected sources\n      - Latency: time from data creation to endpoint availability\n      - Clinical outcomes proxies: duplicate test reduction, medication discrepancy reduction, preventable adverse events\n      - Operational burden: net clinician time saved vs added workflows\n      - AI safety and bias: subgroup performance changes when longitudinal completeness improves\n      - Total cost of ownership: endpoint ops, integration costs, and security compliance\n    - Study design suggestion: stepped-wedge rollout across sites or regions to enable causal inference without requiring all participants to switch simultaneously\n\n- Addressing Likely Counterarguments\n  - Counterargument A: \"This is too hard / too expensive\"\n    - The current model externalizes costs into duplicated testing, preventable errors, and administrative burden (fax, portal chasing, manual reconciliation)\n    - Patient-designated routing consolidates infrastructure investment into a measurable, reusable utility\n    - By tying reimbursement to verified delivery and using certification to reduce bespoke integrations, the system becomes cheaper over time than the current patchwork of one-off interfaces\n  - Counterargument B: \"Patients won't participate\"\n    - Patients already make durable healthcare designations (insurance plans, pharmacies, PCPs, consent forms)\n    - The program can be designed so designation occurs at registration, enrollment, or during standard administrative encounters, with minimal friction\n    - Critically: patients do not need to \"use an app\" for this to work\n  - Counterargument C: \"Privacy risk is unacceptable\"\n    - The risk is unacceptable only if the endpoint lacks rigorous controls\n    - This is precisely why certification, auditability, consent, and security baselines must be central requirements\n    - A poorly secured fragmented environment is not inherently safer than a well-governed convergence model; it is simply less visible and less accountable\n\n- Specific Requests to HHS (Recap as Actionable Items)\n  - Define and certify patient-designated longitudinal endpoints with strict security, consent, provenance, and audit requirements\n  - Require support in certified health IT for patient designation and automated delivery of standardized USCDI-aligned data to the designated endpoint\n  - Establish a persistent routing pointer (record-locator-like) so designations follow patients across care settings\n  - Pilot reimbursement models tied to verified delivery (and later, usability) rather than capability attestations\n  - Fund evaluations that connect longitudinal completeness improvements to clinical outcomes, workflow impact, and AI safety/bias monitoring\n\n- Conclusion\n  - Clinical AI cannot reliably succeed without longitudinal completeness available in real workflows\n  - The most direct way to achieve that completeness is to transfer routing control—by default—from institutions to patients through a one-time designation of a certified longitudinal endpoint, paired with persistent routing and strong consent/audit safeguards\n  - This is a narrow structural change with outsized impact: it improves care coordination immediately, reduces preventable harm, and creates the data conditions necessary for safe, effective, and equitable clinical AI\n\n---",
      "oneLineSummary": "A medical informatics architect and former LSU Health director argues that clinical AI fails not from lack of capability but from incomplete patient records, proposing a \"patient-designated longitudinal endpoint\" system where patients make a one-time choice to route all their health data to a certified hub that treating providers can access—fixing both care coordination and AI reliability in one structural change.\n\n---",
      "commenterProfile": "- **Name/Organization:** Amir Abrams\n- **Type:** Individual (with significant professional expertise)\n- **Role/Expertise:** Software Architect; former Director of Medical Informatics at LSU Health Sciences Center; experience building clinical data warehouses, integration pipelines, and point-of-care systems\n- **Geographic Scope:** National\n- **Stake in Issue:** Professional expertise in clinical data infrastructure; has directly observed how incomplete records degrade AI safety, quality, and equity in real healthcare delivery environments\n\n---",
      "corePosition": "Clinical AI fails in real care settings not because models lack capability, but because they're deployed into workflows where longitudinal patient context is incomplete or unavailable. The solution is a narrow structural change: let patients make a one-time designation of a certified endpoint where all their health data automatically converges, then let treating providers retrieve that complete record with consent. This fixes care coordination immediately and creates the data conditions clinical AI needs to be safe, effective, and equitable.\n\n---",
      "keyRecommendations": "- Define and certify patient-designated longitudinal endpoints\n  - Require acceptance of complete USCDI-aligned payloads (full-fidelity, not lossy summaries)\n  - Mandate standardized query and return to treating systems with patient consent\n  - Require robust provenance, versioning, and audit logs\n  - Enforce strong security controls, consent/revocation mechanisms, and operational responsibilities\n  - Support resilience and availability appropriate for point-of-care retrieval\n\n- Require certified health IT to support patient designation and automated delivery\n  - Support a patient-designated routing instruction\n  - Automatically deliver standardized payloads to that endpoint as events occur\n  - Support reliable event-based update flows (not just occasional exports)\n  - Provide delivery verification (receipt + integrity checks)\n\n- Establish a persistent routing pointer (record-locator-like)\n  - Make patient designations discoverable across care settings\n  - Keep it patient-controlled and privacy-preserving\n  - Design to avoid enabling unauthorized tracking\n  - The pointer says \"where the longitudinal record lives,\" not \"what is in it\"\n\n- Pilot reimbursement models tied to verified delivery rather than capability attestations\n  - Infrastructure payments for certified longitudinal endpoints with pricing guardrails\n  - Quality/value-based measures tied to verified delivery performance (completeness + latency)\n  - Incentives for treating systems that demonstrate retrieval use in workflows\n\n- Fund implementation science evaluations connecting longitudinal completeness to outcomes\n  - Measure USCDI coverage over time vs expected sources\n  - Track latency from data creation to endpoint availability\n  - Assess clinical outcomes proxies: duplicate test reduction, medication discrepancy reduction, preventable adverse events\n  - Monitor AI safety and bias: subgroup performance changes when longitudinal completeness improves\n  - Use stepped-wedge rollout across sites/regions to enable causal inference\n\n---",
      "mainConcerns": "- Clinical AI inherits the incompleteness of fragmented records\n  - Missing history drives false positives/negatives\n  - Missingness is not random—it correlates with socioeconomic access patterns, worsening bias\n  - AI systems amplify semantic inconsistency risks\n\n- Routing control is mis-assigned to institutions and vendors, not patients\n  - Default routing behavior produces episodic exchange, partial aggregation, and brittle \"islands\" of data\n  - Interoperability capability does not guarantee longitudinal completeness\n\n- Current \"build capability + enforce compliance\" approach doesn't work\n  - Does not make completeness an operational default that organizations are paid to produce\n  - Capability attestations are insufficient—need verified delivery\n\n- Fragmentation causes direct clinical harm even without AI\n  - Duplicated labs and imaging because prior results unavailable\n  - Medication reconciliation errors (major source of preventable harm)\n  - Delays in diagnosis due to missing prior workup\n  - Avoidable admissions/readmissions from incomplete outpatient history\n  - Clinician time lost hunting for records across portals and faxes\n\n- Privacy and security risks of convergence endpoints\n  - Increased \"blast radius\" of a breach if designed poorly\n  - Must be addressed through certification, auditability, consent, and security baselines\n  - A poorly secured fragmented environment is not inherently safer—just less visible and less accountable\n\n- Equity impacts of fragmentation\n  - People receiving care across multiple systems (due to access patterns, insurance churn, transportation constraints) are disproportionately harmed\n  - These populations face higher risk of AI underperformance when missingness becomes a hidden input variable correlated with disadvantage\n\n---",
      "notableExperiences": "- Reframes the interoperability problem as a routing control problem\n  - The central failure mode is not \"no standards exist\" but \"the record is incomplete when and where clinical decisions are made\"\n  - Standards maturity (FHIR/USCDI) is not the binding constraint—fragmentation is\n\n- Proposes treating semantic inconsistency exposure as a feature, not a bug\n  - Implementing longitudinal routing will reveal coding variation, normalization drift, and reconciliation problems\n  - Episodic exchange currently masks these issues\n  - Making them visible is the mechanism to make semantic quality measurable and improvable\n\n- Draws analogy to existing patient designations\n  - Patients already make durable healthcare designations (insurance plans, pharmacies, PCPs)\n  - This can work the same way—designation at registration/enrollment with minimal friction\n  - Critically: patients don't need to \"use an app\" for this to work\n\n- Identifies that current costs are externalized and hidden\n  - Duplicated testing, preventable errors, and administrative burden (fax, portal chasing, manual reconciliation) are the real costs of fragmentation\n  - Patient-designated routing consolidates infrastructure investment into a measurable, reusable utility\n\n---",
      "keyQuotations": "- \"Clinical AI fails when deployed into informationally impoverished workflows.\"\n\n- \"The central failure mode is not 'no standards exist,' but 'the record is incomplete when and where clinical decisions are made.'\"\n\n- \"A poorly secured fragmented environment is not inherently safer than a well-governed convergence model; it is simply less visible and less accountable.\"\n\n- \"Patient-designated routing improves equity by making longitudinal completeness a default expectation across sites rather than a privilege of being 'in the right health system.'\""
    },
    "themeScores": {
      "5": 1,
      "8": 1,
      "4.6": 1,
      "5.1": 1,
      "5.2": 1,
      "5.3": 1,
      "5.4": 1,
      "7.3": 1,
      "8.1": 1,
      "9.3": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "Post-deployment Monitoring"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Algorithmic Bias"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Model Drift"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Certification"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Care Coordination"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Diagnosis"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Informed Consent"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Risk Stratification"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Behavioral Health"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Provenance"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Encryption"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "ONC"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Waste"
      },
      {
        "category": "Health Information Systems",
        "label": "EHR"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Medical Coding"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Administrative Burden"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Pilot Program"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "ROI"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "FHIR"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "USCDI"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicaid"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Adverse Event"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Equity"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Patient Safety"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Ambulatory Care"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Health System"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "Implementation Science"
      }
    ],
    "hasAttachments": true,
    "wordCount": 2313,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0004",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Mitchell Berger",
    "submitterType": "Individual",
    "date": "2026-01-08T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Response to HHS RFI on Accelerating the Adoption and Use of AI in Clinical Care\n  - Writing in private capacity; views are author's alone, not those of any other individuals or public/private entities\n\n- Central recommendation: HHS should further emphasize ethical aspects of AI in clinical care work\n  - Involve HHS and other federal bioethics staff and advisory committees\n  - Collaborate with non-federal ethics and bioethics centers, departments, and institutes\n  - Potentially support restoration of a Presidential-Level commission on bioethics to discuss AI and other topics\n\n- Definition of AI ethics\n  - \"A set of values, principles, and techniques that employ widely accepted standards of right and wrong to guide moral conduct in the development and use of AI technologies\" (citing Leslie D., Alan Turing Institute, 2019)\n\n- Ethical issues that may arise with AI in clinical care\n  - Data ownership, privacy, data sharing, and potential (re)identification of individual patients\n  - Potential use of biased tools and algorithms\n  - Autonomous AI systems\n  - Governance and oversight of AI systems\n  - Access to AI benefits\n    - For US-based populations, researchers, and students\n    - For international/lower-middle income nation populations\n  - Data and AI tool/system ownership questions\n  - How AI use will be acknowledged\n    - Authorship considerations\n    - Obtaining patient informed consent\n  - Billing and reimbursement for AI tools/services\n  - Impacts of AI on jobs and provider-patient relationships\n  - Long-term evaluation of AI impacts\n  - Potential dual uses/misuses of AI systems\n\n- Quality concerns with AI systems raise ethical issues\n  - \"AI is only as good as the data you feed into it\"\n  - Certain clinical applications may require additional scrutiny\n    - Behavioral health\n    - Pediatrics\n\n- Positive potential of AI acknowledged\n  - May promote overall access to care\n  - May enhance quality of care\n  - May improve patient outcomes\n  - May produce other just and positive outcomes\n\n- National Artificial Intelligence Research Resource (NAIRR) precedent\n  - Now under National Science Foundation auspices\n  - Emphasized ethical issues in 2022 interim report and 2023 final report\n  - Called for creation of ethics advisory board to guide AI research\n  - Interim report: \"To earn and maintain public trust, research in areas that may impact privacy, civil rights, or civil liberties will need to be reviewed, approved, and performed in a way that meets the expectations of civil society and protects subjects' rights\"\n  - Final report recommended ethics advisory board to advise on \"issues of ethics, fairness, bias, accessibility, and AI risks and blindspots\"\n\n- Specific recommendations for HHS to support ethical AI programs\n  - Include NIH's bioethics staff and others with expertise\n  - Use advisory committees to discuss and obtain input on ethical and moral implications of AI\n  - Collaborate with academic and nonprofit ethics and bioethics centers, institutes, and departments\n\n- Broader ecosystem recommendation\n  - In ideal world, private companies, academic institutions, faith-based entities, and other nonprofits would implement their own ethical programs and advisory boards focused on AI and clinical care\n\n- Presidential Bioethics Commission recommendation\n  - HHS could support restoration of this commission\n  - Historically existed prior to 2017 under several administrations\n  - Past topics researched and addressed:\n    - Access to care\n    - Cloning\n    - Whole genome sequencing\n    - Neuroscience\n    - Synthetic biology\n    - Newborn screening\n    - Clinical research\n    - Pandemics/emergency preparedness\n    - Aging/caregiving\n  - With adequate staff and funding, would be well-positioned to examine ethical aspects of AI in clinical care\n\n- Extensive citations provided\n  - Academic literature on AI ethics\n  - Council of Europe resources on ethical challenges in AI\n  - Articles on AI in low-income countries\n  - NAIRR reports and documentation\n  - NIH Department of Bioethics\n  - Directories of ethics centers and programs\n  - Calls from National Academies and others to reinstate Presidential bioethics commission\n\n---",
      "oneLineSummary": "A private citizen with bioethics knowledge urges HHS to strengthen ethical oversight of clinical AI through federal bioethics staff, academic collaboration, and restoration of a Presidential Bioethics Commission.\n\n---",
      "commenterProfile": "- **Name/Organization:** Mitchell Berger (writing in private capacity)\n- **Type:** Individual\n- **Role/Expertise:** Knowledgeable about bioethics policy, federal advisory structures, and AI ethics literature; familiar with NAIRR, NIH bioethics resources, and history of Presidential bioethics commissions\n- **Geographic Scope:** National\n- **Stake in Issue:** Concerned citizen interested in ensuring ethical frameworks guide AI adoption in healthcare\n\n---",
      "corePosition": "I believe HHS should place greater emphasis on the ethical dimensions of AI in clinical care. This means actively engaging federal bioethics expertise, partnering with academic and nonprofit ethics organizations, and potentially advocating for restoration of a Presidential-level bioethics commission that could provide high-level guidance on AI and other emerging issues.\n\n---",
      "keyRecommendations": "- Involve HHS and other federal bioethics staff and advisory committees in AI clinical care work\n  - Include NIH's Department of Bioethics staff\n  - Use existing advisory committees to discuss ethical and moral implications\n\n- Collaborate with non-federal ethics and bioethics organizations\n  - Academic ethics centers, institutes, and departments\n  - Nonprofit bioethics organizations\n\n- Support restoration of a Presidential Bioethics Commission\n  - Would examine and make recommendations on ethical aspects of AI in clinical care\n  - Requires adequate staff and funding\n  - Could address AI alongside other emerging bioethics topics\n\n- Follow NAIRR's model of creating an ethics advisory board\n  - Provide advice on ethics, fairness, bias, accessibility, and AI risks and blindspots\n\n- Encourage private sector and other institutions to implement their own ethical programs\n  - Private companies, academic institutions, faith-based entities, and nonprofits should develop AI ethics advisory boards\n\n---",
      "mainConcerns": "- Multiple ethical issues arise with AI in clinical care\n  - Data ownership, privacy, and potential patient re-identification\n  - Biased tools and algorithms\n  - Governance and oversight gaps\n  - Inequitable access to AI benefits (domestically and internationally)\n  - Unclear acknowledgment practices (authorship, informed consent)\n  - Billing and reimbursement uncertainties\n  - Impacts on jobs and provider-patient relationships\n  - Long-term evaluation needs\n  - Dual use and misuse potential\n\n- AI quality concerns have ethical dimensions\n  - AI outputs depend on data quality\n  - Certain applications (behavioral health, pediatrics) need additional scrutiny\n\n- Public trust requires ethical review processes\n  - Research impacting privacy, civil rights, or civil liberties needs proper review and approval\n\n- Presidential Bioethics Commission has been absent since 2017\n  - No high-level federal body currently examining these issues\n\n---",
      "notableExperiences": "- Draws parallel to NAIRR's approach as a model for HHS\n  - NAIRR's 2022-2023 reports explicitly called for ethics advisory board creation\n  - Demonstrates precedent for embedding ethics oversight in federal AI initiatives\n\n- Provides historical context on Presidential bioethics commissions\n  - Operated under multiple administrations before 2017\n  - Addressed diverse topics from cloning to pandemic preparedness\n  - Suggests this existing infrastructure could be revived for AI ethics\n\n- Acknowledges AI's positive potential while emphasizing ethical guardrails\n  - Balanced perspective that doesn't oppose AI adoption but seeks responsible implementation\n\n---",
      "keyQuotations": "- \"AI ethics is a set of values, principles, and techniques that employ widely accepted standards of right and wrong to guide moral conduct in the development and use of AI technologies.\"\n\n- \"To earn and maintain public trust, research in areas that may impact privacy, civil rights, or civil liberties will need to be reviewed, approved, and performed in a way that meets the expectations of civil society and protects subjects' rights.\" (quoting NAIRR interim report)\n\n- \"AI is only as good as the data you feed into it.\""
    },
    "themeScores": {
      "8": 1,
      "1.7": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "Trustworthy AI"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Informed Consent"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Screening"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "NIH"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "NSF"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Trust"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "NAIRR"
      }
    ],
    "hasAttachments": true,
    "wordCount": 1191,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0005",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Health AI Institute",
    "submitterType": "Organization",
    "date": "2026-01-08T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Cover Note\n  - Health AI Institute submits comment in response to HHS Request for Information on Accelerating AI Adoption in Clinical Care\n  - Comments informed by applied experience evaluating and implementing AI in clinical settings\n  - Experience includes convening stakeholders across health care ecosystem\n  - Leadership spans federal health services research, clinical informatics, health system operations, and AI technology development\n  - Focus on system-level actions HHS can take to reduce adoption barriers, align incentives, and support evidence-to-practice translation while maintaining patient safety and public trust\n\n- I. Perspective and Context\n  - Health AI Institute supports HHS's goal of enabling innovation while protecting patients, civil rights, and public trust\n  - Perspective grounded in applied, real-world experience\n  - Organization engages across the health AI lifecycle\n    - Evaluation of AI tools in clinical environments\n    - Convening health systems, payers, developers, patients, and regulators\n    - Advising organizations on governance and deployment\n  - Leadership and affiliates include former federal health services research leadership (including AHRQ), CMIOs, AI technologists, and health system executives responsible for implementation at scale\n  - This vantage point allows observation of not only whether AI works, but why adoption succeeds or fails in practice\n  - Consistent finding: AI adoption is constrained less by technical capability than by misalignment among regulation, reimbursement, evidence generation, data infrastructure, and clinical workflows\n\n- II. Cross-Cutting Barriers to AI Adoption\n  - A. Regulatory uncertainty outside FDA-regulated medical devices\n    - FDA has clarified pathways for AI-enabled medical devices\n    - Growing share of clinical AI falls outside traditional device definitions\n      - Documentation support\n      - Care coordination\n      - Operational decision support\n      - Generative AI embedded in workflows\n    - Legal and compliance teams frequently delay or block deployment not due to known safety risks, but due to uncertainty around accountability, liability, and post-deployment expectations\n    - This uncertainty disproportionately affects smaller and resource-constrained organizations\n  - B. Misaligned payment incentives\n    - AI often delivers value through cognitive efficiency, prevention, administrative burden reduction, and care coordination\n    - These outcomes are weakly rewarded under fee-for-service payment\n    - Organizations investing in AI are not those capturing its financial benefits\n    - This undermines sustainability even when clinical value is evident\n  - C. Evidence gaps focused on deployment, not accuracy\n    - Primary evidence gap is no longer whether AI models can perform discrete tasks\n    - Gap is whether they perform reliably, equitably, and safely over time and across settings\n    - Health care organizations struggle with:\n      - Post-deployment monitoring\n      - Bias and drift detection\n      - Workflow integration\n      - Economic evaluation\n  - D. Interoperability that is not AI-ready\n    - Despite progress in data exchange, clinical data often lacks:\n      - Provenance\n      - Context\n      - Labeling\n      - Longitudinal linkage required for scalable AI evaluation and deployment\n    - Interoperability that enables data movement but not data meaning limits AI impact\n  - E. Patient trust and transparency\n    - Patients and caregivers express concerns about:\n      - Transparency\n      - Escalation when AI is wrong\n      - Potential inequities\n    - Addressing these concerns is essential to durable adoption\n\n- III. Strategic Recommendations for HHS\n  - A. Regulation: prioritize clarity and learning (ASTP/ONC, FDA)\n    - Emphasize regulatory clarity and predictability, particularly for non-device AI\n    - Publish cross-agency governance principles\n    - Clarify post-deployment expectations\n    - Support time-limited learning environments or safe harbors for well-scoped AI use cases\n    - Reducing uncertainty will accelerate adoption while preserving patient protections\n  - B. Reimbursement: align payment with value (CMS)\n    - Modernize payment policies to recognize AI as enabling clinical infrastructure when it demonstrably improves quality, access, or efficiency\n    - High-value applications often generate value through predictive insights rather than encounters\n      - AI-enabled telehealth\n      - Remote monitoring\n      - Wearables for chronic disease management\n    - Allow AI costs within value-based models\n    - Support outcome-linked payment pilots\n    - Enable temporary reimbursement pathways\n  - C. Research & development: refocus on evidence-to-practice translation\n    - Rebalance AI research toward real-world implementation and learning\n    - Priorities include:\n      - Post-deployment performance monitoring\n      - Workflow integration\n      - Organizational governance\n      - Economic impact analysis\n    - This learning-oriented approach reflects the legacy of federal health services research and learning health systems\n  - D. Interoperable, AI-Ready Data Infrastructure (ASTP/ONC)\n    - Accelerating AI adoption requires interoperable data infrastructure beyond exchange alone\n    - AI depends on longitudinal, context-rich data with consistent standards, provenance, and governance across care settings\n    - Includes patient-generated data\n    - HHS can catalyze progress by aligning interoperability policy with real-world AI use cases\n    - Support shared benchmarking and evaluation infrastructure\n  - E. Evaluation and trust infrastructure\n    - Rather than certifying individual tools, support shared evaluation infrastructure\n    - Support independent testing ecosystems\n    - Support accreditation aligned with federal principles\n    - This promotes accountability without constraining innovation\n\n- IV. Illustrative Learning-Oriented Federal Actions\n  - Non-prescriptive, learning-oriented approaches to reduce risk and accelerate AI adoption by aligning regulatory, payment, and evidence-generation levers\n  - 1. National Clinical AI Sandbox Program\n    - Time-limited program enabling real-world deployment under defined regulatory and reimbursement flexibility\n    - Paired with structured evaluation to generate evidence on safety, effectiveness, equity, and workflow integration\n    - Primary beneficiaries: clinicians, health systems, regulators\n  - 2. AI Evaluation Commons\n    - Shared national resource supporting:\n      - Standardized benchmarks\n      - AI-relevant data standards\n      - Post-deployment performance metrics\n      - Failure-mode reporting\n    - Reduces duplication across health systems\n    - Enables more transparent, comparable AI evaluation\n    - Primary beneficiaries: health systems, developers, researchers\n  - 3. Temporary AI Payment Pathways\n    - Time-limited reimbursement pathways for high-evidence AI tools across care settings within value-based care models\n    - Allows real-world performance, equity, and cost impacts to be evaluated prior to permanent coverage decisions\n    - Primary beneficiaries: patients, clinicians, value-based care organizations\n  - 4. Organizational AI Readiness Grants\n    - Targeted support for health care organizations to build:\n      - Governance structures\n      - Clinician training\n      - Change management capacity\n      - Human–AI workflow integration\n    - Primary beneficiaries: clinicians, health system leadership\n  - 5. Patient Trust and Transparency Initiative\n    - Development of patient-centered norms and tools for:\n      - Disclosure\n      - Consent\n      - Escalation\n      - Communication when AI is used in clinical care\n    - Addresses patient concerns while strengthening public trust and accountability\n    - Primary beneficiaries: patients and caregivers\n\n- V. Conclusion\n  - AI can improve outcomes, reduce burden, and enhance productivity across the health care system\n  - Achieving this potential requires coordinated federal action aligning regulation, reimbursement, research, and trust infrastructure with the realities of clinical care\n\n- About the Health AI Institute\n  - Provides nonpartisan, nonpolitical platform for collaboration between business leaders, technology experts, industry partners, and policymakers\n  - Mission: foster interdisciplinary collaboration, drive AI-powered innovation, shape the future of medicine\n  - Vision: AI-enabled technologies enhance patient care, improve efficiency, and promote health across the U.S. and beyond\n\n---",
      "oneLineSummary": "A health AI think tank with deep implementation experience argues that AI adoption is blocked less by technology than by misaligned regulations, payment models, and evidence infrastructure, and proposes five concrete federal programs including a national AI sandbox and temporary payment pathways.\n\n---",
      "commenterProfile": "- **Name/Organization:** Health AI Institute\n- **Type:** Academic/Research (Think Tank/Convening Organization)\n- **Role/Expertise:** Applied AI evaluation and implementation; leadership includes former AHRQ leadership, CMIOs, AI technologists, and health system executives with implementation-at-scale experience\n- **Geographic Scope:** National\n- **Stake in Issue:** Advises health systems, payers, developers, and regulators on AI governance and deployment; directly observes adoption barriers across the health AI lifecycle\n\n---",
      "corePosition": "We consistently find that AI adoption is constrained less by AI's technical capability than by misalignment among regulation, reimbursement, evidence generation, data infrastructure, and clinical workflows. HHS should take coordinated action across these domains—providing regulatory clarity for non-device AI, aligning payment with value, and building shared evaluation infrastructure—to accelerate adoption while maintaining patient safety and trust.\n\n---",
      "keyRecommendations": "- Publish cross-agency governance principles and clarify post-deployment expectations for non-device AI\n  - Support time-limited learning environments or safe harbors for well-scoped AI use cases\n- Modernize CMS payment policies to recognize AI as enabling clinical infrastructure\n  - Allow AI costs within value-based models\n  - Support outcome-linked payment pilots\n  - Enable temporary reimbursement pathways for high-evidence AI tools\n- Rebalance federal AI research toward real-world implementation\n  - Prioritize post-deployment monitoring, workflow integration, organizational governance, and economic impact analysis\n- Align interoperability policy with real-world AI use cases\n  - Support shared benchmarking and evaluation infrastructure\n  - Ensure data includes provenance, context, and longitudinal linkage\n- Support shared evaluation infrastructure rather than certifying individual tools\n  - Enable independent testing ecosystems and accreditation aligned with federal principles\n- Create a National Clinical AI Sandbox Program\n  - Time-limited real-world deployment with regulatory/reimbursement flexibility paired with structured evaluation\n- Establish an AI Evaluation Commons\n  - Shared national resource for standardized benchmarks, performance metrics, and failure-mode reporting\n- Launch Organizational AI Readiness Grants\n  - Support governance structures, clinician training, change management, and human-AI workflow integration\n- Develop a Patient Trust and Transparency Initiative\n  - Create patient-centered norms for disclosure, consent, escalation, and communication about AI use\n\n---",
      "mainConcerns": "- Regulatory uncertainty for non-device AI (documentation support, care coordination, generative AI) causes legal/compliance teams to delay or block deployment due to unclear accountability and liability\n  - Disproportionately affects smaller and resource-constrained organizations\n- Fee-for-service payment weakly rewards AI's actual value (cognitive efficiency, prevention, administrative burden reduction, care coordination)\n  - Organizations investing in AI are not capturing its financial benefits, undermining sustainability\n- Evidence gaps have shifted from model accuracy to real-world performance\n  - Health systems struggle with post-deployment monitoring, bias/drift detection, workflow integration, and economic evaluation\n- Current interoperability enables data movement but not data meaning\n  - Clinical data lacks provenance, context, labeling, and longitudinal linkage needed for AI\n- Patient trust concerns around transparency, escalation when AI is wrong, and potential inequities threaten durable adoption\n\n---",
      "notableExperiences": "- Key insight from cross-sector engagements: AI adoption fails not because of technical limitations but because of systemic misalignment across regulation, payment, evidence, data, and workflows\n- Observed pattern: legal and compliance teams block AI deployment not due to known safety risks, but due to uncertainty—a distinction that suggests clearer guidance alone could unlock adoption\n- Investment-benefit disconnect: organizations that invest in AI are consistently not the ones capturing its financial benefits, creating a structural sustainability problem even when clinical value is proven\n- Reframing the evidence gap: the question is no longer \"can AI perform tasks?\" but \"does it perform reliably, equitably, and safely over time and across settings?\"\n- Interoperability insight: data exchange without data meaning (provenance, context, labeling) fundamentally limits AI's potential\n\n---",
      "keyQuotations": "- \"AI adoption is constrained less by AI's technical capability than by misalignment among regulation, reimbursement, evidence generation, data infrastructure, and clinical workflows.\"\n- \"Legal and compliance teams frequently delay or block deployment not due to known safety risks, but due to uncertainty around accountability, liability, and post-deployment expectations.\"\n- \"We repeatedly observe that the organizations investing in AI are not those capturing its financial benefits, undermining sustainability even when clinical value is evident.\"\n- \"Interoperability that enables data movement but not data meaning limits AI impact.\""
    },
    "themeScores": {
      "2": 1,
      "4": 1,
      "5": 1,
      "6": 1,
      "7": 1,
      "9": 1,
      "2.1": 1,
      "2.4": 1,
      "2.5": 1,
      "4.1": 1,
      "4.2": 1,
      "5.4": 1,
      "7.1": 1,
      "9.1": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "Safe Harbor"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Generative AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Model Drift"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Accreditation"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Care Coordination"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Chronic Disease Management"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Provenance"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "AHRQ"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "FDA"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "ONC"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Administrative Burden"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Pilot Program"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Workflow Integration"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Equity"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Patient Safety"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Health System"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Trust"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "Fee-for-Service"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "Value-Based Care"
      }
    ],
    "hasAttachments": true,
    "wordCount": 1277,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0006",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Akshaya Bhagavathula",
    "submitterType": "Individual",
    "date": "2026-01-08T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Clinical AI as a population-level intervention\n  - Clinical AI is no longer a discrete technology to be evaluated solely at deployment\n  - Its safety, effectiveness, and equity can only be governed through continuous, post-deployment evaluation and accountability\n  - This comment submitted by an epidemiologist with expertise in AI, causal inference, and population health\n  - Informed by direct experience evaluating AI systems after deployment in clinical and public-sector environments\n\n- The foundational regulatory problem\n  - Growing mismatch between how clinical AI is currently regulated and how it actually behaves once implemented at scale\n  - Comment responds to HHS focus on improving evaluation, governance, and patient-centered use of AI\n  - Addresses how AI systems should be assessed beyond pre-deployment testing\n  - Addresses how regulatory oversight can remain effective as AI scales and adapts\n  - Addresses how patient-centered innovation can be operationalized as a concrete governance standard\n\n- Current state of AI in healthcare\n  - AI is no longer experimental within health care\n  - Embedded within clinical decision support, utilization management, documentation, triage, and surveillance systems\n  - Dominant risks are no longer confined to algorithm design or pre-deployment performance\n  - Risks emerge after deployment through interaction with clinicians, workflows, institutions, and populations\n\n- The product vs. intervention distinction\n  - Clinical AI is largely regulated as a discrete product\n  - In practice, it functions as a population-level intervention\n  - Once introduced, AI reshapes how decisions are made, how attention is allocated, how workflows evolve, and how care is delivered\n  - Effects accumulate over time and extend beyond individuals directly targeted\n  - Most consequential harms and benefits are systemic, not immediate or isolated\n\n- The causal sequence of deployed AI\n  - Current frameworks evaluate AI at deployment (near top of sequence)\n  - Majority of risk emerges downstream where no systematic oversight exists\n\n- Limitations of pre-deployment evaluation\n  - Pre-deployment evaluation is structurally insufficient though necessary\n  - Validation studies answer whether an algorithm can perform under specified conditions\n  - They do not answer whether the system remains safe, effective, or equitable once it reshapes real-world clinical behavior\n  - Key questions that cannot be resolved before deployment:\n    - Who is most frequently exposed to AI-mediated decisions\n    - How reliance on AI changes clinician judgment\n    - How performance drifts over time\n    - How outcomes differ across subpopulations\n  - These require observation in practice\n\n- Real-world example: triage model failure\n  - An AI model demonstrated strong pre-deployment performance across conventional metrics\n  - After implementation, clinicians increasingly relied on the model for triage decisions\n  - High-risk patients were identified more efficiently\n  - Moderate-risk patients experienced longer delays\n  - No alerts triggered, no technical failures logged\n  - Model behaved exactly as designed\n  - Harm emerged through interaction with workflow and human adaptation, not algorithmic error\n  - Because no post-deployment monitoring was required, effect remained invisible until outcome disparities became apparent months later\n\n- AI as population-level exposure (epidemiologic perspective)\n  - Deployed AI exhibits properties that mirror population-level exposures:\n    - Intensity: how frequently it influences decisions\n    - Latency: effects manifest downstream\n    - Effect modification: different impacts across demographic, clinical, and social groups\n    - Spillover effects: influences patients and clinicians not directly targeted\n    - Risk profile changes over time as data, workflows, and institutional practices evolve\n  - Unlike other population-level interventions, AI is not subject to routine post-deployment surveillance\n\n- The regulatory science failure\n  - This gap represents a regulatory science failure, not a technological one\n  - HHS has long recognized that interventions with diffuse, delayed, or heterogeneous effects cannot be governed through one-time approval alone\n  - Pharmacovigilance, device post-market surveillance, and public health monitoring systems exist because pre-market evidence is insufficient\n  - Clinical AI currently sits outside this paradigm\n\n- Recommendation: Post-Deployment Algorithmic Surveillance (PDAS)\n  - HHS should adopt PDAS as a cross-agency framework for governing clinical AI\n  - Core principle: no AI system influencing care should be presumed safe or effective based solely on pre-deployment evidence\n  - Evaluation shifts from static, product-based model to dynamic, population-based model\n\n- PDAS Element 1: Exposure characterization\n  - Must know who is exposed to AI-mediated decisions, how often, and in what contexts\n  - Without this, no causal assessment is possible\n  - Exposure measurement is foundational yet rarely required\n\n- PDAS Element 2: Continuous population-level performance monitoring\n  - AI systems must be evaluated longitudinally, not episodically\n  - Track calibration, discrimination, and error patterns over time\n  - Mandatory stratification by key subgroups\n  - Performance drift should be treated as a safety signal, not a technical footnote\n  - Real-world example: model performance gradually degraded due to changes in EHR coding practices\n    - No routine monitoring in place\n    - Disparities widened quietly over a year\n    - The model did not fail; oversight did\n\n- PDAS Element 3: Outcome-linked evaluation\n  - After deployment, accuracy alone is not adequate\n  - Relevant question: does AI use improve downstream clinical outcomes, access, or utilization patterns\n  - Surrogate metrics acceptable during development become insufficient at scale\n\n- PDAS Element 4: Structured reporting of algorithmic harms\n  - Many AI-related harms do not resemble traditional adverse events:\n    - Delayed care\n    - Silent misclassification\n    - Workflow distortions\n    - Disparity amplification\n  - These rarely trigger complaints or error reports\n  - They accumulate into population-level harm\n  - PDAS requires mechanisms to capture these signals systematically\n\n- PDAS Element 5: Predefined regulatory response pathways\n  - Surveillance without action is performative\n  - PDAS requires clarity on when recalibration, restriction, suspension, or withdrawal is warranted\n  - Must document and enforce how those actions are taken\n\n- Patient-centeredness as safety requirement\n  - Patient-centeredness is often treated as ethical aspiration or design preference\n  - From a safety perspective, this framing is inadequate\n  - An AI system that influences care but cannot be explained, contested, or reviewed is unsafe\n  - Real-world example: AI-driven utilization decisions delayed care\n    - Clinicians could not explain the rationale\n    - Patients had no appeal pathway\n    - Accountability was diffuse\n    - Technical accuracy was irrelevant; the failure was one of governance\n  - Patient contestability and human accountability must be regulatory requirements, not optional features\n\n- Equity considerations\n  - Disparities associated with AI are frequently attributed to biased training data\n  - Many inequities emerge only after implementation:\n    - Uneven adoption\n    - Differential trust\n    - Resource constraints\n    - Context-specific failures\n  - Equity cannot be ensured through design alone; must be measured continuously\n  - PDAS should include equity-specific monitoring requirements for AI deployed in:\n    - Medicaid\n    - Maternal health\n    - Rural care\n    - Public health settings\n\n- Adaptive and continuously learning AI systems\n  - Static approval models are incoherent when systems are designed to evolve\n  - Regulation must shift toward process-based accountability:\n    - Predefined learning boundaries\n    - Rollback mechanisms\n    - Transparent change logs\n    - Independent auditability\n  - Without these safeguards, adaptation becomes experimentation embedded within routine care\n\n- Interoperability as governance foundation\n  - Surveillance, auditability, contestability, and equity monitoring all depend on ability to access and integrate data across systems\n  - Vendor opacity and lock-in are governance failures, not merely market concerns\n  - HHS's existing interoperability initiatives provide necessary foundation\n  - AI oversight must be explicitly built upon them\n\n- Conclusion\n  - AI will continue to reshape health care regardless of regulatory posture\n  - Question is whether transformation is governed with same rigor applied to other population-level interventions\n  - Population science already provides the tools needed\n  - PDAS advances HHS goals by:\n    - Establishing practical framework for continuous evaluation\n    - Enabling accountable governance\n    - Ensuring patient-centered oversight once AI is embedded in care\n  - PDAS shifts evaluation from static performance metrics to population-level outcomes\n  - Enables timely regulatory response to emerging risks\n  - Ensures patients remain visible and protected within AI-mediated systems\n  - Allows innovation to proceed without normalizing unmeasured harm\n  - Supports responsible AI adoption while maintaining public trust, safeguarding equity, and strengthening evidence base\n\n---",
      "oneLineSummary": "An epidemiologist with direct AI evaluation experience argues that clinical AI must be regulated as a population-level intervention requiring continuous post-deployment surveillance, not just pre-market approval.\n\n---",
      "commenterProfile": "- **Name/Organization:** Dr. Akshaya Bhagavathula, PharmD, PhD, FACE; North Dakota State University\n- **Type:** Academic/Research\n- **Role/Expertise:** Associate Professor of Epidemiology; expertise in digital epidemiology, real-world evidence, population-level AI governance, causal inference, and population health; direct experience evaluating AI systems post-deployment in clinical and public-sector environments\n- **Geographic Scope:** National (based in Fargo, ND)\n- **Stake in Issue:** Professional expertise in evaluating deployed AI systems; has witnessed firsthand how AI harms emerge after deployment and remain invisible without proper surveillance\n\n---",
      "corePosition": "Clinical AI is no longer a discrete product but a population-level intervention whose risks emerge primarily after deployment through interactions with clinicians, workflows, and institutions. Current regulatory frameworks that focus on pre-deployment evaluation are structurally insufficient because they cannot detect the systemic, delayed, and heterogeneous harms that accumulate once AI reshapes real-world clinical behavior. HHS must adopt Post-Deployment Algorithmic Surveillance (PDAS) as a cross-agency framework to govern AI with the same rigor applied to other population-level interventions.\n\n---",
      "keyRecommendations": "- Adopt Post-Deployment Algorithmic Surveillance (PDAS) as a cross-agency framework for governing clinical AI\n  - Core principle: no AI system influencing care should be presumed safe or effective based solely on pre-deployment evidence\n  - Shift from static, product-based evaluation to dynamic, population-based evaluation\n\n- Implement five core PDAS elements:\n  - Exposure characterization: require measurement of who is exposed to AI-mediated decisions, how often, and in what contexts\n  - Continuous population-level performance monitoring: track calibration, discrimination, and error patterns longitudinally with mandatory stratification by subgroups; treat performance drift as a safety signal\n  - Outcome-linked evaluation: assess whether AI use improves downstream clinical outcomes, access, and utilization patterns rather than relying on accuracy metrics alone\n  - Structured reporting of algorithmic harms: create mechanisms to capture delayed care, silent misclassification, workflow distortions, and disparity amplification\n  - Predefined regulatory response pathways: establish clear criteria for when recalibration, restriction, suspension, or withdrawal is warranted\n\n- Treat patient contestability and human accountability as regulatory requirements, not optional features\n  - Patients must be able to understand, contest, and appeal AI-mediated decisions\n\n- Include equity-specific monitoring requirements for AI deployed in Medicaid, maternal health, rural care, and public health settings\n  - Measure equity continuously rather than assuming it can be ensured through design alone\n\n- Shift to process-based accountability for adaptive and continuously learning AI systems\n  - Require predefined learning boundaries, rollback mechanisms, transparent change logs, and independent auditability\n\n- Build AI oversight explicitly upon HHS's existing interoperability initiatives\n  - Address vendor opacity and lock-in as governance failures\n\n---",
      "mainConcerns": "- Current regulatory frameworks evaluate AI at deployment but majority of risk emerges downstream where no systematic oversight exists\n  - Pre-deployment validation studies cannot answer whether systems remain safe, effective, or equitable once they reshape real-world clinical behavior\n\n- AI harms are often invisible under current oversight\n  - No alerts triggered, no technical failures logged, yet harm accumulates through workflow interactions and human adaptation\n  - Outcome disparities may not become apparent for months\n\n- Many AI-related harms do not resemble traditional adverse events\n  - Delayed care, silent misclassification, workflow distortions, and disparity amplification rarely trigger complaints or error reports\n  - These accumulate into population-level harm without detection\n\n- Performance drift goes undetected without routine monitoring\n  - Changes in EHR coding practices or other contextual factors can degrade model performance\n  - Disparities can widen quietly over extended periods\n\n- Patient-centeredness is treated as aspirational rather than as a safety requirement\n  - AI systems that cannot be explained, contested, or reviewed are unsafe regardless of technical accuracy\n  - Patients may have no appeal pathway when AI-driven decisions delay care\n\n- Equity disparities emerge after implementation, not just from biased training data\n  - Uneven adoption, differential trust, resource constraints, and context-specific failures contribute to disparate impact over time\n\n- Static approval models are incoherent for adaptive AI systems\n  - Without proper safeguards, adaptation becomes experimentation embedded within routine care\n\n- Vendor opacity and lock-in undermine governance\n  - Surveillance, auditability, contestability, and equity monitoring all depend on data access and integration\n\n---",
      "notableExperiences": "- Evaluated an AI triage model that performed well pre-deployment but caused harm post-deployment\n  - Clinicians increasingly relied on the model\n  - High-risk patients identified more efficiently, but moderate-risk patients experienced longer delays\n  - No technical failures occurred; the model behaved exactly as designed\n  - Harm emerged through workflow interaction and human adaptation\n  - Effect remained invisible until outcome disparities appeared months later because no post-deployment monitoring was required\n\n- Observed a model whose performance gradually degraded due to changes in EHR coding practices\n  - No routine monitoring in place\n  - Disparities widened quietly over a year\n  - \"The model did not fail. Oversight did.\"\n\n- Encountered cases where AI-driven utilization decisions delayed care\n  - Clinicians could not explain the rationale\n  - Patients had no appeal pathway\n  - Accountability was diffuse\n  - Technical accuracy was irrelevant to the governance failure\n\n- Insight: deployed AI exhibits epidemiologic properties similar to population-level exposures\n  - Intensity, latency, effect modification, spillover effects, and changing risk profiles over time\n  - Yet AI is not subject to the routine post-deployment surveillance applied to other population-level interventions\n\n---",
      "keyQuotations": "- \"At present, clinical AI is largely regulated as a discrete product. In practice, it functions as a population-level intervention.\"\n\n- \"No alerts were triggered. No technical failures were logged. The model behaved exactly as designed. The harm emerged through interaction with workflow and human adaptation, not through algorithmic error.\"\n\n- \"An AI system that influences care but cannot be explained, contested, or reviewed is not merely non-patient-centered; it is unsafe.\"\n\n- \"The model did not fail. Oversight did.\"\n\n- \"Surveillance without action is performative.\""
    },
    "themeScores": {
      "2": 1,
      "6": 1,
      "7": 1,
      "8": 1,
      "9": 1,
      "2.6": 1,
      "3.4": 1,
      "6.1": 1,
      "7.1": 1,
      "8.5": 1,
      "9.1": 1,
      "9.2": 1,
      "9.3": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "Post-deployment Monitoring"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Pre-deployment Evaluation"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Trustworthy AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Model Drift"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Triage"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Population Health"
      },
      {
        "category": "Health Information Systems",
        "label": "CDS"
      },
      {
        "category": "Health Information Systems",
        "label": "EHR"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Medical Coding"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Utilization Management"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicaid"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Adverse Event"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Disparities"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Equity"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Trust"
      }
    ],
    "hasAttachments": true,
    "wordCount": 1729,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0007",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Keith Mountjoy",
    "submitterType": "Individual",
    "date": "2026-01-08T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Response to HHS Request for Information on Accelerating AI Adoption in Clinical Care\n  - Submitted by Keith Mountjoy, Founder of Resonance Tracing Instrument (RTI), Independent Researcher and Instrument Developer\n\n- Question 1: Biggest barriers to private sector innovation in AI for healthcare and clinical adoption\n  - Primary barrier is the lack of real-time, continuous physiological measurement data\n    - Most clinical AI systems rely on static or episodic inputs like imaging, lab results, or charted observations\n    - This forces AI models to infer dynamic internal state changes indirectly\n  - Creates challenges in trust, explainability, and generalization\n    - When AI outputs cannot be clearly tied to observable physical signals, clinicians hesitate to rely on them for decision support\n    - Post-hoc validation workflows increase cost and slow adoption\n  - Mismatch between AI development and clinical workflows\n    - AI tools are often designed as decision-makers rather than assistive measurement and observability layers\n    - This increases regulatory friction and liability concerns\n\n- Question 2: Regulatory, payment policy, or programmatic design changes HHS should prioritize\n  - HHS should prioritize policies distinguishing between measurement-first AI systems and decision-making AI systems\n    - Measurement platforms providing real-time physiological signals and structured state indicators can reduce uncertainty without issuing clinical recommendations\n    - These are often evaluated under frameworks designed for diagnostic devices, which is inappropriate\n  - Programmatic support for pilot-scale, non-authoritative AI systems would accelerate adoption while maintaining safety\n    - Includes R&D pathways allowing AI-enabled measurement tools to be deployed for monitoring, validation, and workflow support prior to full diagnostic claims\n  - HHS should consider revisiting programmatic alignment under existing research and innovation authorities\n    - Examples: cooperative agreements and demonstration programs\n    - Should support technologies enhancing observability and explainability for clinical AI rather than focusing exclusively on end-decision tools\n\n- Question 4: Most promising AI evaluation methods for non-medical devices in clinical care\n  - Most promising evaluation methods emphasize signal quality, stability, and state consistency rather than outcome prediction accuracy alone\n  - Critical metrics include:\n    - Repeatability\n    - Drift detection\n    - Signal-to-noise ratio\n    - Confidence scoring\n  - AI systems managing instrumentation or measurement stacks should be evaluated on ability to:\n    - Detect noise, motion artifacts, and coupling variability\n    - Maintain stable operation across diverse patient populations\n    - Produce interpretable state outputs rather than opaque predictions\n  - HHS could support these processes through targeted grants or cooperative agreements\n    - Focus on AI-assisted measurement validation\n    - Particularly in high-variability, high-noise environments like bedside or outpatient settings\n\n- Question 10: Specific areas of AI research HHS should prioritize\n  - HHS should prioritize AI research strengthening the measurement and observability layer of clinical care\n    - AI systems enhancing real-time physiological sensing\n    - Systems managing complex instrumentation\n    - Structured, explainable internal state tracking usable by clinicians and downstream AI models\n  - Published literature increasingly recognizes AI performance in clinical settings is limited by input data quality and temporal resolution\n    - Research integrating AI with real-time physical measurement platforms can close this gap\n    - Provides continuous, trustworthy signals rather than relying solely on retrospective datasets\n  - Federal R&D investment in AI-enabled measurement platforms would accelerate adoption while improving safety, explainability, and clinician trust\n\n- Public–Private Research and Pilot Pathways\n  - HHS could support public–private pilot programs focusing on AI-enabled measurement and observability technologies rather than clinical decision automation\n  - Pilot frameworks allowing early-stage systems to be evaluated as non-authoritative monitoring tools would enable:\n    - Real-world validation\n    - Workflow integration\n    - Data generation without increasing clinical risk\n  - Programs could be structured through:\n    - Cooperative agreements\n    - Demonstration projects\n    - Targeted R&D funding emphasizing signal reliability, explainability, and interoperability with existing clinical systems\n  - These pilots would bridge the gap between research prototypes and scalable clinical deployment",
      "oneLineSummary": "An independent instrument developer argues that clinical AI adoption is held back by poor real-time measurement data and urges HHS to create distinct regulatory pathways for \"measurement-first\" AI systems that enhance observability rather than make clinical decisions.",
      "commenterProfile": "- **Name/Organization:** Keith Mountjoy, Founder of Resonance Tracing Instrument (RTI) / Quamitry Labs\n- **Type:** Individual\n- **Role/Expertise:** Independent researcher and instrument developer specializing in physiological measurement technology\n- **Geographic Scope:** National (U.S.)\n- **Stake in Issue:** Develops measurement instrumentation that could integrate with clinical AI systems; directly affected by how HHS categorizes and regulates measurement-focused vs. decision-making AI tools",
      "corePosition": "I believe the biggest barrier to clinical AI adoption is the lack of real-time, continuous physiological measurement data—most AI systems are forced to work with static snapshots rather than dynamic signals, which undermines trust and explainability. HHS should create distinct regulatory and funding pathways for \"measurement-first\" AI systems that enhance observability and signal quality, rather than lumping them together with decision-making diagnostic tools.",
      "keyRecommendations": "- Establish policy distinctions between measurement-first AI systems and decision-making AI systems\n  - Measurement platforms that provide real-time signals shouldn't be evaluated under diagnostic device frameworks\n- Create programmatic support for pilot-scale, non-authoritative AI systems\n  - Allow AI-enabled measurement tools to be deployed for monitoring and validation before requiring full diagnostic claims\n- Revisit programmatic alignment under existing research and innovation authorities (cooperative agreements, demonstration programs)\n  - Focus on technologies enhancing observability and explainability rather than exclusively on end-decision tools\n- Fund targeted grants or cooperative agreements for AI-assisted measurement validation\n  - Prioritize high-variability environments like bedside or outpatient settings\n- Support public–private pilot programs for AI-enabled measurement and observability technologies\n  - Structure through cooperative agreements, demonstration projects, or targeted R&D funding\n  - Emphasize signal reliability, explainability, and interoperability with existing clinical systems",
      "mainConcerns": "- Clinical AI systems rely on static or episodic inputs (imaging, labs, charted observations), forcing indirect inference of dynamic internal states\n  - This creates problems with trust, explainability, and generalization\n- Clinicians hesitate to rely on AI outputs that cannot be clearly tied to observable physical signals\n- Post-hoc validation workflows increase costs and slow adoption\n- AI tools are designed as decision-makers rather than assistive measurement layers\n  - This increases regulatory friction and liability concerns\n- Measurement platforms are inappropriately evaluated under frameworks designed for diagnostic devices\n- AI performance in clinical settings is fundamentally limited by input data quality and temporal resolution",
      "notableExperiences": "- Offers a conceptual reframing: AI should be viewed as an \"observability layer\" rather than a decision-maker, which could reduce regulatory burden while maintaining safety\n- Proposes specific technical evaluation metrics for measurement-focused AI that differ from traditional outcome prediction accuracy:\n  - Repeatability, drift detection, signal-to-noise ratio, confidence scoring\n  - Ability to detect noise, motion artifacts, and coupling variability\n- Suggests \"non-authoritative monitoring tools\" as a regulatory category that would allow real-world validation without clinical risk",
      "keyQuotations": "- \"When AI outputs cannot be clearly tied to observable physical signals, clinicians are hesitant to rely on them for decision support.\"\n- \"AI tools are often designed as decision-makers rather than as assistive measurement and observability layers, which increases regulatory friction and liability concerns.\"\n- \"Pilot frameworks that allow early-stage systems to be evaluated as non-authoritative monitoring tools would enable real-world validation, workflow integration, and data generation without increasing clinical risk.\""
    },
    "themeScores": {
      "2": 1,
      "5": 1,
      "6": 1,
      "9": 1,
      "2.3": 1,
      "2.5": 1,
      "6.8": 1,
      "9.1": 1
    },
    "entities": [
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Explainable AI"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Diagnosis"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Claims Processing"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Pilot Program"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Ambulatory Care"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "Cooperative Agreement"
      }
    ],
    "hasAttachments": true,
    "wordCount": 814,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0008",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Nicholas Lewis",
    "submitterType": "Individual",
    "date": "2026-01-08T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- An effective way to promote innovation and effective usage of AI in the medical field would be for HHS and CMS to champion the cause by releasing an interactive Medicare mobile app for beneficiaries and providers\n  - This would ensure market usage and adoption by beneficiaries and providers\n  - Would provide a program \"owned and controlled\" platform for any other future private sector or provider integration\n- The \"Medicare Mobile\" app would provide real-time location tracking and health data for patients and providers\n  - Would be a strong deterrent for fraud, waste, and abuse\n- The app would allow integration of AI-based tools for:\n  - Claim verification\n  - Pre-authorization\n  - Payment determination\n  - Resource allocation\n  - Illness or outbreak detection and monitoring\n  - And much more\n- The app would also provide an on-demand communication solution for patients and providers\n  - Would reduce program administration costs\n- I have a platform and mobile app that I'm willing to donate provided there is any interest",
      "oneLineSummary": "Individual proposes HHS create a government-owned \"Medicare Mobile\" app to drive AI adoption, reduce fraud, and offers to donate his own platform to make it happen.",
      "commenterProfile": "- **Name/Organization:** Nicholas Lewis\n- **Type:** Individual\n- **Role/Expertise:** App/platform developer (has existing mobile app to donate)\n- **Geographic Scope:** National\n- **Stake in Issue:** Technology developer with potential solution to offer",
      "corePosition": "HHS and CMS should lead AI innovation in healthcare by creating an official Medicare mobile app that serves both beneficiaries and providers. A government-owned platform would drive adoption, enable AI integration, and provide a foundation for future private sector partnerships.",
      "keyRecommendations": "- HHS and CMS should release an interactive Medicare mobile app for beneficiaries and providers\n  - Would create a government \"owned and controlled\" platform\n  - Would serve as foundation for future private sector or provider integration\n- App should include real-time location tracking and health data capabilities\n- Integrate AI-based tools for:\n  - Claim verification and pre-authorization\n  - Payment determination\n  - Resource allocation\n  - Illness/outbreak detection and monitoring\n- Include on-demand communication features for patients and providers",
      "mainConcerns": "- Need for government leadership to drive AI adoption in healthcare\n- Fraud, waste, and abuse in Medicare system\n- Program administration costs",
      "notableExperiences": "- Commenter has developed a platform and mobile app that he's willing to donate to the government if there's interest\n  - Unusual offer of free technology transfer to support policy goals",
      "keyQuotations": "- \"I have a platform & mobile app that I'm willing to donate provided there is any interest.\""
    },
    "themeScores": {},
    "entities": [
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Abuse"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Fraud"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Waste"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      }
    ],
    "hasAttachments": false,
    "wordCount": 168,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0009",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Massive Bio",
    "submitterType": "Organization",
    "date": "2026-01-08T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Introduction and Core Thesis\n  - AI in healthcare has reached an inflection point where the core challenge is no longer innovation or model performance, but execution at scale\n  - Despite record per-capita healthcare spending, the U.S. underperforms on outcomes, access, and equity\n    - These failures are starkly visible in cancer care through delayed access, fragmented decision-making, and inefficient use of clinical trials\n  - Clinical trials are among the most underutilized assets in U.S. healthcare\n    - They provide cutting-edge therapy, shift costs from payers to sponsors, generate real-world evidence, and preserve patient choice\n    - Most patients learn about trials too late—after prior authorization delays or disease progression\n    - This is a structural failure, not a technology gap\n  - AI enables a reversal: every eligible patient should receive a Trial and Pathway Report before prior authorization is approved\n    - Implementing \"trial-before-authorization\" would reduce unnecessary utilization, accelerate access to advanced care, preserve clinician and patient agency, and deflate downstream costs\n\n- Execution Model: A Hub-and-Spoke National Architecture\n  - To move from pilots to scale, HHS should prioritize execution-ready models\n  - Massive Bio operates a national hub-and-spoke infrastructure purpose-built for this transformation\n  - The Hub functions as a federated AI orchestration layer\n    - Ingests patient-consented clinical and genomic data\n    - Maps evidence-based pathways\n    - Matches patients to clinical trials in real time\n    - Outputs are auditable, explainable, and compatible with decentralized, hybrid, and just-in-time (JIT) trial designs\n  - The Spokes include community oncology practices, academic centers, patient advocacy organizations, sponsors, and CROs\n    - Enables national scale without centralized data control\n    - Preserves privacy and local clinical autonomy\n  - Cost Deflation is achieved through decentralized and JIT trials, reduced unused infrastructure, and trial-first pathways that allow sponsor-funded therapies to precede payer authorization cycles\n\n- Key Policy Recommendations for HHS\n  - Establish Trial-First AI Pathways\n    - Explicitly support AI-generated trial and pathway reports prior to prior authorization\n    - Enable \"gold card\" mechanisms for real-time approval when evidence-based criteria are met\n    - Treat trial enrollment as a prior-auth exception\n  - Align CMS Incentives\n    - Reimburse AI-enabled trial matching and care coordination via new CPT codes\n    - Integrate trial utilization into value-based care models (APMs, MIPS) to reward cost-deflation through sponsor-funded therapies\n  - Create Safe Harbors for Validated AI\n    - Provide liability clarity for clinicians using HHS-certified AI tools\n    - Launch voluntary ONC certification to validate safety, bias mitigation, and interoperability\n  - Fund National Demonstrations\n    - Shift from pilots to operational-scale demonstrations through public-private partnerships\n    - Including a coordinated national AI deployment effort (\"Genesis Project\" for health)\n  - Enforce Interoperability\n    - Accelerate adoption of FHIR and mCODE\n    - Mandate bulk data access to support robust, representative AI training and benchmarking\n\n- Preserving Trust and Agency\n  - AI must expand choice, not constrain it\n  - The proposed model is consent-driven, privacy-by-design, and explicitly preserves physician autonomy\n    - AI informs but does not dictate\n  - Addressing provider barriers (data fragmentation, misaligned incentives, liability uncertainty) and patient priorities (clarity, access, personalization) is essential for adoption\n\n- About Massive Bio\n  - Founded in 2015, has become a leader in AI-enabled cancer care\n  - Served over 160,000 patients to date across all 50 states and globally\n  - Collaborates with the American Cancer Society (ACS) and other stakeholders to break down silos between academic centers and community care\n  - Technology powers ACS ACTS—a nationwide program helping patients of any background find personalized clinical trial options\n    - Provides AI-matched trial lists and support resources to patients in every state\n  - Reticulum Nexus platform unveiled at ESMO 2025 Congress\n    - AI-driven, multi-agent system that orchestrates the end-to-end oncology journey\n    - From initial patient outreach and consent to data analytics, trial matching, and last-mile enrollment logistics\n    - Connects previously fragmented data sources through native FHIR/mCODE interoperability\n    - Links electronic health records (EHRs), labs, sponsors, community clinics and registries on one fabric\n    - What once took hundreds of hours of manual effort now runs as an auditable, human-in-the-loop AI workflow\n  - Partnership with ACS and recognition by the White House Cancer Moonshot for AI-powered trial pre-screening hubs\n    - Dramatically shortened trial enrollment timelines and boosted participation rates\n    - Proactively identifying eligible patients through genomic and clinical data\n  - Approach keeps patients at the center\n    - Data is used only with patient consent\n    - De-identified for research insights\n    - Integrated with \"privacy-by-design\" governance to maintain trust\n  - Mission is to democratize access to advanced treatments\n    - Using AI to open clinical trials to diverse populations across geography, race, age, and socioeconomic status\n    - Accelerating value-based care through precision targeting of therapies\n\n- Question 1: Biggest Barriers to Private-Sector AI Innovation and Adoption\n  - Fragmented Data and Interoperability Challenges\n    - Quality healthcare AI depends on large volumes of diverse data, but health data today is siloed across EHR systems, labs, imaging archives, and devices\n    - Data fragmentation and lack of seamless interoperability is a top barrier\n    - Many AI developers struggle to obtain diverse, representative datasets due to information blocking or inconsistent standards\n    - Oncology data lives in disparate systems (EMRs, genomic testing labs, clinical trial databases) that historically do not communicate\n    - Massive Bio's AI platform integrates data from community clinics, academic centers, and patient-reported sources using common standards (FHIR, mCODE)\n  - Regulatory Uncertainty and Risk Aversion\n    - Innovators face ambiguity about how existing health regulations apply to AI\n    - Uncertainty around FDA oversight, HIPAA, and liability for AI-driven decisions creates a chilling effect\n    - If an AI clinical decision support system suggests a treatment and an error occurs, it's unclear who is liable\n    - This uncertainty leads to risk-averse institutional cultures where hospital compliance teams delay adoption\n    - Many promising AI solutions remain stuck in pilot phases\n  - Lack of Reimbursement and Misaligned Incentives\n    - Current payment landscape does not reward—and can even penalize—the use of AI in clinical practice\n    - Providers have little financial incentive to adopt AI tools that prevent hospitalizations or automate tasks\n    - Fee-for-service reimbursement is tied to volume of services\n    - Few billing codes or payment models directly reimburse AI-driven care improvements\n    - Absence of reimbursement pathways is repeatedly cited by providers as a barrier\n  - Clinical Integration and Workflow Challenges\n    - Integrating AI into clinical workflows remains difficult\n    - Busy healthcare providers are often overwhelmed with technology\n    - A new AI tool can feel like added complexity unless seamlessly embedded\n    - Training requirements can be a barrier—clinicians need to trust and understand the AI\n    - In smaller practices or rural hospitals, there may be limited IT support\n    - Creates an adoption gap: large health systems charge ahead while smaller providers lag behind\n    - Community oncologists often lack the decision support tools available at major cancer centers\n  - Trust, Transparency, and Bias Concerns\n    - Pervasive barrier is concern among providers and patients about trustworthiness of AI\n    - If AI is seen as a \"black box\" making life-and-death suggestions, clinicians may be reluctant to rely on it\n    - Past high-profile failures have made some skeptical (e.g., IBM's Watson for Oncology)\n    - Well-publicized studies have shown some healthcare algorithms had biases that could perpetuate disparities\n      - For instance, under-representing Black patients in care management programs due to biased training data\n    - Massive Bio has invested heavily in transparency\n      - Trial matching AI provides clinician-readable rationales for why a patient is eligible for a trial\n      - \"Radiant Core\" control plane ensures every AI decision is auditable for quality and compliance\n      - Focused on diversity in data through partnerships with ACS and community clinics\n\n- Question 2: Regulatory, Payment Policy, and Programmatic Changes to Incentivize AI\n  - Evolve Regulatory Frameworks to Foster Innovation with Safety\n    - Modernize FDA's Digital Health Oversight\n      - Update guidance on AI/ML-based software in clinical care\n      - Finalize and implement FDA's proposed framework for AI/ML SaMD allowing \"total product lifecycle\" approach\n      - Allow developers to make algorithm updates under pre-agreed protocols without requiring new submissions each time\n      - Clarify what constitutes a regulated \"medical device\" versus clinical decision support exempt from regulation\n      - Consider regulatory sandboxes or pilot programs for novel AI solutions\n    - Address Liability and Safe Harbors\n      - Explore creating safe harbors or guidance around liability when using approved AI tools\n      - Develop \"best practice\" guidelines for AI-assisted care\n      - Providers adhering to those could be granted certain protections\n      - Work with OIG to ensure Stark Law and Anti-Kickback regulations do not inadvertently impede AI tool adoption\n    - Strengthen Data Privacy & Sharing Policies for AI\n      - Clarify how HIPAA permits use of de-identified data for algorithm training\n      - Explicitly allow patients to authorize use of their data for AI R&D\n      - Develop standard patient consent frameworks for AI\n      - Promote adoption of Privacy Enhancing Technologies (federated learning, differential privacy)\n      - Allow health systems to participate in federated model training without it being considered a HIPAA disclosure\n  - Align Reimbursement and Payment Policy with AI-Driven Care\n    - Create Reimbursement Pathways for AI Services\n      - Establish clear reimbursement mechanisms for clinical uses of AI that add value\n      - Develop new CPT codes or modifiers for AI-assisted procedures and decision support\n      - Introduce Category III (tracking) codes initially, then progress to payment once value is demonstrated\n      - Extend technology add-on payments (NTAP) to AI\n      - Reward outcomes and efficiency—if AI reduces total cost of care or improves quality, providers should share in savings\n    - Incorporate AI into Value-Based Care Models and Quality Programs\n      - Leverage alternative payment models (APMs) to drive AI adoption\n      - Explicitly encourage or require use of advanced analytics/AI in ACOs, bundled payment programs, and specialty models\n      - Include quality measures or bonus points for utilizing evidence-based AI decision support\n      - CMS Innovation Center could pilot an \"AI Accelerator\" within ACOs\n      - Include use of clinical AI in Merit-Based Incentive Payment System (MIPS) Improvement Activities\n    - Reduce Administrative Barriers and Prior Auth Burdens via AI\n      - Modernize prior authorization regulations to recognize AI-based determinations\n      - If an AI algorithm finds that a patient meets evidence-based criteria, payers could be required to automatically approve\n      - Use a \"gold card\" approach for providers using such tools\n      - Pilot this in Medicare Advantage\n      - Support policies that \"clinical trial enrollment can be considered a prior auth exception\"\n  - Programmatic and Policy Design Changes\n    - Expand Public-Private Partnerships and Challenge Programs\n      - Create AI Clinical Care Challenge series—prize competitions around specific needs\n      - Establish Centers of Excellence for AI in Clinical Practice via grant funding\n      - Continue support for consortiums like CancerX\n      - Collaborate with DOE and NSF on Genesis Mission initiative\n    - Revise ONC Certification and CMS Conditions of Participation\n      - Consider ONC-administered AI Certification program—voluntary at first\n      - Assess AI clinical tools for basic safety, interoperability, and bias mitigation\n      - Update hospital Conditions of Participation to require an AI governance plan\n    - Revisit CFR provisions hindering data sharing\n      - Align 42 CFR Part 2 (confidentiality of substance use records) with HIPAA\n      - Clarify 45 CFR 164 (HIPAA Privacy Rule) de-identification standards for genomic information\n\n- Question 3: Novel Legal and Implementation Issues for Non-Medical-Device AI\n  - Liability and Accountability for AI-Influenced Decisions\n    - When an AI system influences clinical decisions, it blurs traditional lines of accountability\n    - Ambiguity over who is legally responsible—clinician, hospital, or software developer\n    - HHS should convene stakeholders to develop consensus guidelines on AI liability\n    - Endorse approach that clinicians remain final authority and thus primarily liable\n    - Following validated AI recommendations could be considered in determining standard of care\n    - Explore model legislation for states addressing AI liability\n  - Indemnification and Vendor Responsibility\n    - Hospitals often seek indemnification from AI vendors for any harm caused by software\n    - Vendors are wary of open-ended liability\n    - HHS could develop model contract language or best practices for AI procurement\n    - Consider an insurance pool or fund for AI-related incidents analogous to National Vaccine Injury Compensation Program\n  - Privacy and Data Use Ethics\n    - AI models trained on patient data might inadvertently \"remember\" sensitive details\n    - Companies may seek to use patient data for secondary AI development in ways patients don't expect\n    - HHS should update privacy regulations and guidance\n    - Clarify how de-identification rules apply to complex data like genomic or free-text data\n    - Develop new guidelines or safe harbors for AI model training\n    - Support development of technical standards for AI model security\n    - Extend privacy and security frameworks to explicitly cover the AI lifecycle\n  - Bias, Fairness, and Health Equity Concerns\n    - AI tools can inadvertently propagate bias against protected classes\n    - HHS should set expectations that AI in clinical settings must be fair and unbiased\n    - Issue guidance or regulations requiring AI undergo bias testing and mitigation\n    - Incorporate this into Medicare Conditions of Participation or certification criteria\n    - Hospitals should have algorithmic bias control programs\n  - Transparency and Explainability Requirements\n    - Traditional medical practice relies on explainable logic\n    - With AI, if neither user nor patient can understand basis for recommendation, it challenges informed consent and accountability\n    - HHS should consider policies pushing for algorithmic transparency appropriate to context\n    - Require patients be informed when AI is involved in their care\n    - Promote standards for explainability\n  - Recommend HHS establish a Healthcare AI Governance Council\n    - Bring together FDA, ONC, CMS, OCR, OIG, and external experts\n    - Continuously monitor emerging issues and update policies\n    - Could issue annual reports or recommendations\n\n- Question 4: Promising Evaluation Methods for Non-Device AI\n  - Promising Pre-Deployment Evaluation Methods\n    - Retrospective Validation with Gold-Standard Datasets\n      - Test AI on retrospective data where outcomes or \"ground truth\" decisions are known\n      - Measure accuracy, sensitivity, specificity\n      - Evaluation should go beyond accuracy to robustness across subgroups\n      - Pre-deployment robustness testing with edge cases or noisy data\n    - Simulation and Clinical Vignettes\n      - Clinicians presented with cases and AI's recommendation\n      - Measure how clinicians respond, whether AI's input improved decision quality or speed\n      - Key metrics include time saved, reduction in cognitive workload, user trust scores\n    - Prospective Pilot Studies\n      - Conduct prospective pilot in limited real-world setting before wider rollout\n      - A/B testing or stepped-wedge trial designs\n  - Promising Post-Deployment Evaluation and Monitoring\n    - Continuous Performance Monitoring and Feedback Loops\n      - Implement ongoing monitoring of AI's outputs versus actual outcomes\n      - Monitoring metrics include real-world accuracy, error rates, override rates, outcome impact\n      - Continual auditing for fairness\n    - Robustness Testing in Production\n      - Periodic robustness testing by injecting test cases\n      - Monitor for data drift\n      - Automated drift detection algorithms\n      - Massive Bio uses Radiant Core to supervise agents and flag anomalies in real time\n    - User Feedback and Governance Committees\n      - Formalize user feedback channels\n      - AI or Clinical Decision Support oversight committees\n      - Track if clinicians are reporting issues, confusion, or near-misses\n  - Workflow and Human-Centered Evaluation Methods\n    - Usability Testing\n      - Heuristic evaluation or direct observation\n      - Metrics include error rates, time to complete tasks, user satisfaction ratings\n    - Impact on Provider Workload and Burnout\n      - Measure how AI affects provider workload\n      - AI scribes reduced physician after-hours documentation time, correlating with 40% drop in burnout in pilot at Mass General Brigham\n    - Patient-Centered Evaluation\n      - Evaluate patient experience and outcomes\n      - Metrics like patient satisfaction scores, uptake rates of AI-suggested options, outcomes\n  - How HHS Should Support These Processes\n    - Funding and Grants\n      - Fund research and development of AI evaluation frameworks and tools\n      - Support creation of open reference datasets and bias testing suites\n      - Fund projects developing best practices for continuous monitoring\n    - Contracts and Cooperative Agreements\n      - Contract entities to perform evaluations on broader scale\n      - Establish centers that partner with HHS to evaluate AI implementations\n      - Create National AI Testing Collaborative\n    - Prize Competitions and Challenges\n      - Run \"AI Evaluation Challenge\" inviting proposals for novel metrics or methods\n      - \"AI Stress Test Challenge\" to design battery of tests for edge-case failures\n    - Standards and Frameworks Endorsement\n      - Collaborate with standards-developing organizations (IEEE, ISO, NIST)\n      - Adopt and tailor NIST AI Risk Management Framework to healthcare\n      - Include criteria for \"baseline algorithm validation and ongoing monitoring\" in ONC certification\n\n- Question 5: Supporting Private-Sector AI through Accreditation, Certification, and Credentialing\n  - Establish \"AI in Healthcare\" Voluntary Certification Program\n    - Set up voluntary certification for AI tools used in clinical care\n    - Certification criteria might include demonstrated accuracy, adherence to privacy/security standards, bias testing results, interoperability capabilities, usability evidence\n    - Officially recognize certified products\n    - Tie incentives to certification (e.g., slight increase in reimbursement or bonus under MIPS)\n  - Support Development of Accreditation Standards for AI Use by Healthcare Organizations\n    - Private sector bodies like The Joint Commission or URAC could develop accreditation criteria\n    - Cover governance, training, monitoring, patient consent/communication\n    - HHS could fund projects with accrediting organizations to develop new standards\n    - Eventually CMS might incorporate into Conditions of Participation\n  - Facilitate Industry-Driven Testing and Benchmarking Consortia\n    - Sponsor AI Clinical Validation Lab where multiple vendors' tools are evaluated on common dataset\n    - Run by trusted third party under contract with HHS\n    - Provide reports or \"seals of quality\" to top performers\n  - Credentialing and Training of Healthcare Workforce in AI\n    - Encourage professional boards to develop certification modules for practitioners to be \"AI-proficient\"\n    - Support grants to medical specialty societies to create continuing education and credentialing programs\n    - Create \"AI Skills Consortium\" defining core competencies for clinicians using AI\n  - Recognize and Promote Exemplars through Public-Private Partnerships\n    - Highlight case studies of accredited/certified AI tools improving care\n    - Establish annual \"HHS AI in Healthcare Awards\"\n  - Leverage Existing Quality and Safety Programs\n    - Integrate AI considerations into existing private-sector-led quality programs\n    - Encourage Patient Safety Organizations to collect and analyze AI-related safety events\n    - Coordinate with malpractice insurers and organizations like ECRI Institute\n  - Support Interoperability and Standards Development\n    - Sponsor \"plug-a-thon\" events for AI integration\n    - Require in federal procurement that AI solutions support certain interoperability standards\n\n- Question 6: Successes and Shortcomings of AI in Clinical Care; High-Potential Novel AI Tools\n  - Successes—AI Tools that Met or Exceeded Expectations\n    - Medical Imaging and Diagnostics\n      - AI for image analysis has shone as early success\n      - FDA-cleared AI algorithms for detecting diabetic retinopathy or identifying pulmonary nodules demonstrated accuracy on par with expert physicians\n      - Cleveland Clinic deployed AI-based sepsis early warning system\n        - 10-fold reduction in false alarm rates\n        - 46% increase in actual sepsis cases identified in time\n        - Alerted clinicians to sepsis even before antibiotics were given\n      - AI tools in pathology improved detection of metastatic cancer cells\n    - Operational Efficiency and Cost Savings\n      - Predictive algorithms for patient flow and staffing helped optimize bed management and reduce length of stay\n      - AI-driven scheduling tools cut wait times\n      - Industry analyses project AI could reduce hospital operating costs by 10-20%, translating to up to $300-$900 billion in annual savings by 2050\n      - AI for automated medical coding and billing sped up reimbursements and lowered administrative costs\n    - Administrative and Documentation Burden Reduction\n      - AI \"scribes\" and NLP tools successful in reducing clinician burden\n      - Mass General Brigham reported about 40% relative drop in physician burnout during AI scribe pilot\n    - Clinical Decision Support and Personalized Medicine\n      - Massive Bio's AI-driven trial matching \"pre-screening hubs\" dramatically shortened time to identify clinical trial options\n      - Boosted trial participation rates\n      - Recognized by White House Cancer Moonshot\n      - Over 50,000 patients per year projected\n      - Cost benefits: patients on trials get expensive therapy covered by trial sponsor\n    - Population Health and Preventive Care\n      - AI risk prediction models helped focus interventions on right patients\n      - Some ACOs use AI to predict which patients are at risk of hospitalization and proactively deploy care managers\n  - Shortcomings—Where AI Tools Have Fallen Short\n    - Generalizability and Real-World Performance Gaps\n      - AI tools that performed well in research settings failed to meet expectations in real world\n      - IBM Watson for Oncology fell short of clinical expectations\n      - AI algorithms showed degraded accuracy when deployed on different patient populations or hospitals\n    - Bias and Health Equity Issues\n      - Algorithm widely used to prioritize patients for care management systematically underestimated health needs of Black patients\n      - Used healthcare spending as proxy for illness, and historically less money was spent on Black patients\n    - Lack of Clear Financial ROI in Some Cases\n      - In some deployments ROI has been elusive or slower than expected\n      - AI scribes improved experience but had \"limited financial ROI\" initially\n      - AI alone doesn't save money; must be paired with effective process changes\n    - Workflow Integration and Alert Fatigue\n      - Some AI tools under-delivered because not well integrated or produced too many alerts\n      - Created alert fatigue and low utilization\n  - High-Potential Novel AI Tools for the Future\n    - AI for Precision Medicine & Complex Decision Support\n      - AI that can synthesize multimodal data (genomics, proteomics, imaging, clinical records) to guide personalized treatment\n      - Could spare patients ineffective treatments\n      - Suggest novel uses for existing drugs or optimal trial opportunities\n      - Massive Bio's AI has started to identify patterns correlating with better trial responses\n    - Generative AI and Large Language Models (LLMs) in Clinical Care\n      - LLMs could function as AI co-pilots for clinicians\n      - Synthesize patient's entire chart and provide concise summary with key recommendations\n      - Draft clinical plans that doctor can refine\n      - Improve patient communication through AI agents that converse with patients\n    - Predictive Analytics for Preventive and Value-Based Care\n      - AI that predicts events like hospital admissions, disease progression, or complications\n      - Enable preemptive action\n      - AI monitoring of heart failure patients via wearables predicting acute decompensation days in advance\n      - AI for frailty and dementia flagging early cognitive decline or frailty risk\n    - Automation of Routine Tasks and Robotic Process Automation (RPA)\n      - AI-driven automation of repetitive administrative tasks\n      - Appointment scheduling bots, AI handling prior authorization paperwork, automated image post-processing\n    - Decentralized and At-Home Care via AI\n      - Tools allowing hospital-level monitoring and treatment at home\n      - AI interpreting data from home devices to manage patients remotely\n      - AI that monitors post-surgery patient's wearable data to detect early signs of complication\n    - Clinical Trial Optimization and Drug Development\n      - AI in trial design (simulating trials, optimizing patient selection criteria)\n      - AI in pharmacovigilance (monitoring for drug side effects in real time)\n      - AI-driven digital twins (virtual patient models) to predict outcomes\n\n- Question 7: Influential Roles in AI Adoption and Primary Hurdles\n  - Key Roles and Decision Makers Influencing AI Adoption\n    - Executive Leadership (C-Suite)\n      - CEO, Chief Medical Officer (CMO), Chief Information Officer (CIO), Chief Technology/Innovation Officer\n      - If C-suite believes in strong digital/AI strategy, institution will pursue AI projects\n      - CFO influential from budget perspective—view on ROI can make or break investment\n    - Clinical Champions (Physician/Nurse Leaders)\n      - Respected physicians or nurse leaders who advocate for AI tool can sway peers\n      - Chief Medical Information Officer (CMIO) bridges IT and clinical practice\n      - If frontline clinicians are not involved or opposed, no amount of executive push will succeed\n    - Governance Committees\n      - IT Governance, Value Analysis, Ethics Boards\n      - Evaluate new technologies, scrutinize proposals for new AI tools\n      - Can be bottleneck if overly conservative or unfamiliar with evaluating AI\n    - IT Department / Chief Information Security Officer (CISO)\n      - Evaluate integration effort, cybersecurity risks, compatibility\n      - CISO's approval often needed if AI involves cloud or external data sharing\n    - Payers and External Decision Makers\n      - If payer starts reimbursing AI-based service, hospital CFO more likely to approve\n      - If regulator or accreditation body starts requiring AI-related competencies, CMO will push to implement\n  - Primary Administrative Hurdles to AI Adoption\n    - Budgeting and ROI Justification\n      - Hospital budgets are tight, AI tools cost money upfront\n      - Many AI benefits are indirect or long-term\n      - Often no line item for \"AI\" in budgets\n    - Procurement and Contracting Processes\n      - Complex procurement processes\n      - Legal contracting addressing data use, IP, liability can be arduous\n      - Some organizations require competitive bidding or extensive due diligence\n    - Integration with Existing Systems\n      - Coordinating between AI vendor and internal IT for integration\n      - If AI can't plug easily into EHR or workflow, may end up as separate system clinicians resist\n    - Training and Change Management\n      - Planning and executing training for staff\n      - Developing new policies\n      - Change management is hard in healthcare\n    - Data Governance and Privacy Concerns\n      - Internal rules about data sharing\n      - Getting approvals through privacy boards or compliance committees\n    - Measurement and Proof of Concept Requirements\n      - Some institutions require extensive pilot studies internally before full adoption\n    - Cultural Resistance and Workforce Acceptance\n      - If organization's culture is not innovation-friendly or staff fear AI\n      - Must communicate AI is there to assist, not replace\n\n- Question 8: Interoperability Opportunities to Fuel AI Development\n  - Integration of Electronic Health Records (EHRs) with AI Tools\n    - Enhanced use of FHIR and full compliance with 21st Century Cures Act's interoperability rules\n    - Standard FHIR profiles for specific needs could let AI modules operate across many hospitals with minimal tweaks\n    - Bulk FHIR data access crucial for research and population-level AI\n    - HHS should continue pushing bulk data standards\n    - Provide synthetic or test FHIR datasets for AI benchmarking\n  - Standardization of Key Clinical Data Types\n    - Genomic Data\n      - Adopt standards like HL7 FHIR Genomics or GA4GH schemas\n      - mCODE (minimal Common Oncology Data Elements) standard defines how to represent oncology-specific data including genomics\n      - Massive Bio's platform uses native FHIR/mCODE integration\n    - Imaging and PACS\n      - Enhanced interoperability via cloud image exchanges or APIs\n      - Advance DICOMweb (RESTful APIs for DICOM) and link with FHIR\n      - TEFCA might be extended to include image exchange\n    - Unstructured Clinical Notes\n      - Adoption of standards like C-CDA and FHIR DocumentReference to share notes in structured formats\n      - Using NLP-specific standards\n    - Patient-Generated Health Data and Devices\n      - Standards like IEEE 11073 for device data\n      - Frameworks to integrate patient-generated data into EHRs\n  - Interoperability to Link Clinical and Research Data\n    - Connect clinical care data with clinical trial and research data\n    - Standardize criteria (efforts to use FHIR for trial eligibility criteria)\n    - Encourage adoption of OMOP common data model\n    - Consider linking claims data with clinical data\n  - Benchmarking Tools and Shared Datasets\n    - Interoperability can enable shared benchmark datasets for AI\n    - HHS could coordinate creation of reference datasets continuously updated via interoperable feeds\n  - Enhanced Interoperability in Social Determinants of Health (SDOH) Data\n    - Need standards (HL7 Gravity Project) to record and share SDOH in health records\n    - Massive Bio's ACS ACTS program identifies social needs like transportation and lodging\n  - Where enhanced interoperability would widen market opportunities\n    - Barrier to entry for AI startups lowers\n    - AI solutions can be EHR-agnostic\n    - Providers would have more choices\n  - How interoperability fuels research\n    - Researchers spend enormous effort on data wrangling\n    - Broad interoperability would bring in missing pieces from rural or minority-heavy populations\n\n- Question 9: Patient and Caregiver Challenges and Concerns about AI\n  - Challenges Patients/Caregivers Want AI to Address\n    - Access to the Right Care and Information\n      - Navigating healthcare system and finding best treatment options is major pain point\n      - Patients want AI to help cut through complexity\n      - Cancer patients often ask \"Is there a clinical trial or new therapy that could help me?\"\n      - Many feel overwhelmed trying to research on their own\n      - Want AI-driven tools that proactively present personalized options\n      - Hope AI can democratize expertise\n    - Timely and Accurate Diagnoses\n      - Frustration about missed or delayed diagnoses\n      - Want AI to assist in catching problems earlier\n      - In rare diseases, want AI to shorten diagnostic odyssey\n    - Better Coordinated Care and Reduced Administrative Burdens\n      - Navigating appointments, refills, referrals are pain points\n      - Want health system to be more coordinated and responsive\n      - Would be thrilled if AI could integrate information so every provider is up to date\n      - Patients often feel they are project managers of their own care\n    - Improved Treatment Experiences and Outcomes\n      - Want better health outcomes—longer survival, less pain, improved quality of life\n      - See AI as means to tailor treatments and manage chronic conditions better\n      - Want AI \"early warning system\" and continuous support\n    - Equity in Access\n      - Patients in underserved areas hope AI can bridge gap\n      - Want assurance treatments are designed for people like them\n  - Concerns Patients/Caregivers Have about AI\n    - Privacy and Data Security\n      - Foremost concern is \"What happens to my data?\"\n      - Worry about misuse or data falling into wrong hands\n      - Fear of hacking or leaks\n      - Want to actively consent and have control\n    - Loss of Human Touch and Personal Interaction\n      - Fear healthcare could become impersonal\n      - Human connection with doctors and nurses is source of comfort\n      - Want assurance AI is assistant, not replacement\n      - Idea of \"robot doctor\" can be intimidating\n    - Bias and Fairness\n      - Concerned AI could be biased or make unfair decisions\n      - Especially from historically underserved groups\n      - Want AI to be tool for health equity, not something that deepens disparities\n      - Want transparency about how AI makes decisions\n    - Accuracy and Safety of AI Decisions\n      - Concern about AI making mistakes\n      - Need proof tools are accurate and won't lead them astray\n      - Want to know AI systems are rigorously tested with oversight\n      - Concept of accountability is key\n    - Understanding and Consent\n      - May feel anxious if AI used without their knowledge\n      - Worry about being \"experimented on\" by unproven tech\n      - Want to be informed AI is involved and to agree to it\n      - Want basic explanation in plain language\n    - Cost and Access Concerns\n      - Wonder if AI-driven care will cost more\n      - Fear AI might be used by insurance to cut costs at expense of care\n      - Want AI to reduce out-of-pocket costs, not be extra charge or gatekeeper\n\n- Question 10: Priority Areas for AI Research and Evidence\n  - Priority Areas for AI Research\n    - Implementation Science for AI in Healthcare\n      - Understand how to integrate AI into real-world clinical settings successfully\n      - Research on best practices for deploying AI in various environments\n      - Capture lessons from successful and failed implementations\n    - Evaluation and Monitoring Frameworks (Safety, Efficacy, Bias)\n      - Develop standardized methods to evaluate AI tools\n      - Metrics for clinical impact, user trust, economic outcomes\n      - Processes for continuous monitoring post-deployment\n    - Bias Mitigation and Fairness in AI\n      - Research into algorithms that detect and correct bias\n      - Methods to ensure AI tools are equitable\n      - Creating diverse training datasets\n      - Sociotechnical research on how biases manifest\n    - Interoperability and Data Sharing Models\n      - Research how to safely and effectively share data for AI development\n      - Privacy-preserving analytics, federated learning, synthetic data\n      - Research on standardization of health data for AI\n    - AI for Public Health and High-Burden Conditions\n      - Focus on national priorities: chronic diseases, mental health, aging-related conditions, health crises\n    - Human-AI Interaction & Explainability Research\n      - Study how AI can effectively communicate with clinicians and patients\n      - Different explanation approaches and how they affect decision-making\n      - How clinicians make"
    },
    "themeScores": {
      "1": 1,
      "2": 1,
      "3": 1,
      "4": 1,
      "5": 1,
      "6": 1,
      "7": 1,
      "8": 1,
      "9": 1,
      "1.1": 1,
      "1.2": 1,
      "1.4": 1,
      "1.5": 1,
      "1.6": 1,
      "1.7": 1,
      "2.1": 1,
      "2.2": 1,
      "2.4": 1,
      "2.5": 1,
      "3.1": 1,
      "3.2": 1,
      "4.1": 1,
      "4.2": 1,
      "4.3": 1,
      "4.6": 1,
      "4.7": 1,
      "5.1": 1,
      "5.9": 1,
      "6.1": 1,
      "6.2": 1,
      "6.4": 1,
      "6.5": 1,
      "6.8": 1,
      "7.1": 1,
      "7.3": 1,
      "7.4": 1,
      "7.5": 1,
      "8.1": 1,
      "8.2": 1,
      "8.3": 1,
      "8.4": 1,
      "8.5": 1,
      "8.6": 1,
      "9.1": 1,
      "9.2": 1,
      "9.3": 1
    },
    "entities": [
      {
        "category": "AI Applications in Healthcare",
        "label": "AI Scribe"
      },
      {
        "category": "AI Applications in Healthcare",
        "label": "Early Warning System"
      },
      {
        "category": "AI Applications in Healthcare",
        "label": "Predictive Analytics"
      },
      {
        "category": "AI Applications in Healthcare",
        "label": "Risk Prediction"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "AI Governance"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Bias Testing"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Human-in-the-Loop"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "NIST AI RMF"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Post-deployment Monitoring"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Robustness Testing"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Safe Harbor"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "AI/ML SaMD"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Agentic AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Algorithmic Bias"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Black Box AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Digital Twin"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Explainable AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Federated Learning"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Generative AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Large Language Model"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Machine Learning"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Model Drift"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Natural Language Processing"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "SaMD"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Accreditation"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Certification"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Conditions of Participation"
      },
      {
        "category": "Accreditation and Certification",
        "label": "FDA Clearance"
      },
      {
        "category": "Accreditation and Certification",
        "label": "ONC Certification"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Care Coordination"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Clinical Trial"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Diagnosis"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Informed Consent"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Screening"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Treatment"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Trial Matching"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Behavioral Health"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Chronic Disease Management"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Genomics"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Oncology"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Pathology"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Population Health"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Precision Medicine"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Cybersecurity"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Governance"
      },
      {
        "category": "Data Privacy and Security",
        "label": "De-identification"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Information Blocking"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS Innovation Center"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "DOE"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "FDA"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "NIST"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "NSF"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "OCR"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "OIG"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "ONC"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Health Information Systems",
        "label": "CDS"
      },
      {
        "category": "Health Information Systems",
        "label": "EHR"
      },
      {
        "category": "Health Information Systems",
        "label": "EMR"
      },
      {
        "category": "Health Information Systems",
        "label": "PACS"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Claims Processing"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Credentialing"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Medical Coding"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Prior Authorization"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Quality Reporting"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Change Management"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Pilot Program"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Procurement"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "ROI"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Total Cost of Care"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Workflow Integration"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "C-CDA"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "CPT"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "DICOM"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "FHIR"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "GA4GH"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "Gravity Project"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "HL7"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "IEEE 11073"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "OMOP"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "TEFCA"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "mCODE"
      },
      {
        "category": "Healthcare Programs",
        "label": "ACO"
      },
      {
        "category": "Healthcare Programs",
        "label": "APM"
      },
      {
        "category": "Healthcare Programs",
        "label": "MIPS"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare Advantage"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Alert Fatigue"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Burnout"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Disparities"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Equity"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Patient Safety"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Academic Medical Center"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Health System"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Rural Hospital"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "CFO"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "CIO"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "CISO"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "CMIO"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "CMO"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Care Manager"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Nurse"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Oncologist"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Physician"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Laws and Regulations",
        "label": "21st Century Cures Act"
      },
      {
        "category": "Laws and Regulations",
        "label": "42 CFR Part 2"
      },
      {
        "category": "Laws and Regulations",
        "label": "Anti-Kickback Statute"
      },
      {
        "category": "Laws and Regulations",
        "label": "HIPAA"
      },
      {
        "category": "Laws and Regulations",
        "label": "HIPAA Privacy Rule"
      },
      {
        "category": "Laws and Regulations",
        "label": "Stark Law"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Indemnification"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Malpractice"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Standard of Care"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Cancer"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Dementia"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Diabetes"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Diabetic Retinopathy"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Heart Failure"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Rare Disease"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Sepsis"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Autonomy"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Experience"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient-Generated Health Data"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "Bundled Payment"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "NTAP"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "Value-Based Care"
      },
      {
        "category": "Professional Organizations",
        "label": "ACS"
      },
      {
        "category": "Professional Organizations",
        "label": "ECRI Institute"
      },
      {
        "category": "Professional Organizations",
        "label": "IEEE"
      },
      {
        "category": "Professional Organizations",
        "label": "ISO"
      },
      {
        "category": "Professional Organizations",
        "label": "Joint Commission"
      },
      {
        "category": "Professional Organizations",
        "label": "URAC"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "Cancer Moonshot"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "Implementation Science"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "RWE"
      },
      {
        "category": "Social Determinants of Health",
        "label": "SDOH"
      },
      {
        "category": "Social Determinants of Health",
        "label": "Underserved Population"
      }
    ],
    "hasAttachments": true,
    "wordCount": 23306,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0010",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "David Bynon",
    "submitterType": "Individual",
    "date": "2026-01-12T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Introduction and Core Challenge\n  - This comment proposes a model-agnostic, non-device protocol for governing clinical and administrative AI\n  - The core challenge facing HHS is not whether AI can be deployed, but how to scale its use without expanding federal operational burden or relying on opaque, non-scalable model-level oversight\n  - Current discussions appropriately emphasize data access, interoperability, and model governance, but these approaches alone do not resolve a recurring operational failure: agentic drift\n\n- Proposed Approach: Deterministic Semantic Substrate\n  - Introduces a deterministic semantic substrate that makes authoritative policy and benefit meaning explicit, versioned, and machine-retrievable at the point of AI use\n  - Requires AI systems to retrieve from authoritative, machine-readable policy memory rather than infer meaning from narrative text\n  - Enables AI outputs to be verified directly against published definitions without inspecting model internals or reconstructing inference\n  - Supports deflationary AI adoption by aligning semantic responsibility with carriers, providers, and intermediaries where existing accuracy obligations already reside\n  - Preserves federal oversight through auditable outputs\n  - Operationalizes the \"AI-enabling infrastructure and documentation\" principles articulated in OMB Memorandum M-25-21\n\n- Practical Barrier: Agentic Drift in Non-Device AI\n  - Agentic drift refers to the tendency of AI systems to diverge from authoritative policy intent, benefit definitions, and regulatory meaning over time when operating without a persistent, verifiable semantic memory substrate\n  - In regulated benefit environments, this manifests as:\n    - Inconsistent interpretations of coverage and cost-sharing across AI systems\n    - Hallucinated or outdated benefit explanations\n    - Divergence between plan documents, regulatory guidance, and AI-generated communications\n    - Limited ability to audit, explain, or reproduce how a specific output was generated\n  - This failure mode is increasingly visible in Medicare Advantage and other federal benefit programs where AI systems assist beneficiaries, providers, and administrative workflows\n\n- Data Transport vs. Semantic Determinism\n  - Existing health IT infrastructure has made substantial progress in data transport\n    - FHIR and related interoperability standards enable structured, standardized exchange of health information\n    - APIs and data services support real-time access to coverage, utilization, and clinical data\n  - However, these mechanisms are intentionally neutral with respect to semantic meaning\n    - They do not encode authoritative policy interpretation, versioned benefit definitions, or regulatory intent in a form that AI systems can deterministically retrieve and apply\n  - A deterministic semantic substrate should be understood not as a replacement for FHIR-based exchange, but as the semantic payload carried alongside it\n  - This distinction is increasingly relevant as agencies seek to satisfy emerging algorithmic transparency expectations, including those articulated in the CMS-0057-F Prior Authorization Final Rule\n\n- Policy Concept: Deterministic Semantic Substrate for Algorithmic Transparency\n  - HHS should recognize and encourage the use of a Deterministic Semantic Substrate for Algorithmic Transparency as a missing governance layer for non-device AI\n  - A deterministic semantic substrate is a structured, machine-readable memory layer embedded within authoritative content surfaces that enables AI systems to retrieve and apply versioned policy meaning deterministically rather than infer it probabilistically\n  - Key characteristics include:\n    - Explicit semantic structure (not inferred meaning)\n    - Stable identifiers and versioned definitions\n    - Provenance metadata tied to authoritative sources\n    - Cryptographically verifiable provenance (e.g., checksums or hashes) to support integrity and auditability\n    - Human-readable and machine-readable parity\n    - Independence from any specific AI model, vendor, or deployment architecture\n  - This approach shifts trust from probabilistic model behavior to content-level governance\n  - By anchoring AI outputs to verifiable, authoritative policy fragments, this approach reduces legal uncertainty by eliminating inconsistent interpretations that can lead to liability claims or regulatory non-compliance\n\n- Quantifiable Benefits for Program Integrity and Fiscal Stewardship\n  - Reduction of Appeals Friction and Administrative Adjudication Costs\n    - As of January 1, 2026, the Amount in Controversy thresholds for Medicare appeals have increased to $200 for ALJ hearings and $1,960 for judicial review\n    - Observed Problem: Probabilistic AI-generated explanations frequently trigger low-dollar, non-clinical appeals\n      - In many cases, the cost of adjudication exceeds the value of the underlying claim, creating negative administrative leverage\n      - These appeals are often driven not by coverage disputes, but by semantic inconsistency between AI explanations, plan materials, and statutory benefit definitions\n    - Impact: Reducing non-clinical, explanation-driven appeals by approximately 20–25% could yield meaningful annual savings—potentially on the order of hundreds of millions of dollars system-wide\n  - Suppression of Agentic Upcoding and Interpretive Drift in the WISeR Model\n    - The Wasteful and Inappropriate Service Reduction (WISeR) Model targets service categories with historically high improper payment risk\n      - Medicare spending on skin substitutes increased from approximately $256 million to over $10 billion in a five-year period\n    - Observed Problem: Technology participants operating AI-assisted utilization management systems are incentivized to optimize toward cost containment benchmarks\n      - This can result in interpretive drift in medical necessity criteria, incorrect service grouping, or overly aggressive denial logic\n    - Impact: Deterministic semantic fragments function as sovereign truth anchors, eliminating the \"interpretation delta\" between policy intent and automated enforcement\n      - Directly supports CMS efforts to protect the projected $19.6 billion in savings associated with 2026 Physician Fee Schedule and utilization reforms\n  - Lowering the Beneficiary Support \"Inquiry Tax\"\n    - CMS projects approximately $25 billion in increased Medicare Advantage payments for 2026\n    - Observed Problem: AI-generated misinformation drives secondary and tertiary contacts to 1-800-MEDICARE and plan call centers\n      - Contacts frequently involve correction of AI-generated explanations, manual verification of benefit rules, and tier-2 escalations\n    - Impact: Improving semantic determinism can reduce escalation rates by approximately 25–30% for explanation-driven inquiries\n      - Represents a deflationary shift in federal customer service costs while improving beneficiary experience and trust\n\n- Program Integrity and Anti-Spoofing Considerations\n  - Binding AI-generated explanations and determinations to authoritative benefit definitions reduces the risk of automated misrepresentation or opportunistic upcoding\n  - Deterministic semantic substrates function as anti-spoofing mechanisms for federal benefits\n\n- Alignment with Existing Regulatory Requirements\n  - Deterministic semantic substrates can support compliance with existing disclosure and accuracy requirements\n    - 42 CFR § 422.111 (Disclosure requirements)\n    - 42 CFR § 422.2267 (Required materials and content)\n  - Provides a technical mechanism to reinforce accuracy obligations already established under Subpart V without introducing new mandates\n\n- Policy Recommendations\n  - Recognize agentic drift as a distinct and addressable risk in non-device AI\n  - Identify deterministic semantic memory as a missing layer in current AI governance frameworks\n  - Encourage adoption of deterministic semantic substrates through guidance, pilots, and innovation models\n  - Support a federal reference implementation for a complex benefit program (e.g., Medicare Part D) to establish a baseline for auditable, machine-readable benefit truth\n  - Avoid conflating data access with semantic determinism when evaluating AI safety and accountability\n  - HHS could advance this approach through existing vehicles—such as the AI Governance Board, CMS sub-regulatory guidance, or limited voluntary demonstrations coordinated with CMMI—without requiring new statutory authority or model-specific regulation\n\n- Appendix A: Illustrative Example of Deterministic Semantic Memory\n  - Problem Scenario: AI Hallucination in Part D Cost Interpretation\n    - Medicare Part D includes statutory caps on beneficiary insulin cost-sharing\n    - Confusion arises when AI systems attempt to explain costs for combination insulin products, non-standard NDC groupings, and formularies with tier-specific pricing\n    - AI systems may hallucinate monthly insulin costs above the statutory cap, misapply caps across benefit phases, or confuse drug-level pricing with plan-level summaries\n  - Deterministic Semantic Approach\n    - Instead of asking an AI system to infer benefit rules from narrative text, a deterministic semantic substrate embeds machine-readable policy meaning directly within authoritative content surfaces\n    - The AI system retrieves the semantic fragment, applies the bound definition, and produces an explanation semantically locked to the authoritative rule\n  - Illustrative Embedded Semantic Fragment (YAML-in-HTML)\n    - Fragment is human-readable, machine-readable, versioned, provenance-bound, checksum-verifiable, and HTML embeddable\n    - Includes authoritative definition with rule text, statutory cap ($35.00), applies-to conditions, exclusions, binding information, provenance, and integrity checksum\n  - How This Prevents AI Hallucination\n    - A system consuming this fragment cannot exceed the defined cap, cannot misapply phase-based pricing, cannot hallucinate alternative interpretations, and can explicitly cite the bound definition\n    - The constraint is enforced at the system level—the AI system does not reason about policy meaning, it applies it\n    - Any output that diverges from the retrieved fragment can be deterministically detected and rejected\n  - Audit-by-Design Characteristics\n    - The semantic fragment is versioned, provenance is explicit, integrity is cryptographically verifiable, policy meaning is stable and reproducible\n    - Auditors can verify which definition was applied, which version was in effect, and whether the AI output conformed to the authoritative rule\n  - Compatibility with Retrieval-Augmented Generation (RAG)\n    - Fully compatible with RAG architectures\n    - Semantic fragments are designated as authoritative constraints, unlike general reference documents\n    - RAG enhances contextual richness while the semantic substrate ensures consistency, auditability, and resistance to hallucination\n\n- Appendix B: Illustrative Demonstration Concept for Provider Directory Accuracy\n  - Rationale for a Provider Directory Demonstration\n    - Provider directory accuracy is a persistent, well-understood challenge across Medicare Advantage and other managed care programs\n    - Inaccurate or inconsistent provider information is a leading cause of beneficiary confusion, call-center escalation, complaints and appeals, and compliance exposure\n    - Even small semantic inconsistencies (e.g., \"in-network\" vs. \"preferred\") generate disproportionate administrative cost\n    - Provider directories are an ideal low-stakes, high-signal environment to demonstrate deterministic semantic anchoring\n  - Illustrative Scope: Phoenix, Arizona\n    - A single metropolitan area with a limited set of credentialed providers and facilities\n    - One or more Medicare Advantage plans operating in the region\n    - A publicly accessible page on a participating organization's existing website\n    - The page would appear as a normal provider directory to human users while embedding a deterministic semantic anchor for AI systems\n  - Conceptual Technical Pattern (Semantic Anchor)\n    - Uses a dual-surface publishing pattern: human-readable content and machine-readable semantic anchor\n    - Illustrative YAML-in-HTML provider anchor includes anchor metadata (version, provenance, last verified, integrity hash) and provider directory entries with NPI, specialty, location, and network status\n  - AI Usage Pattern\n    - AI-enabled tools would retrieve the semantic anchor by ID, verify integrity via the hash, anchor responses to explicit is_in_network and tier fields, and generate human-language responses only after deterministic retrieval\n  - Audit-by-Design Characteristics\n    - The semantic anchor is versioned, provenance is explicit, integrity is cryptographically verifiable\n    - AI outputs can be compared directly to the published anchor\n    - If an AI system incorrectly states that a provider is out-of-network, auditors can simply compare the response against the integrity-verified anchor for that date\n    - No inference, no reconstruction, no model inspection required",
      "oneLineSummary": "An independent systems architect proposes a \"deterministic semantic substrate\" protocol that would anchor AI systems to versioned, machine-readable policy definitions, enabling audit-by-design oversight without model inspection or new regulatory regimes.",
      "commenterProfile": "- **Name/Organization:** David W. Bynon\n- **Type:** Individual\n- **Role/Expertise:** Independent Systems Architect with expertise in AI governance, semantic interoperability, and federal benefits systems\n- **Geographic Scope:** National (references Medicare, CMS, federal benefit programs)\n- **Stake in Issue:** Technical expert proposing a governance framework for non-device AI in healthcare; offers to brief HHS AI Governance Board or CMMI staff",
      "corePosition": "The core challenge HHS faces is not whether AI can be deployed, but how to scale its use without expanding federal operational burden or relying on opaque, non-scalable model-level oversight. I propose a deterministic semantic substrate that makes authoritative policy meaning explicit, versioned, and machine-retrievable—enabling AI outputs to be verified directly against published definitions without inspecting model internals. This approach converts AI governance from a reactive, probabilistic control problem into a preventive, content-governance discipline.",
      "keyRecommendations": "- Recognize agentic drift as a distinct and addressable risk in non-device AI\n  - AI systems diverge from authoritative policy intent when operating without persistent, verifiable semantic memory\n- Identify deterministic semantic memory as a missing layer in current AI governance frameworks\n  - Existing data transport standards (FHIR, APIs) are intentionally neutral with respect to semantic meaning\n- Encourage adoption of deterministic semantic substrates through guidance, pilots, and innovation models\n  - Can be explored through existing sub-regulatory guidance or voluntary innovation pilots\n- Support a federal reference implementation for a complex benefit program (e.g., Medicare Part D)\n  - Would establish a baseline for auditable, machine-readable benefit truth\n- Avoid conflating data access with semantic determinism when evaluating AI safety and accountability\n  - Data transport and semantic determinism serve different functions\n- Advance this approach through existing vehicles without new statutory authority\n  - AI Governance Board, CMS sub-regulatory guidance, or limited voluntary demonstrations coordinated with CMMI",
      "mainConcerns": "- Agentic drift in non-device AI systems\n  - Inconsistent interpretations of coverage and cost-sharing across AI systems\n  - Hallucinated or outdated benefit explanations\n  - Divergence between plan documents, regulatory guidance, and AI-generated communications\n  - Limited ability to audit, explain, or reproduce how a specific output was generated\n- Current approaches are insufficient\n  - Post-deployment monitoring, organizational governance, and improved data completeness still depend on a deterministic semantic layer\n  - FHIR and related standards do not encode authoritative policy interpretation in a form AI systems can deterministically retrieve\n- High-cost operational pressure points in Medicare\n  - Low-dollar, non-clinical appeals driven by semantic inconsistency between AI explanations and statutory benefit definitions\n  - Interpretive drift in medical necessity criteria and overly aggressive denial logic in AI-assisted utilization management\n  - AI-generated misinformation driving secondary and tertiary contacts to beneficiary support channels\n- Risk of automated misrepresentation or opportunistic upcoding\n  - AI systems can misbind beneficiary identity, coverage, or cost information in ways that advantage financial outcomes over regulatory accuracy",
      "notableExperiences": "- Identifies a specific failure mode called \"agentic drift\" that is distinct from data quality or model bias issues\n  - The problem is not that data is unavailable, but that authoritative semantic meaning is not deterministically retrievable\n- Proposes a \"dual-surface publishing pattern\" where content appears normal to humans while embedding machine-readable semantic anchors for AI systems\n- Frames the solution as \"deflationary\" rather than regulatory\n  - Reduces downstream reconciliation costs rather than adding compliance burden\n  - Aligns semantic responsibility with carriers, providers, and intermediaries where existing accuracy obligations already reside\n- Provides concrete cost estimates tied to specific CMS programs\n  - 20-25% reduction in non-clinical appeals could yield hundreds of millions in annual savings\n  - 25-30% reduction in escalation rates for explanation-driven inquiries\n  - References $19.6 billion in projected savings from 2026 Physician Fee Schedule reforms\n- Offers detailed technical implementation examples\n  - YAML-in-HTML semantic fragments with versioning, provenance, and cryptographic verification\n  - Compatible with existing RAG architectures without requiring model-level changes",
      "keyQuotations": "- \"The core challenge HHS faces is not whether AI can be deployed, but how to scale its use without expanding federal operational burden or relying on opaque, non-scalable model-level oversight.\"\n- \"Deterministic semantic fragments function as sovereign truth anchors, eliminating the 'interpretation delta' between policy intent and automated enforcement.\"\n- \"Effective AI governance in healthcare will not be achieved solely by constraining model behavior. It also requires strengthening the semantic signals those models consume.\""
    },
    "themeScores": {
      "1": 1,
      "2": 1,
      "3": 1,
      "5": 1,
      "9": 1,
      "2.1": 1,
      "5.4": 1,
      "5.5": 1,
      "5.7": 1,
      "9.1": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "AI Governance"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "AI Hallucination"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "RAG"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Provenance"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS Innovation Center"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "OMB"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Improper Payment"
      },
      {
        "category": "Fraud and Compliance",
        "label": "WISeR"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Prior Authorization"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Utilization Management"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Pilot Program"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "FHIR"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare Part D"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Physician"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "Physician Fee Schedule"
      }
    ],
    "hasAttachments": true,
    "wordCount": 6658,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0011",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "EHY Consulting LLC",
    "submitterType": "Organization",
    "date": "2026-01-12T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Opening statement\n  - This submission responds directly to each of the ten questions in the HHS Health Sector AI RFI\n  - Structured to support cross-program review where each response stands alone while referencing related questions where system-level dependencies exist\n  - Intent is not to recommend a single policy outcome but to surface implementation realities, governance gaps, and second-order effects when AI systems are deployed within real clinical workflows\n  - A workflow-to-entity crosswalk is included to clarify how responsibilities intersect across HHS StaffDivs and OpDivs to avoid gaps between policy intent and operational execution\n  - Consistent finding across AI deployments: adoption and safety are determined less by model accuracy in controlled settings than by how AI systems interact with real-world clinical workflows, governance structures, data stewardship practices, procurement decisions, and learning feedback loops\n  - Clinical AI functions as a socio-technical system that shapes clinician behavior, redistributes responsibility, and generates compounding learning from data produced through routine operations\n\n- HHS equities map (StaffDivs and OpDivs)\n  - Clinical AI adoption is inherently cross-cutting—the same AI-enabled workflow can engage multiple authorities and equities across HHS\n  - A practical way to avoid gaps is to map responsibilities to the workflow itself\n  - Workflow-to-entity crosswalk (illustrative):\n    - Clinical deployment and payment incentives\n      - Primary equities: CMS (payment policy, quality measures, demonstrations)\n      - Success looks like: Incentives reward measurable workflow value and outcomes, not superficial \"AI presence\"\n    - Health IT, interoperability, data standards\n      - Primary equities: ASTP/ONC (health IT standards, interoperability policy)\n      - Success looks like: Standards enable safe integration and auditability; interoperability does not imply unconstrained secondary use\n    - Privacy, patient rights, and enforcement\n      - Primary equities: OCR (HIPAA privacy/security enforcement and related guidance)\n      - Success looks like: Clear, enforceable boundaries on access, use, and disclosure; meaningful transparency and recourse\n    - Cybersecurity and national-risk posture\n      - Primary equities: CIO (enterprise security practices), ONS (national security risk and resilience processes)\n      - Success looks like: Security-by-design requirements are enforceable; AI is treated as part of critical clinical infrastructure\n    - International alignment, standards, and data-sharing regimes\n      - Primary equities: OGA (international engagement, standards, agreements)\n      - Success looks like: Policies reconcile domestic safeguards with cross-border standards and data-sharing realities\n    - Contract enforcement and vendor/subcontractor controls\n      - Primary equities: Office of Acquisition/contracting functions (across HHS)\n      - Success looks like: RFPs and contracts require auditability, learning controls, and secure operations; flow-down to subcontractors\n    - Safety of AI-enabled clinical products and decision support\n      - Primary equities: FDA (device/SaMD, CDS policy)\n      - Success looks like: Clear expectations for validation, change control, and post-market monitoring\n    - Evidence generation and implementation science\n      - Primary equities: AHRQ (evidence and outcomes research)\n      - Success looks like: Evaluation frameworks reflect real-world workflow effects and heterogeneous settings\n    - Public health integration (where AI crosses into surveillance)\n      - Primary equities: CDC\n      - Success looks like: Defined boundaries between clinical and public health uses; governance for secondary use\n    - Research and biomedical data ecosystems\n      - Primary equities: NIH\n      - Success looks like: Strong stewardship of clinical-research interfaces and reuse pathways\n    - Safety-net and underserved delivery settings\n      - Primary equities: HRSA, IHS, SAMHSA\n      - Success looks like: Adoption pathways account for resourcing constraints, equity, and operational realities\n  - Cross-cutting note: acquisition is the control plane that converts policy intent into enforceable obligations\n    - Interoperability, cloud use, vendor access, and model-update practices are ultimately determined by contract terms, monitoring rights, and operational controls\n\n- Cross-cutting analytic lens: data, workflows, and learning\n  - Practical governance test for clinical AI deployments:\n    - Use case and end-user clarity: identify the decision-maker, operator, and accountable party; specify the action taken because of the model output\n    - Data lifecycle boundaries: define what data enters, what leaves, retention periods, and what is used for care delivery versus model improvement\n    - Learning accumulation and sovereignty controls: specify whether operational data may be used for model improvement and under what constraints; treat derived artifacts (embeddings, fine-tunes, prompts, logs, telemetry) as governed outputs\n    - Security posture and access paths: enforce identity, least privilege, logging, incident response, and subcontractor controls; define cloud and remote access defaults with audit rights\n    - Bias introduction points and drift: identify where bias can enter (training data, workflow selection, labeling, clinician reliance, feedback loops) and require continuous monitoring and rollback\n  - Across multiple clinical domains (including precision oncology and advanced therapies), the most durable advantage—and most consequential vulnerability—often lies not in the AI model or the molecule, but in control over the learning system\n    - The workflow-generated data, outcomes, failure modes, and process metadata that enable iterative optimization over time\n  - Useful distinctions:\n    - Privacy: who can see my data\n    - Sovereignty: who determines what my data means and how much it is worth\n    - Security: who has access to my data and what they can do with it\n  - Many governance approaches address privacy and baseline security but do not explicitly govern learning accumulation (how data becomes compounding capability)\n    - In clinical AI, learning accumulation increasingly determines long-run trust, competitiveness, and resilience\n\n- Response to Question 1: Biggest barriers to private sector innovation and adoption/use in clinical care\n  - Clinical AI frequently crosses mismatched risk frameworks (privacy, cybersecurity, clinical safety, enterprise risk, vendor product risk)\n    - These frameworks do not align by default, and gaps emerge at their seams unless governance is explicitly integrated\n  - Workflow misfit is the dominant barrier\n    - AI systems that perform well in development can fail in practice when they add documentation burden, disrupt clinician cognitive flow, or create ambiguous responsibility at the point of care\n  - Key barriers include:\n    - Unclear accountability for AI-influenced decisions, especially when AI is adaptive or when vendor updates materially change behavior\n    - Trust gaps driven by opacity, inconsistent performance across subpopulations, and unclear post-deployment monitoring\n    - Data provenance and integration challenges (EHR heterogeneity, fragmented identifiers, incomplete documentation, variable labeling)\n    - Operational and security constraints (network segmentation, device integration, patch management, identity and access management)\n    - Procurement realities: contracts that do not specify auditability, model-update controls, data-use boundaries, or subcontractor constraints\n\n- Response to Question 2: Regulatory, payment policy, or programmatic design changes; CFR citations where applicable\n  - Condition participation or incentives on auditability (minimum logs, update provenance, and model-change notification)\n  - Make data-use boundaries enforceable, clarifying whether operational data may be used for model improvement and under what authorizations\n  - Most leverage comes from making governance and monitoring enforceable at the workflow level while avoiding incentives for superficial AI adoption\n  - Priority areas for HHS to consider:\n    - CMS-aligned incentives that reward measurable workflow value and outcomes rather than mere AI integration\n    - Programmatic designs that require lifecycle governance (monitoring, drift management, change control) as a condition of participation in demonstrations, quality programs, or certifications\n    - Clear, enforceable expectations for data stewardship, auditability, and vendor/subcontractor controls, implemented through acquisition and contracting requirements where feasible\n    - Interoperability policies that distinguish data exchange for care delivery from secondary use for model training and learning accumulation\n  - If HHS seeks specific CFR citations, a targeted follow-on request scoped to particular programs (e.g., Medicare conditions of participation, quality reporting, certification programs) would enable more precise citations by commenters\n\n- Response to Question 3: Non-medical devices: novel legal and implementation issues; HHS role\n  - Governance and contracts should explicitly cover workflow metadata and derived artifacts (prompts, embeddings, fine-tunes, telemetry), which often represent the primary signals of value and intent rather than mere exhaust\n  - For non-medical device AI used in clinical care (workflow, triage, documentation, scheduling, administrative decision support), key issues include liability allocation, indemnification, privacy/security obligations, and governance of adaptive behavior post-deployment\n  - Novel legal and implementation issues commonly arise from:\n    - Ambiguity in responsibility when AI influences clinician behavior or care pathways without being a regulated medical device\n    - Contracting and indemnification gaps that fail to define change control, performance representations, or audit rights\n    - Privacy/security frameworks that govern access and disclosure but do not explicitly govern learning accumulation or model improvement using operational clinical data\n    - Third-party and subcontractor risk (cloud hosting, analytics, support), especially when access paths are opaque\n  - HHS can help by publishing model contract clauses and minimum auditability/lifecycle governance expectations usable by providers and payers, aligned across OCR, ASTP/ONC, CIO/ONS, and CMS\n\n- Response to Question 4: Non-medical devices: evaluation methods pre- and post-deployment; mechanisms\n  - Evaluate impact on clinician behavior, including automation bias, alert fatigue, de-skilling, and reliance shifts over time\n  - Most promising evaluation approaches combine pre-deployment validation with post-deployment monitoring sensitive to workflow effects, site-to-site variation, and distribution shift\n  - Recommended evaluation components:\n    - Human-centered workflow evaluation (cognitive load, alert fatigue, override rates, time-to-task, downstream workflow changes)\n    - Robustness and stress testing across settings, subpopulations, and data quality conditions\n    - Post-deployment drift detection and performance monitoring linked to outcomes where feasible\n    - Change-control governance for model updates, including criteria for rollback\n  - HHS can support these via contracts, grants, cooperative agreements, and prize competitions (often with AHRQ/NIH/CMS partners), prioritizing monitoring, auditability, and workflow-aware evaluation methods\n\n- Response to Question 5: Support for accreditation, certification, testing, and credentialing\n  - Establish a minimum governance checklist for certification programs, including update control, monitoring, disclosure of data use for training, and incident response\n  - HHS can amplify private-sector accreditation, certification, industry-driven testing, and credentialing by helping establish baseline expectations for trustworthy clinical AI that are measurable and auditable\n  - Support could include:\n    - Reference expectations for auditability (logging, traceability of inputs/outputs, update history) and minimum cybersecurity posture for clinical AI deployments\n    - Credentialing frameworks for AI governance roles within health care organizations\n    - Model contract language and procurement checklists that incorporate security-by-design and lifecycle monitoring requirements\n    - Public-private benchmarking and validation resources, including shared evaluation protocols and testbeds\n\n- Response to Question 6: Where AI met/exceeded vs fell short; novel tools with greatest potential\n  - Distinguish value created for patients and providers from value transferred to vendors through learning accumulation from operational data\n  - AI tends to meet or exceed expectations when tightly coupled to well-defined workflows with clear success metrics (reducing administrative burden, improving triage consistency, accelerating access, reducing preventable harm)\n  - AI tends to fall short when deployed as a generic overlay without governance, monitoring, or integration into decision rights\n  - High-potential tool categories:\n    - Workflow-aware clinical documentation support that measurably reduces administrative burden without introducing silent errors\n    - Risk stratification and triage tools with clear accountability and monitoring, especially in resource-constrained settings\n    - Tools that surface quality insights by analyzing workflow metadata and outcomes (not only clinical text)\n    - Patient-facing navigation and care coordination supports that improve adherence and reduce friction across care settings\n  - To accelerate learning on \"what works,\" HHS can support standardized reporting and privacy-preserving benchmarking\n\n- Response to Question 7: Organizational roles and administrative hurdles for adoption\n  - Adoption decisions are typically shaped by clinical leadership, operational leadership, compliance/privacy, IT/security leadership, and (increasingly) AI governance committees or enterprise risk management functions\n  - Primary administrative hurdles:\n    - Unclear ownership for AI lifecycle management and monitoring across clinical, IT, and compliance functions\n    - Procurement bottlenecks and contract limitations that impede auditability or change control\n    - Data access and interoperability friction across EHRs and ancillary systems\n    - Training, credentialing, and change management requirements for clinicians and staff\n    - Security approvals and network integration constraints, especially where tools require cloud connectivity\n  - This is why HHS-wide coordination across CMS, ASTP/ONC, OCR, CIO/ONS, OGA, and acquisition functions is essential\n\n- Response to Question 8: Interoperability opportunities; data types, standards, and benchmarking tools\n  - Interoperability expands the attack surface and secondary-use risk unless paired with access control, provenance, and enforceable training boundaries\n  - Enhanced interoperability can widen market opportunities and accelerate AI where it improves data provenance, reduces integration friction, and enables benchmarking—while still respecting governance boundaries on secondary use and learning accumulation\n  - Priority opportunities:\n    - Standardized representations of clinical workflows and outcomes (including provenance/lineage metadata)\n    - Consistent patient identity and record linkage across care settings\n    - Data standards and benchmarking tools that support monitoring and post-deployment evaluation\n    - Mechanisms for privacy-preserving benchmarking and federated evaluation where direct pooling is impractical\n  - Interoperability policy should explicitly distinguish exchange for care delivery from secondary use for model training and learning accumulation\n\n- Response to Question 9: Patient/caregiver desired challenges and concerns\n  - Patients increasingly want clarity not only on who sees their data, but who benefits from derived learning and what recourse exists\n  - Patients and caregivers often prioritize improvements that reduce friction, shorten wait times, improve communication, and increase transparency about decisions affecting care access and quality\n  - Common concerns:\n    - Privacy and security of sensitive health information, including secondary uses beyond direct care\n    - Unclear accountability when AI influences clinical decisions or access to services\n    - Bias and inequitable treatment, especially when monitoring and recourse mechanisms are weak\n    - Reduced human agency or difficulty obtaining explanations, corrections, and escalation pathways\n  - These concerns reinforce the need for lifecycle governance, auditability, and clear decision rights\n\n- Response to Question 10: AI research priorities; literature on impact and costs/benefits/transfers\n  - Prioritize methods to measure learning accumulation and value transfer across stakeholders, and governance models that integrate privacy, sovereignty, and security operationally\n  - HHS research priorities that would accelerate responsible adoption include implementation science for clinical AI, workflow-aware evaluation methods, and scalable approaches to monitoring and governance\n  - Priority research areas:\n    - Methods for post-deployment monitoring, drift detection, and change control in heterogeneous clinical settings\n    - Human-centered evaluation methods that quantify workflow impact and clinician reliance over time\n    - Privacy-preserving benchmarking, federated evaluation, and secure data-sharing architectures\n    - Governance models for adaptive systems that integrate privacy, sovereignty, and security considerations\n  - HHS can help curate and translate literature into practical evaluation playbooks and clarify how costs/benefits/value transfer are measured across providers, payers, vendors, and patients\n\n- Closing\n  - EHY Consulting appreciates the opportunity to contribute and supports HHS's efforts to advance responsible, effective, and trustworthy adoption of AI in clinical care\n  - Comments are intended to be practical and implementable, highlighting workflow-level realities and second-order effects so that accelerated adoption does not outpace governance of safety, trust, and resilience\n  - Welcomes continued dialogue to clarify implementation challenges and governance considerations as policy and practice evolve\n\n---",
      "oneLineSummary": "A national security and AI governance consultant argues that clinical AI adoption hinges less on algorithmic performance than on workflow integration, lifecycle governance, and enforceable data stewardship—with acquisition and contracting serving as the primary control plane for making policy intent operational.\n\n---",
      "commenterProfile": "- **Name/Organization:** EHY Consulting LLC (Edward You)\n- **Type:** Consulting/Advisory\n- **Role/Expertise:** AI governance, national security, clinical AI implementation; background suggests expertise in biosecurity and technology policy\n- **Geographic Scope:** National\n- **Stake in Issue:** Advises on AI governance and implementation; positioned to help organizations navigate the regulatory and operational challenges this RFI addresses\n\n---",
      "corePosition": "Clinical AI is a socio-technical system that reshapes workflows, redistributes accountability, and generates compounding learning from operational data—governance approaches that focus only on privacy or pre-deployment validation are insufficient. The most effective lever HHS has is acquisition and contracting, which converts policy intent into enforceable obligations. We need to govern not just who sees data, but who controls the learning that accumulates from it.\n\n---",
      "keyRecommendations": "- Condition participation or incentives on auditability\n  - Require minimum logs, update provenance, and model-change notification\n  - Make data-use boundaries enforceable with clear authorizations for model improvement\n\n- Publish model contract clauses and minimum governance expectations\n  - Usable by providers and payers\n  - Aligned across OCR, ASTP/ONC, CIO/ONS, and CMS\n  - Include auditability, learning controls, secure operations, and subcontractor flow-down requirements\n\n- Establish minimum governance checklist for certification programs\n  - Update control and monitoring requirements\n  - Disclosure of data use for training\n  - Incident response protocols\n\n- Require lifecycle governance as condition of participation\n  - Monitoring, drift management, and change control for demonstrations, quality programs, or certifications\n\n- Distinguish interoperability for care delivery from secondary use for model training\n  - Interoperability policy should explicitly address learning accumulation boundaries\n\n- Support human-centered evaluation methods\n  - Fund via contracts, grants, cooperative agreements, and prize competitions\n  - Prioritize workflow-aware evaluation, monitoring, and auditability\n\n- Create credentialing frameworks for AI governance roles within healthcare organizations\n\n- Develop privacy-preserving benchmarking and federated evaluation mechanisms\n\n---",
      "mainConcerns": "- Governance frameworks are misaligned and create gaps\n  - Privacy, cybersecurity, clinical safety, enterprise risk, and vendor product risk frameworks don't align by default\n  - Gaps emerge at the seams unless governance is explicitly integrated\n\n- Workflow misfit is the dominant barrier to adoption\n  - AI that performs well in development fails when it adds documentation burden, disrupts cognitive flow, or creates ambiguous responsibility\n\n- Learning accumulation is ungoverned\n  - Many approaches address privacy and security but not how data becomes compounding capability\n  - Derived artifacts (embeddings, fine-tunes, prompts, logs, telemetry) are often treated as exhaust rather than governed outputs\n\n- Procurement and contracting gaps\n  - Contracts often don't specify auditability, model-update controls, data-use boundaries, or subcontractor constraints\n  - Third-party and subcontractor risk (cloud hosting, analytics, support) is often opaque\n\n- Post-deployment risks are underaddressed\n  - Vendor updates can materially change AI behavior without clear accountability\n  - Drift detection and rollback criteria are often undefined\n\n- Interoperability expands attack surface\n  - Enhanced data exchange increases secondary-use risk unless paired with access control, provenance, and enforceable training boundaries\n\n- Patient concerns about data sovereignty\n  - Patients want clarity not just on who sees their data, but who benefits from derived learning and what recourse exists\n\n---",
      "notableExperiences": "- Reframes the core governance challenge as three distinct concepts most frameworks conflate:\n  - Privacy (who can see my data)\n  - Sovereignty (who determines what my data means and how much it is worth)\n  - Security (who has access to my data and what they can do with it)\n\n- Identifies acquisition and contracting as the \"control plane\" that converts policy intent into enforceable obligations—a practical insight that cuts through regulatory complexity\n\n- Provides a detailed workflow-to-entity crosswalk mapping specific HHS divisions to clinical AI touchpoints, offering a practical coordination framework\n\n- Argues that in precision oncology and advanced therapies, the most consequential vulnerability lies not in the AI model or molecule, but in control over the learning system—the workflow-generated data, outcomes, and process metadata\n\n- Suggests treating workflow metadata and derived artifacts (prompts, embeddings, fine-tunes, telemetry) as \"primary signals of value and intent rather than mere exhaust\"\n\n- Offers a practical governance test with five specific checkpoints for any clinical AI deployment\n\n---",
      "keyQuotations": "- \"Adoption and safety are determined less by model accuracy in controlled settings than by how AI systems interact with real-world clinical workflows, governance structures, data stewardship practices, procurement decisions, and learning feedback loops.\"\n\n- \"The most durable advantage—and the most consequential vulnerability—often lies not in the AI model or the molecule, but in control over the learning system: the workflow-generated data, outcomes, failure modes, and process metadata that enable iterative optimization over time.\"\n\n- \"Privacy: who can see my data. Sovereignty: who determines what my data means and how much it is worth. Security: who has access to my data and what they can do with it.\""
    },
    "themeScores": {
      "1": 1,
      "2": 1,
      "3": 1,
      "4": 1,
      "5": 1,
      "6": 1,
      "7": 1,
      "8": 1,
      "9": 1,
      "1.6": 1,
      "2.1": 1,
      "2.4": 1,
      "3.2": 1,
      "6.1": 1,
      "6.3": 1,
      "6.8": 1,
      "7.3": 1,
      "7.4": 1,
      "9.1": 1,
      "9.3": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "AI Governance"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Post-deployment Monitoring"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Pre-deployment Evaluation"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Robustness Testing"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Algorithmic Bias"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Model Drift"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "SaMD"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Accreditation"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Certification"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Conditions of Participation"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Care Coordination"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Treatment"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Triage"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Oncology"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Cybersecurity"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Provenance"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "AHRQ"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CDC"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "FDA"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "HRSA"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "IHS"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "NIH"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "OCR"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "ONC"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "SAMHSA"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Health Information Systems",
        "label": "CDS"
      },
      {
        "category": "Health Information Systems",
        "label": "EHR"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Credentialing"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Quality Reporting"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Administrative Burden"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Change Management"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Procurement"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Alert Fatigue"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Equity"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "CIO"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Laws and Regulations",
        "label": "HIPAA"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Indemnification"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Cancer"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "Cooperative Agreement"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "Implementation Science"
      },
      {
        "category": "Social Determinants of Health",
        "label": "Underserved Population"
      }
    ],
    "hasAttachments": true,
    "wordCount": 2610,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0012",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Sancian LLC",
    "submitterType": "Organization",
    "date": "2026-01-13T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- SANCIAN LLC submits response to HHS's Request for Information on accelerating AI adoption in clinical care\n  - Drawing on over 15 years of experience implementing AI governance across federal health agencies including HHS, CDC, FDA, and NIH\n  - Offers operational framework for translating AI policy into sustainable clinical practice\n\n- Core Position\n  - HHS should treat AI not as technology to be adopted, but as care infrastructure to be governed\n  - The gap between AI potential and AI reality is not a technology problem—it is a governance, implementation, and human factors problem\n\n- Five Principles for Acceleration\n  - Governance-first adoption\n    - Establish accountability structures before technology procurement\n    - Abstract frameworks fail at the point of care; organizations need actionable playbooks\n  - Readiness before reimbursement\n    - Require organizations to demonstrate governance maturity as a prerequisite for AI-related payment incentives\n    - Premature adoption without readiness wastes resources and erodes trust\n  - Payment aligned to burden reduction\n    - Incentivize AI that saves clinician time, reduces documentation burden, and improves access\n    - Not simply reward adoption of new technology\n  - Implementation science over model development\n    - Fund research on real-world deployment, workflow integration, and adoption drivers\n    - Not just algorithmic performance in controlled settings\n  - Human capacity as a success metric\n    - Measure cognitive load reduction, time returned to patient care, and clinician trust\n    - AI that adds burden—even if accurate—fails the people it aims to serve\n\n- Why Now\n  - Accelerating federal AI strategies, workforce strain, and rising care complexity create narrow 24–36 month window\n  - Must embed governance and human-centered design before AI becomes irreversible infrastructure\n  - Rural, safety-net, and underserved settings face greatest risk of being left behind or harmed if equity not embedded from start\n\n- About SANCIAN LLC\n  - Evidence-based AI governance and implementation advisory focused on public health, health systems, and regulated environments\n  - Combines validated frameworks including NIST AI RMF, ISO/IEC 23894, PMBOK, Kotter, and ADKAR\n  - Operational tooling for readiness assessment, equity evaluation, and sustainable adoption\n  - Founder has doctoral-level expertise in public health and clinical management\n  - Operating model: Readiness → Selection → Implementation → Governance → Continuous Improvement\n  - AI SoftLife™ Philosophy: Design AI as capacity infrastructure that reduces cognitive burden, protects clinicians' energy, and sustains humane care delivery\n\n- Question 1: Barriers to Private Sector Innovation and Adoption\n  - Fragmented governance and unclear accountability\n    - Conflicting guidance across HHS divisions\n    - No clarity on which agency's rules apply, particularly for non-device AI outside FDA's traditional scope\n    - Uncertainty delays procurement and suppresses investment\n  - Low organizational readiness\n    - Gaps extend beyond technology to data governance, workflow optimization, change management, and leadership alignment\n    - Organizations underestimate transformation required, leading to failed implementations that erode confidence\n  - Trust gaps across stakeholders\n    - Clinicians, patients, and administrators have legitimate concerns about bias, safety, explainability, and liability\n    - Current deployments have not adequately addressed these concerns\n  - Clinician burnout and workflow friction\n    - AI that adds steps or cognitive load creates friction rather than relief regardless of technical performance\n    - Exhausted care teams cannot absorb poorly designed tools\n  - Procurement complexity and reimbursement uncertainty\n    - Traditional processes add 6–18 months to adoption timelines\n    - Unclear ROI projections make business cases difficult to build\n  - SANCIAN addresses barriers through AI ReadyCheck™, governance-first design, workflow co-design with clinician input, and AI-EQUITYClear™ equity impact assessment\n  - HHS Opportunity: Standardize readiness and governance expectations across programs; encourage \"readiness before reimbursement\"\n\n- Question 2: Regulatory, Payment Policy, and Programmatic Changes\n  - Regulatory Recommendations\n    - Create tiered, risk-based AI governance pathway aligned with NIST AI RMF risk tiers\n      - Lower-risk administrative AI could follow streamlined pathways\n      - Higher-risk clinical decision support would require rigorous evaluation\n      - Provides predictability while maintaining proportionate oversight\n    - Issue guidance for non-device clinical AI accountability\n      - Publish model accountability frameworks addressing roles, responsibilities, and oversight\n      - Relevant touchpoints: ONC's Health IT Certification Program (45 CFR Part 170), Conditions of Participation (42 CFR Parts 482–485)\n    - Enable pilots with real-world evidence requirements\n      - Structured pilots integrating AI into care pathways with mandatory evidence collection\n  - Reimbursement Recommendations\n    - CMS should incentivize AI that delivers measurable value\n    - Clinician time saved—reward AI that reduces documentation and administrative burden\n    - Improved access and continuity—support AI that expands access to underserved and rural populations\n    - Value-based AI add-ons—enable payment adjustments under MSSP, ACO REACH, and bundled payments when AI contributes to outcomes\n    - Current fee schedule (42 CFR Part 414) requires new CPT codes or modifiers for AI-assisted services\n\n- Question 3: Novel Legal and Implementation Issues\n  - Key Issues\n    - Shared liability—allocation among AI developers, healthcare organizations, and clinicians remains unclear\n      - Traditional malpractice frameworks assume human decision-makers; AI disrupts these assumptions\n    - Auditability of adaptive models\n      - Organizations need mechanisms to audit model behavior, track changes, and demonstrate deployed models match validated versions\n    - Data provenance, consent, and security\n      - Questions persist regarding training data sources, patient consent, and AI-specific cybersecurity vulnerabilities\n      - HIPAA (45 CFR Parts 160, 164) requires interpretation for AI contexts\n    - Indemnification gaps\n      - Standard vendor contracts do not adequately address AI-specific risks including performance guarantees, bias warranties, and monitoring obligations\n  - SANCIAN implements role-based accountability matrices, continuous monitoring with model risk logs, and governance charters embedded in operations\n  - HHS Role: Convene cross-sector standards development for liability and auditability; publish reference governance architectures; provide model contract provisions or safe harbor guidance\n\n- Question 4: AI Evaluation Methods\n  - What Works\n    - Pre-deployment: Bias testing across demographic subgroups, scenario-based validation, usability testing, \"shadow mode\" deployment comparing AI recommendations to actual clinical decisions\n    - Post-deployment: Outcome drift monitoring, safety signal surveillance, human override tracking, periodic re-validation\n    - Workflow impact: Traditional accuracy metrics are necessary but insufficient; clinical AI must be evaluated on operational integration\n  - AI SoftLife™ Metrics\n    - Cognitive load reduction—does the AI decrease mental effort?\n    - Time returned to patient care—does the AI free clinicians for presence?\n    - Trust and adoption—do clinicians actually use and trust the AI?\n    - Structured equity evaluation using AI-EQUITYClear™ to assess algorithmic fairness across demographic subgroups\n  - HHS Support: Fund applied evaluation frameworks; support shared testbeds and benchmark datasets particularly for rural and safety-net settings; public-private partnerships could validate methodologies\n\n- Question 5: Supporting Private Sector Activities\n  - The Need\n    - Market needs third-party validation of governance maturity, bias and safety controls, and operational integration\n    - Healthcare organizations lack reliable signals to distinguish quality AI products from marketing\n  - Proposal: AI in Clinical Care Maturity Model\n    - Standardized assessment frameworks for organizations (readiness to adopt, implement, and govern AI responsibly)\n    - Standardized assessment frameworks for vendors (product quality, transparency, safety, and governance features)\n    - Standardized assessment frameworks for care pathways (effectiveness of AI integration into specific clinical workflows)\n  - Additional Mechanisms\n    - Recognize private standards from NIST, IEEE, and healthcare bodies by reference\n    - Provide regulatory safe harbors for certified AI\n    - Fund workforce credentialing through HRSA for AI competencies\n\n- Question 6: AI Performance and Opportunities\n  - Where AI Works\n    - Documentation automation (ambient intelligence, AI-assisted notes)\n    - Imaging triage (radiology, pathology screening)\n    - Risk stratification (sepsis prediction, early warning)\n    - Population health analytics (care gap identification, resource allocation)\n  - Where AI Falls Short\n    - Workflow misalignment creates friction\n    - Alert fatigue leads to override of valid recommendations\n    - Black-box decisioning undermines trust\n    - Equity gaps from non-representative training data worsen disparities in underserved populations\n  - High-Potential Applications\n    - Care navigation (guiding patients through complex journeys)\n    - Social determinants integration (connecting patients to community resources)\n    - Chronic disease coaching (personalized self-management)\n    - Team-based decision support\n    - Frailty/dementia care for aging populations\n  - AI should support presence, not replace judgment; highest-value applications restore clinician capacity to be fully present with patients\n\n- Question 7: Decision-Makers and Administrative Hurdles\n  - The Reality\n    - Multiple stakeholders hold effective veto power: CIO/CTO (infrastructure), CMIO/CMO (clinical safety), Compliance/Legal (regulatory exposure), CFO (budget)\n    - Result: no single owner of AI outcomes; decisions stall; accountability diffuses; promising initiatives die in committee\n  - Primary Hurdles\n    - Procurement and IT security cycles add 6–18 months\n    - Absence of governance structures forces ad hoc decisions\n    - Medical staff bylaws rarely address AI\n    - Legal review delays from lack of AI expertise\n    - Change management underinvestment undermines adoption\n  - SANCIAN Solution\n    - Named AI Executive Owner model—single accountable leader with cross-functional authority\n    - Cross-functional AI governance councils—formalized structures with defined decision rights\n    - Decision clarity before tooling—governance infrastructure before product evaluation\n  - HHS should encourage accountable leadership models in guidance and grants, including Conditions of Participation updates requiring AI oversight designation\n\n- Question 8: Interoperability for AI Development\n  - Priority Data Types\n    - FHIR-based clinical data with bulk access for population AI\n    - Social determinants of health (currently inconsistent)\n    - Patient-reported outcomes captured outside encounters\n    - Imaging and genomic data for precision medicine applications\n  - Standards and Benchmarking\n    - Model cards and audit metadata for standardized AI documentation\n    - Benchmark datasets for fairness representing diverse populations\n    - Provenance tracking for data lineage and model versioning\n  - Interoperability must include governance data, not just clinical data\n    - Standards for AI oversight artifacts—audit logs, performance results, bias assessments—would enable consistent governance and regulatory oversight\n  - HHS should fund interoperability for AI oversight artifacts and require transparency standards in funded programs; USCDI expansion should prioritize AI-relevant data elements\n\n- Question 9: Patient and Caregiver Perspectives\n  - What Patients Want\n    - Access (timely appointments, expanded availability)\n    - Continuity (coordinated care across providers)\n    - Clarity (understandable information, timely results)\n    - Less friction (reduced paperwork, streamlined processes)\n    - Personalized but fair care (tailored treatment without discrimination)\n  - Patient Concerns\n    - Bias and exclusion (fear of discrimination)\n    - Loss of human connection (AI replacing empathy)\n    - Data misuse (uncertainty about privacy)\n    - Lack of recourse (unclear accountability for harm)\n    - Accessibility (barriers for those with disabilities or limited digital access)\n  - Human-in-the-loop by design; transparency as trust infrastructure; equity checks embedded early\n  - HHS should make patient trust a core success metric—not a soft outcome, but the foundation of sustainable adoption\n\n- Question 10: AI Research Priorities\n  - High-Impact Research Areas\n    - Implementation science in real clinics—how to deploy effectively, including change management and adoption drivers\n    - Equity and bias mitigation in deployment—detecting and addressing bias as AI operates in practice\n    - Human-AI teaming—optimal collaboration patterns and trust calibration\n    - Safety and drift monitoring—detecting failures and ensuring robustness across contexts\n    - Economic impact of burden reduction—effects on clinician time, costs, and workforce sustainability\n  - Literature Gap\n    - Plenty on models, not enough on adoption, outcomes, and workflow impact\n    - Most studies evaluate controlled settings; evidence on real-world implementation is thin\n    - Rural and safety-net settings are particularly underrepresented\n  - HHS Opportunity: Fund longitudinal pilots tied to outcomes and workforce wellbeing; support public-private learning collaboratives for sharing implementation lessons including failures\n\n- Closing Position\n  - HHS should treat AI not just as technology, but as care infrastructure—governed, evaluated, reimbursed, and funded to:\n    - Improve outcomes for patients across all populations, including rural and safety-net settings\n    - Restore clinician capacity by reducing burden rather than adding it\n    - Advance equity by embedding fairness from design through deployment\n    - Build durable public trust through transparency, accountability, and human-centered design\n  - Adoption fails when governance is abstract and care teams are exhausted\n  - AI succeeds when designed for clarity, accountability, and human ease\n  - Accelerate AI by making it operationally safe, economically viable, and emotionally sustainable for the people who deliver and receive care\n  - Next 24–36 months represent a narrow window",
      "oneLineSummary": "An AI governance consultancy with 15 years of federal health agency experience argues HHS should treat AI as \"care infrastructure to be governed\" rather than technology to be adopted, offering operational frameworks emphasizing governance-first adoption, readiness before reimbursement, and human capacity as the ultimate success metric.",
      "commenterProfile": "- **Name/Organization:** SANCIAN LLC\n- **Type:** Business\n- **Role/Expertise:** AI governance and implementation advisory specializing in public health, health systems, and regulated environments; founder has doctoral-level expertise in public health and clinical management; 15+ years experience with HHS, CDC, FDA, and NIH\n- **Geographic Scope:** National, based in Atlanta, GA\n- **Stake in Issue:** Provides AI governance consulting services to federal health agencies and health systems; offers proprietary frameworks (AI ReadyCheck™, AI-EQUITYClear™, AI SoftLife™) that could be adopted or referenced in HHS guidance",
      "corePosition": "HHS should treat AI not as technology to be adopted, but as care infrastructure to be governed. The gap between AI potential and AI reality is not a technology problem—it is a governance, implementation, and human factors problem. The next 24–36 months represent a narrow window to embed governance and human-centered design before AI becomes irreversible infrastructure, with rural, safety-net, and underserved settings facing the greatest risk if equity is not embedded from the start.",
      "keyRecommendations": "- Establish governance-first adoption requiring accountability structures before technology procurement\n  - Organizations need actionable playbooks, not abstract frameworks\n  - Require named AI Executive Owner with cross-functional authority\n  - Update Conditions of Participation to require AI oversight designation\n\n- Implement \"readiness before reimbursement\" policy\n  - Require organizations to demonstrate governance maturity as prerequisite for AI-related payment incentives\n  - Standardize readiness and governance expectations across HHS programs\n\n- Align payment to burden reduction rather than technology adoption\n  - Reward AI that saves clinician time and reduces documentation burden\n  - Support AI that expands access to underserved and rural populations\n  - Enable value-based AI add-ons under MSSP, ACO REACH, and bundled payments\n  - Create new CPT codes or modifiers for AI-assisted services\n\n- Create tiered, risk-based AI governance pathway aligned with NIST AI RMF\n  - Streamlined pathways for lower-risk administrative AI\n  - Rigorous evaluation for higher-risk clinical decision support\n  - Issue guidance for non-device clinical AI accountability\n\n- Fund implementation science over model development\n  - Prioritize research on real-world deployment, workflow integration, and adoption drivers\n  - Support longitudinal pilots tied to outcomes and workforce wellbeing\n  - Fund public-private learning collaboratives including sharing of failures\n\n- Develop AI in Clinical Care Maturity Model\n  - Standardized assessment frameworks for organizations, vendors, and care pathways\n  - Recognize private standards from NIST, IEEE, and healthcare bodies\n  - Provide regulatory safe harbors for certified AI\n\n- Expand interoperability to include AI governance data\n  - Standards for AI oversight artifacts: audit logs, performance results, bias assessments\n  - USCDI expansion should prioritize AI-relevant data elements\n  - Model cards and audit metadata for standardized AI documentation\n\n- Address legal and liability gaps\n  - Convene cross-sector standards development for liability and auditability\n  - Publish reference governance architectures\n  - Provide model contract provisions or safe harbor guidance",
      "mainConcerns": "- Fragmented governance creates adoption paralysis\n  - Conflicting guidance across HHS divisions with no clarity on which agency's rules apply\n  - Particularly problematic for non-device AI outside FDA's traditional scope\n  - Uncertainty delays procurement and suppresses investment\n\n- Organizations lack readiness for AI adoption\n  - Gaps extend beyond technology to data governance, workflow optimization, change management, and leadership alignment\n  - Failed implementations erode confidence in future AI initiatives\n  - Premature adoption without readiness wastes resources and erodes trust\n\n- AI that adds burden fails regardless of accuracy\n  - Exhausted care teams cannot absorb poorly designed tools\n  - Alert fatigue leads to override of valid recommendations\n  - Workflow misalignment creates friction rather than relief\n\n- Accountability structures are absent or diffuse\n  - Multiple stakeholders hold effective veto power (CIO, CMIO, Compliance, CFO)\n  - No single owner of AI outcomes; decisions stall; promising initiatives die in committee\n  - Medical staff bylaws rarely address AI\n\n- Liability and auditability remain unresolved\n  - Traditional malpractice frameworks assume human decision-makers\n  - Standard vendor contracts don't address AI-specific risks\n  - Organizations lack mechanisms to audit adaptive model behavior\n\n- Equity gaps threaten underserved populations\n  - Non-representative training data worsens disparities\n  - Rural and safety-net settings face greatest risk of being left behind or harmed\n  - Evidence on real-world implementation in these settings is thin\n\n- Research priorities are misaligned\n  - Plenty of research on models, not enough on adoption, outcomes, and workflow impact\n  - Most studies evaluate controlled settings rather than real-world implementation",
      "notableExperiences": "- Identified the \"governance-first\" paradox: abstract frameworks fail at the point of care, yet organizations keep trying to adopt AI before establishing accountability structures\n- Proposed measuring AI success by \"time returned to patient care\" and \"cognitive load reduction\" rather than traditional accuracy metrics—reframing AI as capacity infrastructure rather than decision tool\n- Observed that procurement and IT security cycles add 6–18 months to AI adoption timelines, creating a structural barrier that policy changes alone cannot address\n- Articulated the \"multiple veto\" problem: CIO, CMIO, Compliance, and CFO each hold effective veto power over AI adoption, resulting in no single owner and promising initiatives dying in committee\n- Proposed interoperability standards for AI governance artifacts (audit logs, bias assessments, performance results)—not just clinical data—as essential infrastructure for consistent oversight\n- Framed the next 24–36 months as a \"narrow window\" before AI becomes irreversible infrastructure, creating urgency for embedding governance and equity now",
      "keyQuotations": "- \"HHS should treat AI not as technology to be adopted, but as care infrastructure to be governed.\"\n\n- \"AI that adds burden—even if accurate—fails the people it aims to serve.\"\n\n- \"Adoption fails when governance is abstract and care teams are exhausted. AI succeeds when it is designed for clarity, accountability, and human ease.\"\n\n- \"The gap between AI potential and AI reality is not a technology problem—it is a governance, implementation, and human factors problem.\""
    },
    "themeScores": {
      "1": 1,
      "2": 1,
      "3": 1,
      "4": 1,
      "5": 1,
      "6": 1,
      "7": 1,
      "8": 1,
      "9": 1,
      "1.1": 1,
      "1.4": 1,
      "2.1": 1,
      "2.3": 1,
      "3.1": 1,
      "3.2": 1,
      "4.2": 1,
      "4.3": 1,
      "4.6": 1,
      "6.1": 1,
      "6.2": 1,
      "6.3": 1,
      "6.5": 1,
      "6.8": 1,
      "7.4": 1,
      "8.1": 1,
      "8.2": 1,
      "8.3": 1,
      "8.5": 1,
      "9.1": 1,
      "9.3": 1
    },
    "entities": [
      {
        "category": "AI Applications in Healthcare",
        "label": "Ambient AI"
      },
      {
        "category": "AI Applications in Healthcare",
        "label": "Early Warning System"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "NIST AI RMF"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Post-deployment Monitoring"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Safe Harbor"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Shadow Mode"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Algorithmic Bias"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Explainable AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Model Drift"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Certification"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Conditions of Participation"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Informed Consent"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Screening"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Genomics"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Pathology"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Precision Medicine"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Radiology"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Governance"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Provenance"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "HRSA"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "NIST"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "ONC"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Health Information Systems",
        "label": "CDS"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Pilot Program"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Procurement"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "ROI"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Workflow Integration"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "CPT"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "FHIR"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "USCDI"
      },
      {
        "category": "Healthcare Programs",
        "label": "ACO"
      },
      {
        "category": "Healthcare Programs",
        "label": "ACO REACH"
      },
      {
        "category": "Healthcare Programs",
        "label": "MSSP"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Burnout"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Disparities"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Equity"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Health System"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Safety-Net Provider"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "CFO"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "CIO"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "CMIO"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "CMO"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Laws and Regulations",
        "label": "45 CFR Part 170"
      },
      {
        "category": "Laws and Regulations",
        "label": "HIPAA"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Indemnification"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Malpractice"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Dementia"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Sepsis"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Trust"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "Bundled Payment"
      },
      {
        "category": "Professional Organizations",
        "label": "IEEE"
      },
      {
        "category": "Professional Organizations",
        "label": "ISO"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "RWE"
      },
      {
        "category": "Social Determinants of Health",
        "label": "Underserved Population"
      }
    ],
    "hasAttachments": true,
    "wordCount": 2923,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0013",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Kanav Jain",
    "submitterType": "Individual",
    "date": "2026-01-14T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- This response addresses the RFI on accelerating AI adoption in clinical care\n- Because this is an RFI rather than a rulemaking action, its principal value is agenda-setting\n  - It defines which governance dimensions will be treated as central as HHS explores policy, program, and operational levers\n- Primary recommendation: AI governance that prioritizes transparency without enforceable contestability will fail in practice\n  - Explanations, model cards, and \"human-in-the-loop\" language do not produce accountability unless affected parties can compel review, correction, and repair within defined time bounds\n  - Affected parties include both patients and clinicians\n- Treat AI as a source of exposure, not a neutral tool\n  - When AI influences clinical action, eligibility, documentation, or resource allocation, it becomes part of the causal chain of harm\n  - Governance must reflect this by requiring durable decision provenance and enforceable reversal pathways\n- Minimum governance requirements HHS should advance:\n  - Decision provenance as a default output\n    - AI-influenced actions should generate a durable record\n    - Record should include: inputs, model/version, confidence/uncertainty, downstream action taken, and accountable authority\n  - Contestability with binding timelines\n    - Create standard pathways for clinicians and patients to contest AI-driven outcomes\n    - Must include deadlines and escalation procedures\n  - Override and rollback as real control surfaces\n    - \"Human oversight\" should be defined as functional override and rollback capability\n    - Not merely a nominal review step\n  - Liability clarity and anti-evasion design\n    - Avoid governance frameworks that allow responsibility to be laundered through vendor disclaimers or performative \"human review\"\n    - System design must not prevent meaningful intervention\n- Accelerating adoption without enforceable remedy externalizes risk onto clinicians and patients and undermines trust\n- The question is not whether an AI system can explain itself\n  - It is whether harm can be forced to matter through correction, repair, and rollback",
      "oneLineSummary": "An independent systems designer argues that AI governance without enforceable contestability and reversal mechanisms will fail, urging HHS to require decision provenance, binding timelines for challenges, and real override capabilities rather than performative transparency.",
      "commenterProfile": "- **Name/Organization:** Kanav Jain\n- **Type:** Individual\n- **Role/Expertise:** Independent researcher and systems designer\n- **Geographic Scope:** Local (Chicago, IL)\n- **Stake in Issue:** Professional interest in AI governance design and accountability frameworks",
      "corePosition": "AI governance that prioritizes transparency without enforceable contestability will fail in practice. When AI influences clinical decisions, it becomes part of the causal chain of harm—governance must require durable decision provenance and enforceable reversal pathways, not just explanations and model cards.",
      "keyRecommendations": "- Require decision provenance as a default output for all AI-influenced actions\n  - Record must include: inputs, model/version, confidence/uncertainty, downstream action taken, and accountable authority\n- Establish contestability pathways with binding timelines\n  - Standard pathways for clinicians and patients to challenge AI-driven outcomes\n  - Must include deadlines and escalation procedures\n- Define \"human oversight\" as functional override and rollback capability\n  - Not merely a nominal review step\n- Design liability frameworks to prevent responsibility laundering\n  - Block vendor disclaimers and performative \"human review\" from shielding accountability\n  - Ensure system design allows meaningful intervention",
      "mainConcerns": "- Transparency-focused governance without enforcement mechanisms will fail\n- \"Human-in-the-loop\" language creates illusion of accountability without substance\n- Risk of accelerated adoption being externalized onto clinicians and patients\n- Governance frameworks may allow responsibility laundering through vendor disclaimers\n- System designs may prevent meaningful human intervention while claiming oversight exists",
      "notableExperiences": "- Frames the RFI's value as \"agenda-setting\" rather than policy—recognizing that which dimensions get prioritized now will shape future regulation\n- Reframes AI from \"neutral tool\" to \"source of exposure\"—shifting the governance paradigm\n- Distinguishes between \"explaining itself\" and \"forcing harm to matter\"—a useful conceptual separation for evaluating governance proposals\n- Identifies \"performative human review\" as a specific failure mode where nominal oversight masks system designs that prevent real intervention",
      "keyQuotations": "- \"AI governance that prioritizes transparency without enforceable contestability will fail in practice.\"\n- \"The question is not whether an AI system can explain itself; it is whether harm can be forced to matter through correction, repair, and rollback.\"\n- \"Avoid governance frameworks that allow responsibility to be laundered through vendor disclaimers or performative 'human review' while the system design prevents meaningful intervention.\""
    },
    "themeScores": {
      "3": 1,
      "1.6": 1,
      "3.3": 1,
      "3.4": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "AI Governance"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Human-in-the-Loop"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Model Card"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      }
    ],
    "hasAttachments": false,
    "wordCount": 326,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0014",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Ferz.AI",
    "submitterType": "Organization",
    "date": "2026-01-14T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Executive Summary\n  - FERZ AI welcomes the opportunity to respond to HHS's RFI on accelerating AI adoption in clinical care\n  - This response serves two purposes\n    - Provide concrete recommendations addressing HHS's specific questions\n    - Equip federal reviewers with a technical framework for evaluating AI governance claims from vendors\n  - The AI governance market includes many solutions marketed as \"guardrails,\" \"responsible AI,\" \"ethical AI,\" and \"trustworthy AI\"\n    - These terms lack precise technical definitions\n    - Makes it difficult for federal reviewers to evaluate competing claims\n  - Acquisition officers benefit from distinguishing between\n    - Marketing claims (\"our AI is safe and responsible\")\n    - Statistical governance (probabilistic approaches providing confidence intervals)\n    - Deterministic governance (fail-closed enforcement with reproducible, auditable decision artifacts)\n  - Document provides\n    - A taxonomy of AI governance approaches with strengths and limitations\n    - Evaluation criteria for federal reviewers to assess vendor claims\n    - Specific questions to ask vendors claiming governance capabilities\n    - FERZ's recommendations for regulatory frameworks to accelerate trustworthy AI adoption\n  - Key insight: The fundamental barrier to AI adoption in clinical care is not AI capability—it is the inability to prove that an AI system's governance was functioning correctly at the time of any specific decision\n    - Without such proof, liability cannot be allocated, regulatory compliance cannot be demonstrated, and patient trust cannot be justified\n    - This is a governance problem, not an AI problem\n\n- Part I: Understanding AI Governance—A Taxonomy for Federal Reviewers\n  - 1.1 The Three Layers of AI Systems\n    - Model Layer\n      - The AI model itself (GPT-4, Claude, Gemini)\n      - Includes training data, architecture, learned parameters\n      - Model-level governance (RLHF, Constitutional AI) attempts to make the model \"safer\" through training\n      - Inherently probabilistic—the model tends to behave well but cannot guarantee it\n    - Application Layer\n      - Software that wraps the model—user interface, integration points, business logic\n      - Where prompts are constructed and outputs processed\n      - Application-level governance (prompt engineering, output filtering) adds checks around the model\n      - Still largely probabilistic—filters can miss edge cases\n    - Deployment Layer\n      - Operational environment where AI actions occur—clinical workflows, EHR integration, decision execution\n      - The \"last mile\" before real-world impact\n      - Deployment-level governance (FERZ's focus) enforces constraints at the execution boundary\n      - Can be deterministic—nothing executes unless governance validates it\n    - Critical distinction: Most AI \"safety\" approaches operate at Model or Application Layer where governance is inherently probabilistic\n      - Deterministic governance operates at Deployment Layer where execution can be gated by mathematically verifiable constraints\n\n  - 1.2 The Fundamental Problem: Probabilistic vs. Deterministic Governance\n    - Probabilistic Governance: \"Likely Compliant\"\n      - Includes RLHF, Constitutional AI, content filtering, most \"guardrails\"\n      - Operates on statistical principles\n      - Provides confidence that a system usually behaves correctly, expressed as accuracy percentages or confidence intervals\n      - How it works: Train the model to prefer \"good\" outputs; use classifiers to flag \"bad\" outputs; filter problematic content before delivery\n      - What vendors claim: \"95% accuracy,\" \"reduced harmful outputs by 80%,\" \"enterprise-grade safety\"\n      - Risk exposure: At clinical scale, even small residual failure rates produce frequent governance escapes\n        - The question is whether escapes are prevented pre-execution and provable/auditable when they occur\n        - Probabilistic approaches cannot guarantee either\n      - Key limitation: Cannot provide proof that governance was functioning correctly for any specific decision\n        - After an adverse event, no way to demonstrate the system was operating within governance boundaries\n    - Deterministic Governance: \"Provably Compliant\"\n      - Provides fail-closed enforcement with reproducible, independently verifiable audit artifacts at the governance layer\n      - Distinct from claiming the underlying AI model is deterministic—it is not\n      - Determinism achieved at execution boundary through\n        - Pre-execution validation against explicit rules\n        - Recorded snapshots of all inputs, policies, and outputs\n        - Reproducible evaluation that can be independently verified\n      - How it works: Every AI output passes through validation layer checking explicit rules\n        - If validation fails, output is blocked (\"deny-by-default\" or \"fail-closed\")\n        - Every decision generates cryptographic proof bundle capturing inputs, policy state, and verdict\n      - What this enables: Bit-for-bit reproducible governance decisions, cryptographic audit trails, bounded residual errors quantifiable by rule coverage and implementation factors\n      - In healthcare deployment: Every AI decision validated against explicit rules before execution\n        - Failed validations blocked\n        - Every decision can be independently verified and replayed\n      - Key advantage: After adverse event, governance compliance can be independently verified by third parties\n        - Liability can be allocated based on provable governance state at decision time\n\n  - 1.3 Side-by-Side Comparison: Critical Differences\n    - Compliance assurance\n      - Probabilistic: Statistical confidence (e.g., \"95% accurate\")\n      - Deterministic: Verifiable conformance artifacts with bounded residual risk\n    - Failure mode\n      - Probabilistic: \"Fail-open\"—ungoverned outputs may proceed\n      - Deterministic: \"Fail-closed\"—nothing executes without validation\n    - After adverse event\n      - Probabilistic: Cannot prove governance was working\n      - Deterministic: Cryptographic proof of governance state\n    - Decision reproducibility\n      - Probabilistic: Cannot reproduce specific decisions\n      - Deterministic: Bit-for-bit reproducible (\"replay test\")\n    - Audit capability\n      - Probabilistic: Logs of outputs and classifications\n      - Deterministic: Cryptographic proof chain (Proof-Carrying Decisions)\n    - Liability allocation\n      - Probabilistic: Unclear—governance state unknowable\n      - Deterministic: Clear—governance state provable\n    - FDA 21 CFR Part 11\n      - Probabilistic: Difficult to satisfy—records not verifiable\n      - Deterministic: Designed for compliance—immutable audit trails\n    - Regulatory inspection\n      - Probabilistic: Requires trust in vendor's claims\n      - Deterministic: Third-party verification possible\n\n  - 1.4 Why This Distinction Matters for Healthcare\n    - Healthcare is a precision-critical domain where statistical assurances are insufficient\n    - Scenario: Clinical AI system assists with medication recommendations\n      - Vendor claims high accuracy in avoiding dangerous drug interactions\n      - Large hospital processes thousands of AI-assisted medication decisions monthly\n      - With probabilistic governance\n        - Some fraction of decisions may have ungoverned drug interaction risks\n        - Fraction may be small but absolute number becomes significant at scale\n        - After adverse event, no mechanism to prove whether governance was functioning correctly for that specific decision\n      - With deterministic governance\n        - Every medication recommendation validated against drug interaction rules before reaching clinician\n        - Failed validations blocked and escalated\n        - After any decision, cryptographic proof demonstrates exactly which rules were applied and whether they passed\n    - The difference fundamentally changes liability allocation, regulatory compliance posture, and patient safety assurance\n\n  - 1.5 Emerging Architectures: RAG, Agentic AI, and Governance Topology\n    - Two architectural patterns accelerating AI deployment in healthcare\n      - Retrieval-Augmented Generation (RAG)—grounds AI outputs in retrieved documents\n      - Agentic AI—enables AI systems to take actions autonomously\n    - Why RAG and Agentic Patterns Increase Governance Requirements\n      - These patterns do not reduce governance complexity—they increase it\n      - RAG systems introduce dynamic, unpredictable inputs\n        - AI outputs depend on retrieved context that changes with each query—patient records, clinical literature, formulary data\n        - Model-level governance cannot anticipate what documents will be retrieved\n        - Governance must validate outputs regardless of retrieval context\n      - Agentic systems execute actions with real-world consequences\n        - API calls, database writes, order placements, message transmissions\n        - Post-hoc monitoring cannot reverse incorrectly administered medication, misconfigured treatment plan, or erroneous prior authorization denial\n        - Governance must operate pre-execution, not post-detection\n    - Reviewer Note: Be skeptical of claims that RAG or agentic architectures are inherently \"safer\" or \"more accurate\"\n      - These architectures shift where errors occur, not whether they occur\n      - Fundamental question remains: can you prove governance was functioning for any specific decision?\n    - Governance Topology in Multi-Agent Systems\n      - As agentic AI evolves toward multi-agent architectures, critical question: where does governance operate?\n      - Per-Agent Governance distributes guardrails across individual agents\n        - Creates structural vulnerabilities\n        - Agent-to-agent communication may bypass governance entirely\n        - Emergent behaviors from agent interactions remain ungoverned\n        - No unified audit trail across agent ensemble\n        - Inconsistent policy enforcement across agents with different guardrail implementations\n        - \"Governance arbitrage\"—sophisticated actors may route sensitive operations through least-governed agent\n      - Centralized Governance at Execution Boundary validates all actions before execution regardless of originating agent\n        - Ensures consistent policy enforcement, unified auditability\n        - Captures emergent behaviors at point of real-world impact\n        - Governance layer is model-agnostic and agent-agnostic—governs actions, not agents\n      - Example: Clinical agentic system with retrieval agent, reasoning agent, and action agent\n        - With per-agent governance: Each agent has separate guardrails but no single point validates that chain of interactions produced compliant outcome\n        - With centralized execution-boundary governance: Every action passes through deterministic validation regardless of initiating agent; audit trail captures full decision chain\n    - Questions for Multi-Agent AI Proposals\n      - Does governance operate per-agent or at execution boundary?\n      - If per-agent, how are inter-agent communications governed?\n      - How are emergent behaviors from agent interactions detected and controlled?\n      - Is there unified audit trail across all agents, or separate logs per agent?\n      - Can you replay a decision involving multiple agents and obtain identical results?\n\n- Part II: Evaluating AI Governance Claims—A Guide for Federal Reviewers\n  - 2.1 Red Flags: Marketing Language vs. Technical Substance\n    - \"AI Guardrails\"\n      - Usually means content filtering or output classifiers\n      - Probabilistic, may miss edge cases\n      - Ask: What happens when guardrails fail? Can you prove they were active for a specific decision?\n    - \"Responsible AI\"\n      - Usually means training-time interventions (RLHF, Constitutional AI)\n      - Model tends to behave responsibly\n      - Ask: How do you verify responsible behavior for a specific decision? What are your error bounds?\n    - \"Enterprise-Grade Safety\"\n      - Marketing term with no technical definition\n      - May mean access controls or logging\n      - Ask: Define \"enterprise-grade.\" What specific guarantees do you provide? How are they verified?\n    - \"Explainable AI\"\n      - Post-hoc rationales\n      - May explain why AI probably made a decision but cannot prove compliance\n      - Ask: Can explanations be independently verified? Can decisions be replayed to confirm the explanation?\n    - \"AI Monitoring\"\n      - After-the-fact analysis\n      - Detects problems after they occur, not before\n      - Ask: How do you prevent violations rather than detect them? What is your pre-execution validation?\n    - \"99% Accurate\"\n      - 1% of decisions are ungoverned\n      - At scale, this means thousands of potential failures\n      - Ask: What happens to the 1%? Can you identify which specific decisions are in that 1%?\n\n  - 2.2 The Four Tests: Evaluating Governance Capabilities\n    - FERZ has published the Four Tests Standard (4TS) as vendor-neutral framework for evaluating AI governance claims\n    - Stop Test\n      - Question: Can the system be halted before side-effects occur? Does governance validation happen before execution?\n      - Verify by asking: \"What happens if governance validation fails?\"\n      - Answer must be \"the action is blocked,\" not \"we log it for review\"\n    - Ownership Test\n      - Question: Is there clear authority signing governance policy? Can you identify who authorized the AI's operational boundaries?\n      - Verify by asking: \"Show me the signature chain for governance policy\"\n      - Must be cryptographic proof of who authorized what\n    - Replay Test\n      - Question: Can any historical decision be reproduced and verified? Can a regulator independently confirm what happened?\n      - Verify by asking: \"Show me a decision from last week and reproduce it\"\n      - Replayed decision must match original exactly\n    - Escalation Test\n      - Question: Are human oversight routes defined? When governance denies an action, what happens? Is there mandatory custody transfer?\n      - Verify by asking: \"What happens when you deny an action?\"\n      - Must be defined escalation paths with human decision-makers\n    - Key insight: Governance system that cannot pass all four tests provides statistical confidence at best\n      - Cannot provide proof-level assurance required for regulated healthcare environments\n\n  - 2.3 Technical Due Diligence Questions\n    - Architecture Questions\n      - At what layer does your governance operate? (Model, Application, or Deployment)\n      - Is governance validation performed before or after execution?\n      - What is default behavior when governance cannot validate an action? (Fail-open vs. fail-closed)\n      - Can your system operate with any AI model, or is it tied to specific model provider?\n    - Compliance Verification Questions\n      - How do you prove governance was functioning correctly for a specific historical decision?\n      - Can a third party independently verify your compliance claims without trusting your attestations?\n      - What cryptographic mechanisms ensure audit trail integrity?\n      - How do you satisfy FDA 21 CFR Part 11 requirements for electronic records and signatures?\n    - Error Bound Questions\n      - What are your quantified error bounds? (Not accuracy percentages—actual mathematical error bounds)\n      - How did you derive those bounds? (Empirical testing vs. mathematical proof)\n      - What assumptions must hold for those bounds to be valid?\n      - How do you detect and handle cases that fall outside your bounded domain?\n    - Operational Questions\n      - What is your governance validation latency? (Must not significantly impact clinical workflows)\n      - How do you handle governance policy updates without disrupting operations?\n      - What happens if your governance system experiences an outage?\n      - Can you demonstrate air-gapped deployment for sensitive environments?\n\n- Part III: Responses to HHS Specific Questions\n  - Question 1: Barriers to Private Sector Innovation\n    - Primary barriers are governance-related, not capability-related\n    - The Governance Gap: Healthcare requires certainty; current AI governance provides only statistical confidence\n      - Organizations cannot deploy AI when they cannot prove compliance\n    - Liability Uncertainty: Without clear governance boundaries, liability cannot be allocated\n      - Legal departments block deployments when governance state is unknowable\n    - Audit Trail Inadequacy: FDA 21 CFR Part 11 requires verifiable electronic records\n      - Current AI systems produce outputs, not evidence\n      - No cryptographic proof that governance was functioning at decision time\n    - Regulatory Uncertainty: Non-medical device AI falls into gray zones\n      - Organizations cannot invest when regulatory requirements are unclear\n    - FERZ Recommendation: HHS should establish clear governance requirements distinguishing between statistical and deterministic assurance levels\n      - Creates pathway for organizations to deploy AI with appropriate governance for their risk tolerance\n\n  - Question 2: Regulatory and Policy Changes\n    - Regulatory Recommendations with CFR Citations\n      - Citations are illustrative, not exhaustive\n      - Recommend HHS issue guidance clarifying application to AI governance rather than amending underlying regulations\n      - 21 CFR Part 11 (Electronic Records and Signatures)\n        - Issue guidance recognizing cryptographic proof bundles (e.g., Proof-Carrying Decisions) as satisfying §11.10 audit trail requirements and §11.50 signature manifestation requirements\n        - Current language contemplates traditional electronic signatures\n        - Guidance should clarify cryptographic attestation of policy state satisfies integrity and attribution requirements\n      - 45 CFR Part 170 (Health IT Certification)\n        - Consider adding AI governance attestation requirements to §170.315 certification criteria\n        - Systems claiming clinical decision support functionality should demonstrate reproducible governance validation as certification condition\n      - 42 CFR Part 493 (CLIA)\n        - Issue guidance on how AI-assisted laboratory analytics should satisfy §493.1251 procedure manual requirements and §493.1283 retention requirements\n        - Deterministic governance artifacts could satisfy \"complete record\" requirements\n      - 45 CFR Part 164 (HIPAA Security)\n        - Clarify how AI governance audit trails interact with §164.312(b) audit control requirements and §164.312(c) integrity requirements\n        - Cryptographic proof chains may exceed current compliance expectations\n        - Guidance should recognize this\n    - Payment Policy Recommendations\n      - Governance Attestation as Reimbursement Condition\n        - For AI-assisted clinical workflows in high-risk categories, require governance attestation artifacts as condition of reimbursement\n        - Creates market incentive for governance adoption\n      - Incentives for Burden-Reduction Tools\n        - Provide enhanced reimbursement for AI-assisted documentation, coding, and prior authorization when governance artifacts enable post-payment integrity verification\n        - Aligns payer interests with governance adoption\n      - Pilot Reimbursement Models\n        - CMS should pilot reimbursement models for AI tooling where governance artifacts support audit\n        - Generates evidence on governance value while managing risk\n\n  - Question 3: Novel Legal Issues\n    - The Black Box Problem: Current AI cannot prove governance state at decision time\n      - Creates fundamental liability allocation challenges\n      - Who is responsible when governance cannot be verified?\n    - Temporal Governance: AI systems may be updated between deployment and adverse event\n      - Without governance infrastructure capturing policy state at decision time, retrospective analysis is impossible\n    - Multi-Party Accountability: Clinical AI involves model developers, governance providers, deploying institutions, and clinicians\n      - Clear boundary definitions needed\n    - Privacy-Preserving Audit: HIPAA requires protecting PHI while maintaining auditable records\n      - Deterministic governance with cryptographic proofs can address this\n      - Regulatory recognition needed\n    - HHS's Role\n      - Define minimum governance architecture standards for clinical AI\n      - Create accountability frameworks for multi-party deployments\n      - Publish reference architectures demonstrating acceptable approaches\n    - Mapping to FDA SaMD and CDS Realities\n      - Deterministic governance artifacts support auditability and change control regardless of whether tool is regulated as device\n      - For CDS outside device regulation under 21st Century Cures Act criteria, governance artifacts provide documentation and traceability for institutional risk management\n      - For tools qualifying as Software as a Medical Device (SaMD), same artifacts support FDA expectations for traceability, change control, and post-market surveillance\n      - Governance layer provides compliance infrastructure scaling across regulatory spectrum without requiring different architectures for different categories\n\n  - Question 4: AI Evaluation Methods\n    - Most promising evaluation methods focus on governance verification rather than AI capability assessment\n    - Pre-Deployment Evaluation\n      - Governance Coverage Analysis: What percentage of AI outputs pass through deterministic validation?\n        - Target: 100% for high-risk decisions\n      - Rule Completeness Testing: Do governance rules cover all compliance requirements (HIPAA, FDA, state regulations)?\n      - Adversarial Testing: Systematic testing of governance behavior under unusual or adversarial inputs\n    - Post-Deployment Evaluation\n      - Replay Verification: Can any historical decision be replayed and obtain identical results?\n        - Enables post-hoc audit without trusting logs\n      - Escalation Analysis: How often do decisions escalate to human review? Are thresholds appropriately calibrated?\n      - Governance Drift Detection: Monitor for changes in AI behavior that might indicate model drift affecting governance effectiveness\n    - HHS Opportunity: Support development of governance evaluation tools and conformance testing infrastructure\n      - 4TS conformance bundle provides a model\n\n  - Questions 5-10: Summary Responses\n    - Question 5 (Private Sector Activities)\n      - HHS should recognize deterministic governance in certification criteria\n      - Fund pilot programs\n      - Create testing infrastructure\n      - Establish public-private partnerships for reference architectures\n    - Question 6 (AI Tool Performance)\n      - AI succeeds in domains with clear boundaries and low compliance complexity (imaging, analytics)\n      - Underperforms in governance-heavy domains\n      - Common thread is governance inadequacy, not capability limitation\n      - Deterministic governance unlocks high-value applications currently blocked by compliance concerns\n    - Question 7 (Decision-Makers)\n      - General Counsel and Risk Management are typically blocking stakeholders\n        - Cannot approve deployments without governance guarantees\n      - CMOs focus on clinical efficacy; CIOs on integration; Compliance Officers on regulations\n      - Deterministic governance directly addresses Risk Management/Legal blockers\n    - Question 8 (Interoperability)\n      - Critical gaps exist in standards for exchanging governance attestations, audit trail formats, and AI governance metadata in FHIR\n      - 4TS Proof-Carrying Decision (PCD) format provides foundation\n      - Concrete recommendation: HHS could sponsor HL7 FHIR Implementation Guide for exchanging governance attestations alongside CDS Hooks responses or as extensions to relevant FHIR resources\n        - Would enable EHRs and clinical systems to verify governance state without proprietary integrations\n    - Question 9 (Patient Concerns)\n      - Patient concerns about privacy, accuracy, dehumanization, and bias are fundamentally governance concerns\n      - Deterministic governance addresses them\n        - Cryptographic audit trails ensure verifiable privacy compliance\n        - Reproducible decisions enable accuracy demonstration\n        - Escalation protocols preserve human oversight\n        - Bias detection provides equity assurance\n    - Question 10 (Research Priorities)\n      - HHS should prioritize research in\n        - Deterministic governance methods\n        - Cryptographic audit infrastructure\n        - Causal inference for clinical AI\n        - Human-AI collaboration frameworks\n\n- Part IV: About FERZ—Deterministic Governance Infrastructure\n  - FERZ AI develops deterministic AI governance infrastructure software\n  - Unlike approaches that attempt to make AI models \"safer\" through training, FERZ operates at deployment layer\n    - Wraps any AI system in deterministic validation shell guaranteeing compliance with explicitly defined rules\n  - Core Technology\n    - LASO(f): Multi-tier linguistic and action governance framework\n      - Eight validation tiers (syntax, semantics, pragmatics, stylistics, discourse, morphology, phonology/graphemics, lexicon) with bidirectional propagation\n      - Designed for low-latency, high-accuracy validation\n    - DELIA: Flat constraint architecture with Z3 SMT solver for rapid deterministic validation\n      - Ideal for action governance requiring ALLOW/DENY/ESCALATE decisions\n    - CausaCore: Cross-domain causal modeling with four specialized engines for drug interaction analysis, clinical trial design, and precision medicine\n    - 4TS Standard: Vendor-neutral open standard for AI governance verification (CC BY-NC-ND 4.0)\n    - Performance benchmarks available upon request under appropriate evaluation conditions\n  - Regulatory Alignment\n    - FERZ systems designed for regulatory compliance from ground up\n    - FDA 21 CFR Part 11: Electronic signatures, immutable audit trails, record integrity verification\n    - HIPAA: Access controls, encryption, audit logging for protected health information\n    - EU AI Act: High-risk system governance, documentation requirements, explainability\n    - SEC Rule 17a-4: WORM storage, tamper-evidence, 7-year retention\n  - Offer of Collaboration\n    - FERZ stands ready to support HHS\n    - Pilot Programs: Deploy LASO(f) and DELIA in hospital pilots to generate real-world evidence\n    - Standards Development: Collaborate on healthcare-specific adaptations of 4TS Standard\n    - Technical Consultation: Provide briefings to HHS staff on deterministic governance approaches\n    - Research Partnerships: Participate in HHS-sponsored research through cooperative agreements or CRADAs\n  - Vendor-Neutral Guidance\n    - Encourage HHS to adopt evaluation criteria and governance principles regardless of vendor\n    - Four Tests Standard (4TS) published under open license to enable vendor-neutral assessment\n    - Interest is in establishing clear standards that accelerate trustworthy AI adoption\n    - FERZ benefits from well-defined market, not from regulatory ambiguity\n  - Closing statement: The promise of AI in clinical care is immense. The barrier is trust. Deterministic governance infrastructure can bridge that gap—transforming AI from a liability concern into a verified asset.\n\n---",
      "oneLineSummary": "AI governance infrastructure company argues that healthcare AI adoption is blocked not by capability limitations but by the inability to prove compliance at decision time, and urges HHS to establish standards distinguishing probabilistic \"guardrails\" from deterministic, cryptographically verifiable governance.\n\n---",
      "commenterProfile": "- **Name/Organization:** FERZ AI (FERZ LLC), submitted by Neil Nair, Chief Business Officer\n- **Type:** Business\n- **Role/Expertise:** Developer of deterministic AI governance infrastructure software; expertise in deployment-layer governance, cryptographic audit trails, and regulatory compliance frameworks (FDA 21 CFR Part 11, HIPAA, EU AI Act)\n- **Geographic Scope:** National (U.S. federal regulatory focus)\n- **Stake in Issue:** Direct commercial interest in HHS adopting governance standards that recognize deterministic validation approaches; positioned to benefit from pilot programs and certification criteria requiring provable AI governance\n\n---",
      "corePosition": "We believe the fundamental barrier to AI adoption in clinical care is not AI capability—it's the inability to prove that governance was functioning correctly at the time of any specific decision. Without such proof, liability cannot be allocated, regulatory compliance cannot be demonstrated, and patient trust cannot be justified. HHS should establish clear governance requirements that distinguish between statistical assurance (\"likely compliant\") and deterministic assurance (\"provably compliant\"), creating pathways for organizations to deploy AI with appropriate governance for their risk tolerance.\n\n---",
      "keyRecommendations": "- Establish clear governance requirements distinguishing statistical from deterministic assurance levels\n  - Create pathway for organizations to deploy AI with governance appropriate to risk tolerance\n- Issue guidance on existing regulations rather than amending them\n  - 21 CFR Part 11: Recognize cryptographic proof bundles as satisfying audit trail and signature requirements\n  - 45 CFR Part 170: Add AI governance attestation requirements to Health IT certification criteria\n  - 42 CFR Part 493 (CLIA): Clarify how AI-assisted laboratory analytics satisfy procedure manual and retention requirements\n  - 45 CFR Part 164 (HIPAA): Clarify how AI governance audit trails interact with audit control and integrity requirements\n- Implement payment policy changes to incentivize governance adoption\n  - Require governance attestation artifacts as reimbursement condition for high-risk AI-assisted workflows\n  - Provide enhanced reimbursement for AI-assisted documentation/coding when governance artifacts enable post-payment verification\n  - CMS should pilot reimbursement models where governance artifacts support audit\n- Define minimum governance architecture standards for clinical AI\n  - Create accountability frameworks for multi-party deployments\n  - Publish reference architectures demonstrating acceptable approaches\n- Sponsor HL7 FHIR Implementation Guide for exchanging governance attestations\n  - Enable EHRs to verify governance state without proprietary integrations\n- Support development of governance evaluation tools and conformance testing infrastructure\n- Prioritize research in deterministic governance methods, cryptographic audit infrastructure, causal inference for clinical AI, and human-AI collaboration frameworks\n- Recognize deterministic governance in certification criteria; fund pilot programs; create testing infrastructure; establish public-private partnerships for reference architectures\n\n---",
      "mainConcerns": "- Current AI governance provides only statistical confidence, not the certainty healthcare requires\n  - Organizations cannot deploy AI when they cannot prove compliance\n- Liability uncertainty blocks deployment\n  - Legal departments cannot approve when governance state is unknowable after adverse events\n- Audit trail inadequacy\n  - FDA 21 CFR Part 11 requires verifiable electronic records\n  - Current AI systems produce outputs, not evidence—no cryptographic proof governance was functioning at decision time\n- Regulatory uncertainty for non-medical device AI\n  - Falls into gray zones; organizations cannot invest when requirements are unclear\n- Marketing language obscures technical reality\n  - Terms like \"guardrails,\" \"responsible AI,\" \"enterprise-grade safety\" lack precise definitions\n  - Federal reviewers cannot distinguish substance from noise\n- Probabilistic governance fails at scale\n  - Even small residual failure rates produce frequent governance escapes at clinical volumes\n  - Cannot identify which specific decisions failed\n- RAG and agentic AI architectures increase governance complexity, not reduce it\n  - Dynamic inputs and autonomous actions require pre-execution validation\n  - Post-hoc monitoring cannot reverse real-world consequences\n- Per-agent governance in multi-agent systems creates vulnerabilities\n  - Agent-to-agent communication may bypass governance\n  - Emergent behaviors remain ungoverned\n  - \"Governance arbitrage\" possible through least-governed agent\n- The \"black box problem\" creates fundamental liability allocation challenges\n  - AI systems may be updated between deployment and adverse event\n  - Without governance capturing policy state at decision time, retrospective analysis impossible\n- Critical gaps in interoperability standards\n  - No standards for exchanging governance attestations, audit trail formats, or AI governance metadata in FHIR\n\n---",
      "notableExperiences": "- Reframes the AI adoption problem as governance, not capability\n  - \"This is a governance problem, not an AI problem\"\n  - AI succeeds in domains with clear boundaries and low compliance complexity; underperforms in governance-heavy domains\n- Identifies General Counsel and Risk Management as the actual blocking stakeholders\n  - CMOs, CIOs, and Compliance Officers have different concerns\n  - Deterministic governance directly addresses the legal/risk blockers\n- Introduces concept of \"governance arbitrage\" in multi-agent systems\n  - Sophisticated actors may route sensitive operations through least-governed agent\n- Proposes the \"Four Tests Standard\" (4TS) as vendor-neutral evaluation framework\n  - Stop Test: Can system halt before side-effects occur?\n  - Ownership Test: Is there cryptographic proof of who authorized governance policy?\n  - Replay Test: Can any historical decision be reproduced exactly?\n  - Escalation Test: Are human oversight routes defined?\n- Distinguishes three layers where governance can operate\n  - Model Layer (inherently probabilistic)\n  - Application Layer (still probabilistic)\n  - Deployment Layer (can be deterministic)\n- Provides practical due diligence questions for federal acquisition officers\n  - Architecture, compliance verification, error bounds, and operational questions\n- Offers specific CFR citations with recommended guidance approaches rather than regulatory amendments\n- Patient concerns about privacy, accuracy, dehumanization, and bias are reframed as fundamentally governance concerns addressable through deterministic approaches\n\n---",
      "keyQuotations": "- \"The fundamental barrier to AI adoption in clinical care is not AI capability—it is the inability to prove that an AI system's governance was functioning correctly at the time of any specific decision. Without such proof, liability cannot be allocated, regulatory compliance cannot be demonstrated, and patient trust cannot be justified.\"\n\n- \"Federal reviewers should be skeptical of claims that RAG or agentic architectures are inherently 'safer' or 'more accurate.' These architectures shift where errors occur, not whether they occur.\"\n\n- \"The promise of AI in clinical care is immense. The barrier is trust. Deterministic governance infrastructure can bridge that gap—transforming AI from a liability concern into a verified asset.\""
    },
    "themeScores": {
      "1": 1,
      "2": 1,
      "3": 1,
      "4": 1,
      "9": 1,
      "1.6": 1,
      "2.6": 1,
      "3.3": 1,
      "4.6": 1,
      "5.5": 1,
      "9.1": 1,
      "9.2": 1
    },
    "entities": [
      {
        "category": "AI Applications in Healthcare",
        "label": "AI Scribe"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "AI Governance"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Human-in-the-Loop"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Post-deployment Monitoring"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Trustworthy AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Agentic AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Algorithmic Bias"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Black Box AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Explainable AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Model Drift"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "RAG"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "RLHF"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "SaMD"
      },
      {
        "category": "Accreditation and Certification",
        "label": "CLIA"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Certification"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Clinical Trial"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Treatment"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Precision Medicine"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Audit Trail"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Encryption"
      },
      {
        "category": "Data Privacy and Security",
        "label": "PHI"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "FDA"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Health Information Systems",
        "label": "CDS"
      },
      {
        "category": "Health Information Systems",
        "label": "EHR"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Claims Processing"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Medical Coding"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Prior Authorization"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Pilot Program"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "FHIR"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "HL7"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Adverse Event"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Equity"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Patient Safety"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Laws and Regulations",
        "label": "21 CFR Part 11"
      },
      {
        "category": "Laws and Regulations",
        "label": "21st Century Cures Act"
      },
      {
        "category": "Laws and Regulations",
        "label": "42 CFR Part 493"
      },
      {
        "category": "Laws and Regulations",
        "label": "45 CFR Part 170"
      },
      {
        "category": "Laws and Regulations",
        "label": "EU AI Act"
      },
      {
        "category": "Laws and Regulations",
        "label": "HIPAA"
      },
      {
        "category": "Laws and Regulations",
        "label": "HIPAA Privacy Rule"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Trust"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "CRADA"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "Cooperative Agreement"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "RWE"
      }
    ],
    "hasAttachments": true,
    "wordCount": 4084,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0015",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Lumenex Advisory",
    "submitterType": "Organization",
    "date": "2026-01-14T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Introduction\n  - Submitting response based on more than two decades of experience in the U.S. healthcare ecosystem\n  - Focus areas: healthcare operations, data integration, payment models, and technology implementation\n  - Perspective grounded in realities of how clinical, administrative, and financial systems function together in practice, not theoretical AI capability\n  - AI holds significant promise for improving clinical outcomes, reducing administrative burden, and lowering costs\n  - Adoption often stalls not because of model performance, but because of structural, financial, and organizational friction\n  - Comments focus on friction points related to private sector innovation and reimbursement alignment\n\n- Response to Question 1: Biggest barriers to private sector innovation in AI for health care and its adoption in clinical care\n  - Most significant barriers are not technical—they are organizational, administrative, and incentive-based\n  - Ownership and accountability for adoption are often unclear\n    - AI initiatives frequently treated as IT implementations\n    - In reality, they require coordinated change across clinical, operational, compliance, and administrative teams\n    - When responsibility for workflow redesign and ongoing maintenance is diffused, adoption stalls even when technology performs as intended\n  - Recurring barrier: resistance driven by perceived increases in workload\n    - Experience implementing large-scale provider data management systems prior to modern AI tools\n    - Downstream teams resisted adoption because new workflows were viewed as \"extra steps\"\n    - Long-term value of structured data capture (reliable, high-quality reporting) was not immediately visible to end users\n    - Leadership desired improved analytics and insights but paid insufficient attention to explaining how outcomes depended on consistent, incremental workflow changes\n  - Over time, these steps typically became easier and more efficient, but early resistance significantly slowed adoption\n  - AI magnifies this dynamic\n    - Without deliberate workflow design, education, and change management, AI tools risk being perceived as additive burden rather than enabling infrastructure\n  - Most common point of failure is the handoff to end users\n    - When cultural transformation and workflow ownership are not addressed explicitly, even well-designed systems fail to achieve sustained use\n    - This is not a technology failure but an implementation and governance one\n\n- Response to Question 2: Regulatory, payment policy, or programmatic design changes HHS should prioritize\n  - Misaligned incentives and uncertainty around value recognition remain significant obstacles\n  - Payment models rarely account for trust, education, and governance work required for adoption\n    - Many AI tools deliver value by improving efficiency, reducing administrative burden, or augmenting clinician decision-making rather than producing discrete billable events\n    - Organizations struggle to justify investment when reimbursement pathways are unclear or nonexistent\n  - Even relatively narrow AI tools face adoption hurdles when trust is not addressed\n    - Example: implementing AI-enabled note-taking tools in healthcare settings\n    - Frequently raises concerns about privacy, recording, and data usage\n    - Despite conducting due diligence to ensure SOC 2, HIPAA, and GDPR compliance, adoption was initially limited\n    - Adoption improved only after clear guardrails were established around how, when, and for what purpose tools would be used\n    - Education and transparency were essential to building confidence and enabling responsible use\n  - HHS could accelerate adoption by aligning reimbursement and program design with real-world implementation costs\n    - Incentives should recognize not only clinical outcomes but also operational effort required to safely deploy AI\n    - This includes training, governance, and workflow integration\n    - Would reduce adoption risk\n  - Pilot programs, demonstration models, or value-based payment structures that explicitly accommodate AI-enabled efficiency gains would encourage experimentation while generating evidence for broader policy\n\n- Conclusion\n  - AI adoption in clinical care will not accelerate through technical innovation alone\n  - Requires alignment across organizational governance, administrative capacity, and financial incentives\n  - By focusing on incentive alignment, reimbursement clarity, and reduction of administrative friction, HHS can enable meaningful, responsible adoption\n  - Goal: deliver real value to patients, clinicians, and the healthcare system as a whole",
      "oneLineSummary": "A veteran healthcare operations consultant argues that AI adoption stalls not from technical shortcomings but from organizational friction, unclear accountability, and payment models that ignore the real costs of implementation.",
      "commenterProfile": "- **Name/Organization:** Karina Lupercio, Founder & Principal Consultant, Lumenex Advisory LLC\n- **Type:** Business\n- **Role/Expertise:** 20+ years in healthcare operations, data integration, payment models, and technology implementation\n- **Geographic Scope:** National (U.S. healthcare ecosystem)\n- **Stake in Issue:** Advises healthcare organizations on technology implementation; directly observes barriers to AI adoption in practice",
      "corePosition": "AI adoption in clinical care fails not because the technology doesn't work, but because organizations lack clear ownership, payment models don't recognize implementation costs, and end users see new tools as extra burden rather than help. HHS should focus on aligning incentives and reimbursement with the real-world effort required to deploy AI responsibly.",
      "keyRecommendations": "- Align reimbursement and program design with real-world implementation costs\n  - Recognize not just clinical outcomes but operational effort: training, governance, workflow integration\n  - This would reduce adoption risk for organizations\n- Create pilot programs, demonstration models, or value-based payment structures that explicitly accommodate AI-enabled efficiency gains\n  - Encourage experimentation while generating evidence for broader policy\n- Focus on incentive alignment, reimbursement clarity, and reduction of administrative friction",
      "mainConcerns": "- Ownership and accountability for AI adoption are often unclear\n  - AI initiatives treated as IT projects when they require coordinated change across clinical, operational, compliance, and administrative teams\n  - Diffused responsibility causes adoption to stall even when technology works\n- Resistance driven by perceived increases in workload\n  - End users view new workflows as \"extra steps\" without understanding long-term value\n  - AI magnifies this dynamic without deliberate workflow design and change management\n- Payment models don't account for trust, education, and governance work\n  - Many AI tools improve efficiency or augment decision-making rather than producing billable events\n  - Organizations can't justify investment when reimbursement pathways are unclear\n- Trust barriers slow adoption even for narrow AI tools\n  - Privacy and data usage concerns require clear guardrails and education before adoption improves",
      "notableExperiences": "- Prior experience implementing large-scale provider data management systems revealed a pattern: downstream teams resisted adoption because they saw new workflows as \"extra steps,\" even though leadership wanted the analytics those workflows enabled\n  - Over time, steps became easier, but early resistance significantly slowed adoption\n  - AI magnifies this same dynamic\n- Implemented AI-enabled note-taking tools in healthcare settings\n  - Despite SOC 2, HIPAA, and GDPR compliance, adoption was initially limited due to privacy and recording concerns\n  - Adoption only improved after establishing clear guardrails around how, when, and for what purpose tools would be used\n- Key insight: the most common point of failure is the handoff to end users—when cultural transformation and workflow ownership aren't addressed, even well-designed systems fail",
      "keyQuotations": "- \"Adoption often stalls not because of model performance, but because of structural, financial, and organizational friction.\"\n- \"Without deliberate workflow design, education, and change management, AI tools risk being perceived as additive burden rather than enabling infrastructure.\"\n- \"This is not a technology failure, but an implementation and governance one.\""
    },
    "themeScores": {
      "1": 1,
      "4": 1,
      "6": 1,
      "1.1": 1,
      "4.2": 1,
      "6.1": 1,
      "6.4": 1,
      "6.8": 1
    },
    "entities": [
      {
        "category": "Data Privacy and Security",
        "label": "SOC 2"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Quality Reporting"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Administrative Burden"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Change Management"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Workflow Integration"
      },
      {
        "category": "Laws and Regulations",
        "label": "GDPR"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "Value-Based Care"
      }
    ],
    "hasAttachments": true,
    "wordCount": 733,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0016",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Steven Zecola",
    "submitterType": "Individual",
    "date": "2026-01-20T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- The Dimensions of Artificial Intelligence in the Healthcare Industry\n  - HHS should be commended for seeking to leverage technology to lower healthcare costs and improve care\n  - Since being diagnosed with Parkinson's disease 24 years ago, I have watched healthcare R&D proceed at a glacial pace across multiple disease fronts\n  - Conversely, costs and related subsidies have skyrocketed\n  - AI offers the potential to fundamentally change these trends\n  - HHS should view AI's short-term effect as improving odds of success and shortening feedback cycles\n  - To achieve significant breakthroughs, HHS via FDA will need to completely revamp its approach to drug discovery and development\n  - Rather than being an overzealous gatekeeper at every step, FDA's role should shift to being an auditor with specifically delimited responsibilities\n\n- Dimension #1: Incorporation of AI into Drug Discovery\n  - The biggest benefit to healthcare industry performance from AI is achievable from drug discovery\n  - Federal government expends $44 billion per year funding basic healthcare research\n  - A small fraction of basic research initiatives pass through the \"Valley of Death\" and make it to clinical trial\n    - Average length of this phase is approximately 15 years per endeavor\n    - FDA involvement impacts the pace of progress\n  - Even after overcoming this initial barrier, less than 10% of promising remedies receive final FDA approval\n  - Accounting for costs of failures, average FDA drug approval costs society almost $3 billion and takes decades to reach market from lab inception\n  - This system is a prescription for failure, appearing every day in high costs and poor performance\n  - AI identifies potential treatments much faster than traditional methods\n    - Enables efficient exploration of biological variability\n    - Can dramatically increase experiments by studying up to a trillion interactions between variables\n    - Can process vast amounts of biological data, uncover hidden causal relationships, and generate new actionable insights\n  - More than 50% of nation's healthcare costs are attributable to patients with multiple chronic illnesses\n    - This figure exceeds 75% when complex diseases are included\n  - AI is particularly promising for complex, multifactorial conditions where conventional reductionist approaches have failed\n    - Neurodegenerative diseases\n    - Autism spectrum disorders\n    - Multiple chronic illnesses\n  - Over 30 million people in the U.S. affected by rare diseases, with more than 7,000 different types\n  - HHS should encourage AI initiatives to address these conditions\n    - Potential to improve health of nation's most disadvantaged citizens\n    - Potential to significantly cut costs of providing care\n  - Let the scientists study science, not the FDA requirements\n    - By being forced to have one eye on the finish line, scientists tend to craft their work to satisfy the FDA rather than the rudiments of a flawed biological process\n  - Short-run recommendations:\n    - HHS should direct grants toward AI-generated basic research with emphasis on hard-to-solve illnesses\n    - FDA should put in place a new deregulatory system for AI-initiated programs to enable breakthrough treatments in compressed timetable\n\n- Dimension #2: Incorporation of AI into the Drug Development Process\n  - Simply relying on AI for drug discovery while subjecting advances to current approval process would undermine the technology\n  - Current FDA clinical procedures entail an average of 12 years and $2 billion for each drug approval\n    - Even scheduling a meeting can take six months for a \"fast track\" application\n  - Enhancements to existing processes alone would not capture true potential of AI\n  - AI can already improve fulfillment of exhaustive regulatory documentation requirements\n    - Documentation today adds up to as much as 30% of the cost of compliance\n  - Short-run AI improvements to drug development:\n    - Automating and validating regulatory documentation\n    - Enhancing trial design and participant stratification\n    - Monitoring safety and efficacy in near real-time\n    - Reducing administrative and compliance costs\n  - In the U.K., the Medicines and Healthcare Products Regulatory Agency reported clinical trial approval times were twice as fast with AI and associated reforms\n  - For greater long-term gains:\n    - HHS should not be involved in pre-clinical AI research\n    - Should collapse all clinical work utilizing AI into one elongated trial rather than discrete Phase I, II and III trials\n    - AI can be used to continually update and validate documentation\n    - This change would not require statutory change or agency rulemaking because clinical trial design is not codified in FDA rules\n  - Proposed new trial approach:\n    - As participants are added, safety results examined and reported in real time\n    - Once trial surpasses certain number such as 1000 participants with proven efficacy and meeting safety protocols, it would be approved for roll-out\n    - Government role would be as auditor to validate trial output\n    - Would include experimental validation, mechanistic understanding, and ethical oversight\n    - Real-world data could more readily be collected after launch as another safety feature\n  - FDA personnel would shift from episodic gatekeepers to continuous auditors\n    - Requires fundamental change in organizational culture\n    - Safety concerns would remain important but responsibility and accountability more equally shared with applicants and trial participants\n  - Prolonged suffering of existing patients should be factored into public welfare analysis when reviewing preliminary safety results\n    - 33 million people with one or more of 7000 rare diseases now have near 0% chance of relief under current regulatory procedures\n    - Their plight must be considered while contours of biology are examined\n  - Specific example: AI-generated treatment of Familial Adenomatous Polyposis (FAP)\n    - New treatment produced large, sustained reduction in polyps with no significant safety concerns\n    - Company hopes to meet with FDA in 1H2026\n    - Current FAP treatment is expensive, intrusive, and lifelong\n      - Early detection strategies cost $10k+\n      - Late detection costs $37k+\n      - Cost to treat metastatic colorectal cancer (for which FAP predisposes) can reach $300,000\n      - Constant exams and excisions are the norm\n    - FDA is not inclined to ask these patients whether they would like to try the pill or whether continued excisions would be preferable\n    - The answer from most patients would be obvious if given the choice\n\n- Dimension #3: Enhance Data Collection to Empower AI\n  - AI harbors the proverbial statistical stigma of \"garbage in, garbage out\"\n  - Comprehensive and accurate data is essential to AI success\n  - This is another area where healthcare industry has failed\n  - Industry has evolved with each provider encouraging patients to sign up for customer portals\n    - Providers typically treat portal information as their own for research purposes\n    - However, providers do not own the data\n    - Each patient owns his or her data\n  - Few instances where comprehensive data has been collected\n    - Example: In 2010, The Michael J. Fox Foundation launched the Parkinson's Progression Markers Initiative (PPMI) to find biological markers of Parkinson's onset and progression\n    - Led to impressive finding of a tool that can detect pathology not only in people diagnosed with Parkinson's but also in individuals at high risk of developing it\n    - However, study was geared towards Parkinson's disease and has only a few thousand participants\n  - HHS should establish national standards for patient-facing data collection that:\n    - Use interoperable formats\n    - Capture both diagnostic outcomes and relevant explanatory variables\n    - Preserve patient ownership and informed consent\n    - Enable longitudinal tracking while protecting privacy and security\n  - As information from participants is collected over time, researchers would use AI to identify combinations of variables pointing towards promising results\n  - HHS should establish goal of enrolling 100,000 participants within two years\n\n- Dimension #4: Use of AI to Establish Standards of Care and Price Ceilings\n  - No national standards of care for diseases or other health maladies in the United States\n  - Patients oftentimes do not understand:\n    - The nature of their infliction\n    - The options to treat it\n    - The costs of various options to remedy it\n  - On parallel tracks:\n    - HHS might fund basic research targeted to a particular ailment\n    - FDA might or might not approve it\n    - Medicare might or might not cover it\n    - Some insurance companies may cover treatment and some may not\n  - Costs of various treatments may vary greatly from facility to facility, unbeknownst to patient\n  - Healthcare practitioners have desire and economic incentive to provide best (and likely most expensive) possible service\n  - As an economist would conclude, supply curves and demand curves in healthcare do not lead to pareto optimal solution\n    - There is a market failure, primarily relating to lack of actionable information\n  - Short-run AI applications:\n    - Aggregating and analyzing how care is delivered across the country\n    - Identifying patterns associated with better outcomes and lower costs\n    - Informing evidence-based minimum standards of care\n    - Improving transparencies around pricing and performance\n  - Longer-term applications:\n    - Outputs could establish minimum standard of care for all or most ailments\n    - These standards would be mandatorily covered by insurance\n    - Outputs could be supplemented by regional price ceilings based on comprehensive industry analysis\n  - Future version could be programmed to automatically calculate prescribed minimum standards of care and price ceilings to mimic functioning of demand and supply curves\n    - Algorithm could use specified level of federal subsidy as equilibrium\n    - As federal subsidy exceeds certain pre-set limits, AI would address disequilibrium by providing lawmakers options to lower price ceiling for certain conditions and/or lower minimum standard of care\n  - In scenarios where stipulated federal subsidy was exceeded:\n    - Some classes of patients would be denied payment for best available treatment unless they had supplemental insurance\n    - Some healthcare providers would suffer diminution of profits\n  - Would require Congressional approval, but such tradeoffs are occurring now without factual inputs or informed choices\n  - AI could address industry's massive information failure and tackle ever-increasing subsidies\n\n- Dimension #5: Incorporation of AI into HHS's Internal Processes\n  - AI can improve efficiency and effectiveness of HHS's internal operations\n  - Potential percentage gains would be smaller than drug discovery and development dimensions\n  - Even modest improvements can yield meaningful savings given scale of federal healthcare spending\n  - Early AI applications should focus on areas with clear return on investment and limited risk:\n    - Fraud, waste, and abuse detection\n    - Claims auditing and compliance monitoring\n    - Internal workflow automation\n  - HHS should deploy these tools incrementally, evaluate performance, and use lessons learned to inform broader adoption\n\n- Dimension X: \"Accelerating\" the Adoption of \"AI\" as Part of Clinical Care\n  - HHS issued companion RFI on Dec. 23 seeking to accelerate AI adoption in clinical care\n  - HHS should proceed very carefully on issues raised in this RFI\n  - AI software defines clinical care as direct, hands-on medical treatment, diagnosis, and management of patients by trained healthcare professionals\n  - HHS should not involve itself in anything beyond establishing standards of care for several reasons\n  - First reason: HHS is likely to slow down AI adoption in expanded capacity\n    - Example: A few years ago, HHS recognized need to reorganize Center for Evaluation and Drug Research\n    - After multiple-year effort, described reorganization as: \"The office is now comprised of six cross-functional support offices and eight clinical offices. The six cross-functional support offices oversee 12 divisions and several staff groups. The eight clinical offices oversee OND's 27 clinical divisions and six pharm/tox review divisions\"\n    - This is not the sort of thinking or organizational design to accelerate adoption of new technology\n  - Second reason: We are in the first inning of AI system development\n    - HHS should not be in business of accelerating or selecting any one AI system or application\n    - Should set level playing field and let market decide best technologies\n  - Third reason: HHS's legal authority does not extend to anything and everything related to healthcare\n    - HHS derives authority from series of laws\n    - Public Health Service Act provides authority to lead responses to public health emergencies, declare emergencies, manage Strategic National Stockpile, and control diseases\n    - HIPAA authorizes HHS to set standards for electronic health information exchange, privacy, and security\n    - Other statutes govern specific areas like SAMHSA for substance abuse, OHRP for human research, OIG for oversight\n  - Proactively selecting or pushing particular technology for patient care would stretch HHS's legal authority\n  - Could retard advancements as result of extended litigation\n  - Multiple AI systems are \"in the works\"\n  - HHS should not be \"in the business\" of picking winners and losers through acceleration program\n\n- Conclusion\n  - AI offers opportunity for significant improvements in healthcare outcomes and efficiencies\n  - Only achievable if integrated into regulatory and governance framework designed for its capabilities\n  - Shoehorning AI into existing structures will blunt its impact and increase implementation risk\n  - Each dimension requires separate dedicated, multidisciplinary team reporting to Office of Deputy Secretary\n  - After strategic direction established, teams should be tasked with:\n    - Developing detailed implementation plans including budgetary requirements\n    - Identifying any statutory or regulatory barriers\n    - Establishing timelines, milestones, and evaluation criteria\n    - Addressing ethical and equity considerations\n  - Drug discovery and drug development represent highest-impact dimensions for AI implementation\n  - HHS should make use of external expertise in fashioning details of appropriate regulatory framework for these dimensions\n  - Detailed plans for implementing AI should be approved and finalized before end of 2026\n  - HHS should take proactive, forward-looking role in harnessing AI technology to deflate healthcare costs and make America healthy again",
      "oneLineSummary": "A Parkinson's patient of 24 years and economist argues that FDA should transform from \"overzealous gatekeeper\" to auditor, collapsing clinical trials into single continuous processes to unlock AI's potential for drug discovery and slash the $3 billion average cost of bringing treatments to market.",
      "commenterProfile": "- **Name/Organization:** Steven Zecola\n- **Type:** Individual\n- **Role/Expertise:** Patient advocate with 24 years of experience as a Parkinson's disease patient; economics background evident in market analysis\n- **Geographic Scope:** National (Potomac Falls, VA)\n- **Stake in Issue:** Direct personal stake as a patient with a chronic neurodegenerative disease who has watched healthcare R&D proceed at \"glacial pace\" while costs have skyrocketed; advocates for patients with rare and complex diseases who have \"near 0% chance of relief\" under current regulatory procedures",
      "corePosition": "I believe AI offers the potential to fundamentally transform healthcare, but only if HHS completely revamps its approach to drug discovery and development. Rather than being an overzealous gatekeeper at every step, the FDA's role should shift to being an auditor with specifically delimited responsibilities. The current system—where drug approvals cost society almost $3 billion and take decades—is a prescription for failure, and shoehorning AI into existing structures will blunt its impact.",
      "keyRecommendations": "- Transform FDA's role from gatekeeper to auditor\n  - Shift from episodic gatekeeping to continuous auditing\n  - Share responsibility and accountability more equally with applicants and trial participants\n  - Factor prolonged suffering of existing patients into public welfare analysis\n\n- Restructure clinical trials for AI-driven drug development\n  - Collapse Phase I, II, and III trials into one elongated continuous trial\n  - Approve treatments once they surpass threshold (e.g., 1000 participants) with proven efficacy and safety\n  - Monitor safety and efficacy in real-time as participants are added\n  - This change would not require statutory change or agency rulemaking\n\n- Direct federal research grants toward AI-generated basic research\n  - Emphasize hard-to-solve illnesses: neurodegenerative diseases, autism spectrum disorders, rare diseases\n  - Remove HHS involvement from pre-clinical AI research\n\n- Establish national patient data collection standards\n  - Use interoperable formats\n  - Capture diagnostic outcomes and relevant explanatory variables\n  - Preserve patient ownership and informed consent\n  - Enable longitudinal tracking while protecting privacy\n  - Goal: enroll 100,000 participants within two years\n\n- Use AI to establish national standards of care and price ceilings\n  - Aggregate and analyze care delivery patterns across the country\n  - Establish minimum standards mandatorily covered by insurance\n  - Implement regional price ceilings based on comprehensive industry analysis\n  - Eventually automate calculations using federal subsidy levels as equilibrium\n\n- Deploy AI for HHS internal operations\n  - Fraud, waste, and abuse detection\n  - Claims auditing and compliance monitoring\n  - Internal workflow automation\n  - Deploy incrementally and evaluate performance\n\n- Avoid \"accelerating\" AI adoption in clinical care\n  - HHS should not pick winners and losers among AI systems\n  - Set level playing field and let market decide\n  - Stay within existing legal authority\n\n- Create dedicated multidisciplinary teams for each AI dimension\n  - Report to Office of Deputy Secretary\n  - Develop implementation plans with budgets, timelines, and milestones\n  - Identify statutory or regulatory barriers\n  - Finalize detailed plans before end of 2026",
      "mainConcerns": "- Current drug approval system is fundamentally broken\n  - Average FDA drug approval costs society almost $3 billion\n  - Takes decades to reach market from lab inception\n  - Less than 10% of promising remedies receive final FDA approval\n  - Average clinical trial phase takes 15 years\n  - Even \"fast track\" meeting scheduling takes six months\n  - Regulatory documentation adds up to 30% of compliance costs\n\n- Scientists are forced to satisfy FDA rather than pursue science\n  - Having \"one eye on the finish line\" distorts research priorities\n  - Scientists craft work to satisfy FDA rather than address flawed biological processes\n\n- Patients with rare and complex diseases are abandoned\n  - 33 million people with 7,000+ rare diseases have \"near 0% chance of relief\" under current procedures\n  - Over 50% of healthcare costs attributable to patients with multiple chronic illnesses\n  - 75% when complex diseases included\n  - Conventional reductionist approaches have failed for multifactorial conditions\n\n- Healthcare data collection is fragmented and inadequate\n  - Providers treat patient data as their own\n  - Patients actually own their data\n  - Few comprehensive data collection efforts exist\n  - Existing studies like PPMI have only a few thousand participants\n\n- Healthcare market suffers from massive information failure\n  - No national standards of care\n  - Patients don't understand their conditions, options, or costs\n  - Treatment costs vary greatly between facilities\n  - Providers have economic incentive to provide most expensive services\n  - Supply and demand curves don't lead to pareto optimal solution\n\n- HHS organizational culture is ill-suited to accelerate technology adoption\n  - Recent FDA reorganization resulted in byzantine structure of offices, divisions, and staff groups\n  - \"This is not the sort of thinking or organizational design to accelerate the adoption of new technology\"\n\n- HHS may exceed its legal authority\n  - Proactively selecting or pushing particular technology could stretch legal authority\n  - Could retard advancements through extended litigation\n  - HHS should not be picking winners and losers",
      "notableExperiences": "- Personal 24-year journey with Parkinson's disease provides direct window into glacial pace of healthcare R&D\n  - Has watched costs and subsidies skyrocket while progress stalls\n  - Understands firsthand the desperation of patients waiting for treatments\n\n- Economist's perspective on healthcare market failure\n  - Applies supply/demand curve analysis to explain why healthcare doesn't reach pareto optimal solutions\n  - Proposes algorithmic approach using federal subsidy levels as equilibrium point\n  - Suggests AI could automatically calculate standards of care and price ceilings\n\n- Specific knowledge of Parkinson's Progression Markers Initiative (PPMI)\n  - Launched by Michael J. Fox Foundation in 2010\n  - Led to tool that can detect Parkinson's pathology in both diagnosed patients and high-risk individuals\n  - Limited by having only a few thousand participants\n\n- Detailed case study of FAP (Familial Adenomatous Polyposis) treatment\n  - AI-generated treatment produced large, sustained polyp reduction with no significant safety concerns\n  - Current treatment costs: early detection $10k+, late detection $37k+, metastatic colorectal cancer up to $300,000\n  - Patients endure constant exams and excisions\n  - FDA not inclined to ask patients whether they'd prefer to try the pill\n\n- UK regulatory comparison\n  - Medicines and Healthcare Products Regulatory Agency achieved clinical trial approval times twice as fast with AI and associated reforms\n\n- Insight that clinical trial design is not codified in FDA rules\n  - Collapsing trials into single continuous process would not require statutory change or agency rulemaking",
      "keyQuotations": "- \"Let the scientists study science, not the FDA requirements. By being forced to have one eye on the finish line, scientists tend to craft their work to satisfy the FDA, rather than the rudiments of a flawed biological process.\"\n\n- \"The 33 million people with one or more of the 7000 rare diseases now have a near 0% chance of relief under current regulatory procedures, and their plight must be considered while the contours of biology are examined.\"\n\n- \"This system is a prescription for failure, and the results of such a scheme appear every day in the real world in the form of high costs and poor performance.\""
    },
    "themeScores": {
      "2": 1,
      "5": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "AI Governance"
      },
      {
        "category": "Accreditation and Certification",
        "label": "FDA Clearance"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Clinical Trial"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Diagnosis"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Informed Consent"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Treatment"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Pathology"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "FDA"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "OHRP"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "OIG"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "SAMHSA"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Abuse"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Fraud"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Waste"
      },
      {
        "category": "Health Information Systems",
        "label": "HIE"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "ROI"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Equity"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Laws and Regulations",
        "label": "HIPAA"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Standard of Care"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Cancer"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Parkinson's Disease"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Rare Disease"
      }
    ],
    "hasAttachments": true,
    "wordCount": 2604,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0017",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Vamsi Varra",
    "submitterType": "Individual",
    "date": "2026-01-20T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- I am a dermatologist whose research specializes in artificial intelligence applications in clinical care\n- One of the challenges for introducing safe AI tools to improve healthcare is navigating the regulations surrounding it\n  - Information for companies developing clinical AI products would be helpful to distinguish AI tools which do or do not require FDA approval\n- In the future, autonomous medical decision making without involvement of a human clinician could be possible\n  - It will be crucial to create regulatory standards for compliance of those tools",
      "oneLineSummary": "A dermatologist specializing in AI research calls for clearer FDA guidance on clinical AI tools and proactive standards for future autonomous medical decision-making systems.",
      "commenterProfile": "- **Name/Organization:** Vamsi Varra\n- **Type:** Individual\n- **Role/Expertise:** Dermatologist; researcher specializing in artificial intelligence applications in clinical care\n- **Geographic Scope:** Not specified\n- **Stake in Issue:** Directly involved in developing and introducing AI tools for healthcare; navigates regulatory landscape for clinical AI applications",
      "corePosition": "As a dermatologist researching AI in clinical care, I see regulatory navigation as a key barrier to introducing safe AI tools in healthcare. Companies need clearer guidance on which AI products require FDA approval, and we need to proactively develop standards for autonomous AI systems before they become reality.",
      "keyRecommendations": "- Provide clearer information to help companies distinguish between AI tools that do and do not require FDA approval\n- Create regulatory standards for autonomous medical decision-making tools that operate without human clinician involvement\n  - Standards should be developed proactively, before such tools become widespread",
      "mainConcerns": "- Navigating current regulations is a significant challenge for introducing safe AI tools to healthcare\n- Lack of clarity on FDA approval requirements for different types of clinical AI products\n- Future autonomous AI systems will need compliance standards that don't yet exist",
      "notableExperiences": "- Offers perspective from dual role as practicing clinician and AI researcher\n- Identifies regulatory uncertainty as a barrier to beneficial AI adoption in healthcare\n- Anticipates shift toward autonomous medical decision-making without human clinician involvement",
      "keyQuotations": "- \"One of the challenges for introducing safe AI tools to improve healthcare is navigating the regulations surrounding it.\"\n- \"In the future autonomous medical decision making without involvement of a human clinician could be possible. It will be crucial to create regulatory standards for compliance of those tools.\""
    },
    "themeScores": {
      "2": 1,
      "2.6": 1
    },
    "entities": [],
    "hasAttachments": false,
    "wordCount": 101,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0018",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Binita Ashar",
    "submitterType": "Individual",
    "date": "2026-01-20T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Cover letter introduction\n  - Submission focuses on auditability for AI-enabled and networked surgical systems\n  - Need for reliable, cross-system reconstruction of procedural events\n  - Supports safety, postmarket learning, and accountability as these technologies scale\n\n- Auditability of AI-Enabled and Networked Surgical Systems\n  - A foundational barrier has emerged in advanced, AI-enabled and networked surgical systems\n  - These systems integrate devices, software, cloud platforms, networks, and clinician decision-making in real time\n  - When adverse events, unexpected outcomes, or performance questions arise, it is often difficult to reconstruct what actually occurred\n\n- The core problem is auditability, not data availability\n  - Relevant information is fragmented across manufacturers, hospitals, electronic health records, device logs, and cloud systems\n  - No reliable way to assemble a coherent, verifiable account of events\n  - This fragmentation constrains patient safety, postmarket learning, payer confidence, and public trust\n\n- Centralized control of all data is neither realistic nor desirable\n  - HHS should elevate auditability as a system requirement for high-risk, AI-enabled care\n  - Definition of auditability: authorized parties can reconstruct relevant aspects of a procedure across systems using preserved, reliable records under clear rules of access and use\n\n- HHS coordination recommendations\n  - Coordinate across FDA, CMS, and ONC to define minimum attributes of an auditable procedural record\n    - Traceability\n    - Data integrity\n    - Continuity across software updates\n    - Authorized access for safety investigations and quality improvement\n  - Align innovation funding and demonstration programs to prioritize systems that support auditability\n  - Encourage interoperability standards that enable cross-system reconstruction of events\n  - CMS could incorporate support for auditable records into innovation models or quality programs\n    - Better align payment incentives with safer and more learnable AI deployment\n\n- Benefits of making auditability a baseline expectation\n  - Reduces legal and implementation uncertainty\n  - Supports safer adoption of AI\n  - Enables continuous learning across health systems without slowing innovation\n\n- Closing argument\n  - Fragmentation is inevitable; lack of auditability is not\n  - Elevating auditability will help HHS achieve faster, safer, and more trustworthy AI adoption in clinical care",
      "oneLineSummary": "A surgeon and medical device expert argues that HHS should make auditability a baseline requirement for AI-enabled surgical systems, enabling cross-system reconstruction of procedural events to support safety and learning without centralizing data control.",
      "commenterProfile": "- **Name/Organization:** Binita S. Ashar, MD, MBA, FACS, MAMSE; Founder, Clarity Surgical Advisors LLC\n- **Type:** Individual\n- **Role/Expertise:** Surgeon (Fellow of American College of Surgeons), medical device/surgical systems advisor, MBA, Member of American Medical & Biological Engineering\n- **Geographic Scope:** National\n- **Stake in Issue:** Works at the intersection of surgery and medical technology; directly affected by inability to reconstruct events when AI-enabled surgical systems produce adverse outcomes",
      "corePosition": "HHS should elevate auditability as a system requirement for high-risk, AI-enabled surgical care. The problem isn't data availability—it's that relevant information is fragmented across manufacturers, hospitals, EHRs, device logs, and cloud systems with no reliable way to reconstruct what happened during a procedure. Centralized data control isn't the answer; instead, we need clear standards that allow authorized parties to piece together procedural events across systems.",
      "keyRecommendations": "- Define minimum attributes of an auditable procedural record through FDA, CMS, and ONC coordination\n  - Traceability\n  - Data integrity\n  - Continuity across software updates\n  - Authorized access for safety investigations and quality improvement\n- Align innovation funding and demonstration programs to prioritize systems that support auditability\n- Encourage interoperability standards enabling cross-system reconstruction of events\n- Have CMS incorporate auditable record support into innovation models or quality programs\n  - Align payment incentives with safer, more learnable AI deployment",
      "mainConcerns": "- Current AI-enabled surgical systems integrate devices, software, cloud platforms, networks, and clinician decision-making in real time, but when things go wrong, no one can reconstruct what happened\n- Information fragmentation across manufacturers, hospitals, EHRs, device logs, and cloud systems\n- This fragmentation constrains:\n  - Patient safety\n  - Postmarket learning\n  - Payer confidence\n  - Public trust\n- Legal and implementation uncertainty without clear auditability standards",
      "notableExperiences": "- Reframes the AI safety problem: it's not about data availability but about auditability—the ability to reconstruct events across fragmented systems\n- Offers a practical middle ground: centralized data control is \"neither realistic nor desirable,\" but auditability standards can achieve safety goals without it\n- Proposes using CMS payment incentives as a lever for safer AI deployment",
      "keyQuotations": "- \"This is not primarily a data availability problem. Rather, it is an auditability problem.\"\n- \"Fragmentation is inevitable; lack of auditability is not.\"\n- \"By auditability, I mean that authorized parties can reconstruct relevant aspects of a procedure across systems using preserved, reliable records under clear rules of access and use.\""
    },
    "themeScores": {
      "5": 1,
      "9": 1,
      "2.4": 1,
      "5.7": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "Trustworthy AI"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "FDA"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "ONC"
      },
      {
        "category": "Health Information Systems",
        "label": "EHR"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Adverse Event"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Patient Safety"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Quality Improvement"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Health System"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Trust"
      }
    ],
    "hasAttachments": true,
    "wordCount": 515,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0019",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Pal Randhawa",
    "submitterType": "Individual",
    "date": "2026-01-20T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Submitter Background\n  - Board-certified neurosurgeon with approximately 15 years of frontline clinical experience\n    - Emergency departments, operating rooms, and intensive care units\n    - Sustained work in Level I trauma and referral centers\n  - Holds Executive MBA with focus on healthcare operations and infrastructure resilience\n  - Dual perspective integrates tactical clinical realities with strategic resource management for national health security\n  - Experience includes firsthand observation of workforce strain, capacity degradation, and coordination failures under stress\n  - Participated in collaborative and advisory efforts related to disaster preparedness and national critical infrastructure discussions\n  - Comments submitted in personal capacity, views do not represent any employer, institution, or advisory body\n\n- Executive Summary: The Case for \"Deflationary Infrastructure\"\n  - U.S. Healthcare Sector operates beyond safe margins\n    - AAMC projects deficit of up to 86,000 physicians by 2036\n  - AI should be prioritized as \"Deflationary Infrastructure\"\n    - A protective capability designed to preserve the system's most expensive and fragile asset: staffed clinical capacity\n\n- Proposed Doctrinal Standard: Workforce Readiness Telemetry (WRT)\n  - Proposes HHS adopt WRT as a critical interoperability standard\n  - Definition: anonymized, aggregate indicators of operational strain, cognitive load, and staffing fragility\n  - Intended for system-level planning and readiness assessment\n  - Explicitly NOT designed for individual performance evaluation, disciplinary action, or employment decisions\n  - Intended to complement existing interoperability frameworks (USCDI, TEFCA), not replace them\n  - Focuses on upstream operational inputs that precede clinical events\n\n- Question 1: Barriers to Private Sector Innovation and Adoption\n  - Bottom Line Up Front: Adoption is throttled by a \"Readiness Bind\"\n    - Hospitals lack cognitive bandwidth to adopt tools that don't immediately reduce operational friction\n  - The Gap: HHS lacks standardized metric for \"Human Capacity\"\n    - Staffing measured in FTEs (Full-Time Equivalents), a lagging indicator\n    - FTEs fail to capture burnout, moral injury, or disengagement\n    - Facility may appear fully staffed on paper yet remain operationally degraded due to unmeasured cognitive load\n  - The Solution: Prioritize AI that serves as Operational Radar\n    - Tools that predict attrition 30-60 days in advance allow proactive intervention\n    - Reduces multi-million dollar annual cost of turnover\n    - WRT leverages existing metadata streams and minimal viable indicators\n    - Low-burden for frontline systems\n  - Strategic Alignment\n    - Directly operationalizes NHSS 2023-2026 Objective 1.3 (\"Strengthen the recruitment, retention, and preparedness of the HPH workforce\")\n    - Moves workforce retention from passive goal to active, measured capability\n  - Data Support\n    - Nursing turnover costs average hospital $5.7M annually\n    - Loss of single high-acuity physician costs $500,000 to >$1M in replacement costs alone\n    - Consistent with recent AMA and AMN Healthcare analyses\n  - Revenue Impact\n    - For high-revenue specialties (e.g., Neurosurgery), six-month vacancy results in >$3M in lost clinical capacity\n    - AI that preserves this capacity represents critical form of deflationary infrastructure\n\n- Question 3: Governance and Implementation of Non-Medical Device AI\n  - Bottom Line Up Front: Workflow-embedded AI falls into a \"Liability Gap\" that chills adoption\n    - Frontline clinicians currently bear disproportionate share of risk for AI-influenced workflows\n  - The Gap: FDA regulation covers devices but not \"Algorithm-Augmented Workflow\"\n    - Frontline clinicians often shoulder ambiguity and accountability for AI-influenced workflows\n    - Creates adoption hesitancy in high-stakes care\n  - Recommendation: HHS's internal AI Governance Board should advocate for exploration of time-limited federal safe-harbor mechanisms during declared staffing emergencies\n    - If clinician follows federally validated AI protocol during crisis, they should enjoy liability protection\n    - Essential for deploying AI in high-stakes environments without paralyzing the workforce\n\n- Question 8: Interoperability to Accelerate AI Adoption\n  - Bottom Line Up Front: Interoperability must expand beyond Clinical Data (USCDI) to include Operational Telemetry\n  - The Gap: Current interoperability standards track the patient but ignore the system\n  - The Solution: Create standard for sharing \"Aggregate Readiness Signals\"\n    - Example: Bed Capacity + Staffing Viability\n  - Strategic Outcome: Enables Regional and National Surge Prediction\n    - Beyond daily operations, aggregated telemetry creates real-time heat map for mass casualty, pandemic, or geopolitical events\n    - Aligns with NDAA Section 735 readiness requirements\n    - Allows federal planners to load-balance across national grid before regional collapse occurs\n  - The ONC Role\n    - Uniquely positioned to define this operational layer\n    - Must preserve trust, minimize burden, and enable downstream innovation\n    - Ensures readiness data becomes national asset rather than proprietary silo\n\n- Question 10: Prioritized Areas of Research\n  - Bottom Line Up Front: Research must validate AI as tool for Cognitive Augmentation, not just automation\n  - Priority 1: Cognitive Load Mapping\n    - Primary safety endpoint for clinical AI should be: Does this tool reduce time-to-decision or increase cognitive burden?\n  - Priority 2: Degraded-State Validation\n    - AI tools must be tested for reliability under \"Degraded Conditions\"\n    - Examples: cyber outages, communications latency, mass casualty surges, geopolitical conflict\n    - Aligns with CISA's 2025 Cybersecurity Performance Goals for critical infrastructure resilience\n\n- Conclusion: The Aviation Parallels\n  - Successful high-reliability sectors (Aviation, Energy) don't use technology to replace expert operators\n  - They use it to augment situational awareness and prevent error\n  - Healthcare AI should follow \"Human-System Teaming\" doctrine\n  - By shifting focus from \"replacing doctors\" to \"stabilizing the grid,\" HHS can transform AI from disruptive novelty into stabilizing component of national health security\n\n- References cited\n  - ASPR NHSS 2023-2026\n  - AMA on physician burnout costs\n  - NSI Nursing Solutions 2025 retention report\n  - AMN Healthcare on physician turnover costs\n  - White House NSM-22 on critical infrastructure\n  - CISA Cybersecurity Performance Goals 2.0\n  - AAMC on physician supply and demand\n\n---",
      "oneLineSummary": "A neurosurgeon with 15 years of trauma center experience proposes \"Workforce Readiness Telemetry\" as a new interoperability standard, arguing AI should function as \"deflationary infrastructure\" that predicts workforce strain and prevents system collapse rather than replacing clinicians.\n\n---",
      "commenterProfile": "- **Name/Organization:** Pal Randhawa, MD, EMBA, FAANS\n- **Type:** Individual (Healthcare Provider)\n- **Role/Expertise:** Board-certified neurosurgeon with ~15 years frontline clinical experience in Level I trauma centers, EDs, ORs, and ICUs; Executive MBA focused on healthcare operations and infrastructure resilience; involvement in disaster preparedness and critical infrastructure discussions\n- **Geographic Scope:** National\n- **Stake in Issue:** Directly experiences workforce strain, capacity degradation, and coordination failures; works in high-acuity specialty where physician vacancies cost >$3M in lost capacity; bears liability risk for AI-influenced clinical decisions\n\n---",
      "corePosition": "I argue that AI should be prioritized as \"Deflationary Infrastructure\"—a protective capability designed to preserve the healthcare system's most expensive and fragile asset: its staffed clinical capacity. Rather than focusing on replacing doctors, HHS should use AI to stabilize the workforce grid by predicting attrition and preventing capacity degradation before it occurs. This requires a new interoperability standard—Workforce Readiness Telemetry—that tracks operational strain alongside clinical data.\n\n---",
      "keyRecommendations": "- Adopt Workforce Readiness Telemetry (WRT) as a critical interoperability standard\n  - Anonymized, aggregate indicators of operational strain, cognitive load, and staffing fragility\n  - Designed for system-level planning, NOT individual performance evaluation or disciplinary action\n  - Complements existing frameworks (USCDI, TEFCA) rather than replacing them\n\n- Prioritize AI that serves as \"Operational Radar\"\n  - Tools that predict attrition 30-60 days in advance for proactive intervention\n  - Leverage existing metadata streams with minimal burden on frontline systems\n\n- Explore time-limited federal safe-harbor mechanisms for AI use during declared staffing emergencies\n  - Clinicians following federally validated AI protocols during crises should receive liability protection\n  - Essential for deploying AI in high-stakes environments without paralyzing the workforce\n\n- Expand interoperability standards beyond clinical data to include Operational Telemetry\n  - Create standard for sharing \"Aggregate Readiness Signals\" (bed capacity + staffing viability)\n  - Enable regional and national surge prediction through real-time heat maps\n  - ONC should define this operational layer to ensure readiness data becomes national asset\n\n- Prioritize research on Cognitive Load Mapping\n  - Primary safety endpoint: Does this tool reduce time-to-decision or increase cognitive burden?\n\n- Require Degraded-State Validation for AI tools\n  - Test reliability under cyber outages, communications latency, mass casualty surges, geopolitical conflict\n\n---",
      "mainConcerns": "- \"Readiness Bind\" throttles AI adoption\n  - Hospitals lack cognitive bandwidth to adopt tools that don't immediately reduce operational friction\n\n- HHS lacks standardized metric for \"Human Capacity\"\n  - FTEs are lagging indicators that fail to capture burnout, moral injury, or disengagement\n  - Facilities may appear fully staffed on paper while operationally degraded\n\n- \"Liability Gap\" chills adoption of workflow-embedded AI\n  - FDA regulation covers devices but not \"Algorithm-Augmented Workflow\"\n  - Frontline clinicians shoulder disproportionate ambiguity and accountability\n  - Creates adoption hesitancy in high-stakes care\n\n- Current interoperability standards track patients but ignore the system\n  - No mechanism for sharing operational readiness signals across facilities or regions\n\n- Projected physician deficit of up to 86,000 by 2036\n  - Healthcare sector already operating beyond safe margins\n\n- Massive financial impact of workforce instability\n  - Nursing turnover costs average hospital $5.7M annually\n  - Single high-acuity physician loss costs $500K-$1M+ in replacement\n  - Six-month neurosurgery vacancy results in >$3M lost clinical capacity\n\n---",
      "notableExperiences": "- Proposes novel conceptual framework: AI as \"Deflationary Infrastructure\"\n  - Shifts paradigm from \"replacing doctors\" to \"stabilizing the grid\"\n  - Frames workforce capacity as the system's most expensive and fragile asset\n\n- Draws explicit parallel to aviation and energy sectors\n  - High-reliability industries use technology to augment situational awareness, not replace expert operators\n  - Advocates \"Human-System Teaming\" doctrine for healthcare AI\n\n- Identifies unmeasured \"cognitive load\" as hidden driver of system degradation\n  - A facility can be fully staffed on paper yet operationally compromised\n  - Current metrics miss burnout, moral injury, and disengagement\n\n- Connects AI governance to national security frameworks\n  - Links WRT to NHSS 2023-2026 Objective 1.3, NDAA Section 735, NSM-22, and CISA CPG 2.0\n  - Positions aggregated readiness telemetry as enabling federal load-balancing before regional collapse\n\n- Quantifies the \"deflationary\" value proposition with specific dollar figures\n  - Makes concrete case that preserving workforce capacity has measurable ROI\n\n---",
      "keyQuotations": "- \"The U.S. Healthcare Sector operates beyond safe margins... AI should be prioritized as 'Deflationary Infrastructure'—a protective capability designed to preserve the system's most expensive and fragile asset: its staffed clinical capacity.\"\n\n- \"A facility may appear fully staffed on paper, yet remain operationally degraded due to unmeasured cognitive load.\"\n\n- \"Successful high-reliability sectors (Aviation, Energy) do not use technology to replace the expert operator; they use it to augment situational awareness and prevent error. Healthcare AI should follow this 'Human-System Teaming' doctrine.\""
    },
    "themeScores": {
      "3": 1,
      "5": 1,
      "6": 1,
      "2.5": 1,
      "3.1": 1,
      "6.3": 1,
      "6.5": 1,
      "6.6": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "AI Governance"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Certification"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Cybersecurity"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CISA"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "FDA"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "ONC"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "FTE"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "TEFCA"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "USCDI"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Burnout"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Physician"
      },
      {
        "category": "Laws and Regulations",
        "label": "NDAA"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Professional Organizations",
        "label": "AAMC"
      },
      {
        "category": "Professional Organizations",
        "label": "AMA"
      }
    ],
    "hasAttachments": true,
    "wordCount": 1166,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0020",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Pictor Labs Inc",
    "submitterType": "Organization",
    "date": "2026-01-23T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Pictor Labs is a virtual staining company advocating for digital pathology adoption through AI technology\n  - Delivers results in minutes while preserving tissue for additional analyses\n  - Applauds HHS initiative to learn about AI impact in healthcare and influence of reimbursement policy on clinical care access\n\n- AI has evolved to allow development of critical healthcare tools\n  - For our company, AI increases efficiency and improves normal laboratory workflow of staining histological slides\n  - Tissues can be virtually stained with:\n    - Routine biomedical stains (e.g., H&E)\n    - Special tissue stains such as Masson's Trichrome\n    - Immunohistochemical stains that selectively identify single molecules of interest\n\n- Tissue staining is a critical first step in analysis of biopsies/needle aspirations\n  - Essential for ultimate diagnosis and decisions about additional stains for differential or confirming diagnosis\n  - Becomes a bottleneck to all subsequent procedures due to time involved for irreversible chemical staining\n  - HHS should prioritize these first pivotal steps that many healthcare decisions rely upon\n  - AI processes can make this step more time-effective by circumventing delays in diagnosis\n  - Making this primary step available can reduce disparities in healthcare access and equity\n    - Pathology laboratories capable of various staining may be functionally or locally limited\n    - May also be financially prohibitive for some patients\n\n- Added benefits for American autonomy\n  - Digital staining can reduce economic dependence on other countries\n  - US laboratories currently depend on obtaining traditional staining chemicals from China and India\n  - Finished kits may be bottled in US/EU, but upstream dyes/intermediates frequently originate abroad\n  - Supply chain shocks quickly affect US labs logistically and financially with altered pricing\n  - Virtual staining allows US laboratories to maintain workflow without dependence on foreign supply chains\n\n- Areas where we seek HHS assistance\n  - A defined and consistent pathway to reimbursement that is flexible for many scenarios\n    - Current approaches for coding and coverage determination move more slowly than technology advancement\n    - This hampers confidence in reimbursement necessary for bringing innovation to market\n  - Changing the inertia of the healthcare system to utilize AI tools\n    - Need education on new technologies to increase efficiency and output of healthcare staff\n\n- Current reimbursement avenues\n  - New Technology Add-on Payment (NTAP) exists but is technology-specific with complex approval process\n  - Potential alternatives include analytical pathway similar to AI-driven diabetic retinal exams\n    - While meritorious, involves extensive analysis for each step and cannot be incorporated quickly enough to incentivize\n  - Health Tech Investment Act (S. 1399) aims to assign Advanced Health Behavior Services to new technology ambulatory payment classification\n    - Has garnered support from AdvaMed (representative body for medical biotechnology sector)\n    - Currently waiting for formal discussion\n\n- Simple first step proposal\n  - Establish identical reimbursement for current staining procedures (e.g., CLIA fees)\n  - Would greatly attenuate hesitancy by pathology laboratories to adopt AI-driven technology\n  - If a stained slide is reimbursed, whether accomplished by approved AI or chemical processes should be unimportant\n  - Both AI and traditional staining require investment of time and resources\n    - For AI staining, time expenditure is upfront in development and validation\n  - The stain should be reimbursed equally\n\n- Overlooked benefit of virtual staining: tissue retention\n  - Virtual stains require only a single tissue slice, so significant biopsy tissue is retained\n  - Critical for patients since additional molecular tests are frequently needed for diagnosis confirmation and therapy guidance\n  - Several common biopsy locations frequently have inadequate samples and trigger repeat sampling\n    - Breast, melanoma, colorectal, kidney, lung, thyroid\n  - Repeat biopsies are a real, recurring source of healthcare cost and patient burden\n  - If virtually stained slide circumvents re-biopsy:\n    - Significant cost is saved\n    - Patient spared another invasive procedure, delays, frustration, and anxiety\n  - Prioritizing this primary staining step ensures benefits will be multifactorial, more efficient, and not restricted to single demographic or therapeutic area\n\n- Education presents another critical challenge\n  - USCAP and Digital Pathology Association have acknowledged potential of this technology and necessity for advancement\n  - Both societies set compelling example through collaboration with AI and digital pathology companies\n  - DPA has worked with FDA for years to secure clearances for whole slide imaging equipment\n    - Created more productive workplace for pathologists and healthcare institutions\n    - Enhanced patient care\n  - Medical education towards AI digital pathology is timely, recognized, and supported\n  - Suggests intensive interaction between HHS and USCAP/DPA\n\n- Addressing reluctance to utilize novel AI technology\n  - Prevalent viewpoint suggests reluctance stems from \"the black box nature of AI\"\n  - Governing bodies can provide significant assistance in adoption\n  - Support for medical education can be accomplished through:\n    - Supporting CME\n    - Including modern technology use for hospital accreditation\n    - Establishing required insurance coverage for AI-dependent processes aligned with reimbursement strategies\n  - Would ensure more rapid clinical decisions impacting patient care\n  - Would ensure healthcare system stays current with technological progress\n\n- Summary\n  - HHS should prioritize foundational and simple healthcare steps such as virtual histopathological staining for AI-driven advancement\n  - Adoption, reimbursement, and acceptance of AI-driven processes is critical to continued healthcare system successes\n  - While highlighting policy needs related to our specific business, these principles are applicable across all healthcare companies innovating with AI\n\n---",
      "oneLineSummary": "Virtual staining company urges HHS to prioritize reimbursement parity for AI-driven tissue staining, arguing it would speed diagnoses, preserve precious biopsy tissue, reduce repeat procedures, and decrease dependence on foreign chemical supply chains.\n\n---",
      "commenterProfile": "- **Name/Organization:** Pictor Labs Inc\n- **Type:** Business\n- **Role/Expertise:** Virtual staining company; contributors include Director of Medical Affairs (PhD), Chief Medical Officer (MD, PhD), and Chief Executive Officer (PhD)\n- **Geographic Scope:** National (United States)\n- **Stake in Issue:** Directly affected by reimbursement policies for AI-driven pathology tools; seeking market adoption of their virtual staining technology\n\n---",
      "corePosition": "We believe HHS should prioritize foundational healthcare steps like virtual histopathological staining for AI advancement. A simple first step would be establishing identical reimbursement for AI-driven staining as for traditional chemical staining—if a stained slide is reimbursed, whether accomplished by AI or chemical processes should be unimportant. This would remove adoption hesitancy while delivering faster diagnoses, preserved tissue for additional testing, and reduced dependence on foreign supply chains.\n\n---",
      "keyRecommendations": "- Establish reimbursement parity between AI-driven and traditional chemical staining procedures\n  - Use existing CLIA fee structures as baseline\n  - Both methods require time and resource investment; AI's investment is simply front-loaded in development and validation\n- Create a defined, consistent, and flexible pathway to reimbursement\n  - Current coding and coverage determination processes move too slowly for technology advancement\n  - Consider analytical pathway model used for AI-driven diabetic retinal exams\n- Advance the Health Tech Investment Act (S. 1399)\n  - Would assign Advanced Health Behavior Services to new technology ambulatory payment classification\n  - Already has AdvaMed support\n- Partner with professional societies for education initiatives\n  - Intensive interaction between HHS and USCAP/DPA recommended\n  - Support CME programs on AI technology\n  - Include modern technology use in hospital accreditation requirements\n- Establish required insurance coverage for AI-dependent processes aligned with reimbursement strategies\n\n---",
      "mainConcerns": "- Current reimbursement pathways are inadequate for AI innovation\n  - NTAP is technology-specific with complex approval process\n  - Coding and coverage determination moves slower than technology advancement\n  - Lack of reimbursement confidence hampers bringing innovation to market\n- Healthcare system inertia toward adopting AI tools\n  - \"Black box nature of AI\" creates reluctance among practitioners\n  - Education gaps on new technologies limit adoption\n- Supply chain vulnerabilities in traditional staining\n  - US labs depend on staining chemicals from China and India\n  - Upstream dyes/intermediates originate abroad even when kits are bottled domestically\n  - Supply chain shocks affect US labs logistically and financially\n- Tissue waste from traditional staining methods\n  - Chemical staining is irreversible and consumes tissue\n  - Common biopsy locations frequently yield inadequate samples\n  - Repeat biopsies create healthcare costs and patient burden\n- Healthcare access disparities\n  - Pathology laboratories capable of various staining may be locally limited or financially prohibitive\n\n---",
      "notableExperiences": "- Virtual staining preserves tissue that traditional chemical staining destroys—a single tissue slice enables the stain while retaining material for the molecular tests frequently needed for diagnosis confirmation and therapy guidance\n- Common biopsy sites (breast, melanoma, colorectal, kidney, lung, thyroid) frequently trigger repeat sampling due to inadequate tissue, creating recurring costs and patient burden that virtual staining could prevent\n- Supply chain reality: while staining kits may be \"bottled in the US/EU,\" the upstream dyes and intermediates frequently originate in China and India, making US labs vulnerable to foreign supply disruptions\n- Professional societies (USCAP, DPA) are already collaborating with AI companies and have successfully worked with FDA on whole slide imaging clearances—a model for HHS engagement\n\n---",
      "keyQuotations": "- \"If a stained slide is reimbursed, whether or not that is accomplished by approved AI or chemical processes should be unimportant.\"\n- \"Repeat biopsies are a real, recurring source of healthcare cost and patient burden. If a virtually stained slide circumvents a re-biopsy, significant cost is saved, not to mention sparing the patient another invasive procedure, delays, frustration, and anxiety.\"\n- \"Current approaches for coding and coverage determination move more slowly than the advancement of technology, and thus hampers confidence in reimbursement that is necessary for bringing innovation to market.\""
    },
    "themeScores": {
      "4": 1,
      "4.3": 1,
      "6.4": 1
    },
    "entities": [
      {
        "category": "AI Applications in Healthcare",
        "label": "Virtual Staining"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Black Box AI"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Accreditation"
      },
      {
        "category": "Accreditation and Certification",
        "label": "CLIA"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Diagnosis"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Pathology"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "FDA"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Medical Coding"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Disparities"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Equity"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Pathologist"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Anxiety"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Diabetes"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Autonomy"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "NTAP"
      },
      {
        "category": "Professional Organizations",
        "label": "AdvaMed"
      },
      {
        "category": "Professional Organizations",
        "label": "DPA"
      },
      {
        "category": "Professional Organizations",
        "label": "USCAP"
      }
    ],
    "hasAttachments": true,
    "wordCount": 1252,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0021",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Scientific Knowledge Accelerator Foundation",
    "submitterType": "Organization",
    "date": "2026-01-23T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Feedback on how current HHS regulations impact AI adoption and use for clinical care\n  - FDA guidance document for Clinical Decision Support Software (January 2026, FDA-2017-D-6569) explains certain CDS software functions are \"excluded from the definition of device\"\n    - Exclusion requires meeting all 4 specific criteria\n    - Key criterion: software must be \"intended for the purpose of enabling such health care professional to independently review the basis for such recommendations that such software presents so that it is not the intent that such health care professional rely primarily on any of such recommendations to make a clinical decision\"\n  - In simple terms, CDS developers can avoid regulation by providing clear rationale/evidence for recommendations so healthcare professionals can independently decide\n  - FDA regulation of CDS software is limited to situations where CDS replaces clinical judgment without opportunity to apply clinical judgment\n  - Traditional human-generated recommendations have established forms for explaining rationale/evidence\n    - Ideally represent balanced view of benefits and harms (systematic review)\n    - Not selective views that only report evidence supporting the recommendation\n    - Often include certainty of or confidence in evidence\n    - Include judgments related to balance of benefits and harms\n    - Sometimes called evidence-based medicine (EBM) - representing answers based on systematic evaluation rather than selective reporting to justify desired answers\n  - AI-generated recommendations create new challenges\n    - Need to represent actual rationale/evidence used by AI and full consideration of relevant data inputs\n    - Must avoid selective reporting to justify AI recommendations\n    - High risk of bias if rationale/evidence is selected AFTER the answer is determined rather than through recognizing data that directly contributes to AI's decision making\n    - Generative AI can generate rationales to support its \"answer\" that are contextually incorrect\n    - Selective evidence for healthcare professional review undermines independence and allows AI to bias clinical care\n  - Requirements for reporting evidence supporting AI-generated recommendations\n    - Must be human-interpretable (to meet FDA guidance)\n    - Must be machine-interpretable (to enable accurate generation and representation by AI)\n    - Interoperable and re-usable methods would help everyone across the ecosystem achieve optimal communication\n  - We have developed an interoperable and re-usable method\n    - Builds on HL7 FHIR standard (Fast Healthcare Interoperability Resources) for health data exchange\n    - Expanded FHIR to support data exchange for science\n    - Described in Evidence Based Medicine on FHIR (EBMonFHIR) Implementation Guide at https://build.fhir.org/ig/HL7/ebm\n  - EBMonFHIR Implementation Guide details\n    - Defines profiles and value sets for representation of scientific knowledge\n    - Intended for developers of systems using FHIR for scientific knowledge data exchange\n    - Intended for authors of more specialized implementation guides\n    - Covers broad scope of scientific knowledge representation:\n      - Citations for identification, location, classification, and attribution of knowledge artifacts\n      - Components of research study design including eligibility criteria (cohort definitions) and endpoint analysis plans\n      - Research results including statistic findings, variable definitions, and certainty of findings\n      - Assessments of research results\n      - Aggregation and synthesis of research results\n      - Judgments regarding evidence syntheses and contextual factors related to recommendations\n      - Recommendations\n      - Compositions combining these knowledge types\n    - Covers syntactic interoperability (Resource StructureDefinitions) and semantic interoperability (value sets)\n    - Includes about 100 Profiles, Extensions, and ValueSets for data exchange of scientific knowledge\n\n- Feedback on ways HHS may invest in R&D to integrate AI in care delivery and create long-term market opportunities\n  - Many R&D opportunities across entire scope of EBMonFHIR Implementation Guide\n  - Specific highlighted opportunity: SummaryOfNetEffect Profile\n    - Structure definition for combining data elements related to a decision\n    - Data elements include:\n      - Specification of Population represented for decision\n        - Example: people with chronic kidney disease, anemia, being treated with dialysis\n      - Specification of Intervention represented for decision\n        - Example: erythropoiesis-stimulating agents (ESA) - medicines to stimulate red blood cell production\n      - Specification of Comparator represented for decision\n        - Example: no ESAs\n      - Specification of outcomes to be considered\n        - Example: needing blood transfusion, mortality, vascular access thrombosis\n      - For each outcome:\n        - Risk for outcome without treatment (or with Comparator)\n          - Could be derived from clinical research evidence or individual patient risk prediction\n        - Estimated change in outcome from treatment (Intervention)\n          - Should be derived from clinical research evidence\n        - Desirability of outcome (desirable or undesirable)\n          - Determines whether estimated change is benefit or harm\n        - Relative importance of outcome\n          - Could be derived from population surveys or individualized for personal decision making\n        - Net effect contribution (calculated by multiplying effect estimate and relative importance)\n      - Specification of net effect estimate (statistically combining net effect contributions)\n  - Example: SummaryOfNetEffect for ESAs for CKD patients with anemia receiving dialysis\n    - Available at https://fevir.net/resources/Composition/422055\n    - Evidence shown in both human-interpretable and machine-interpretable forms\n  - Further representation for individualized shared decision making at https://netbenefitcalculator.com/422055\n  - Nearly infinite combinations for sharing this evidence to support CDS\n  - HHS investment in R&D to establish common basis for sharing this data would create new long-term market opportunities\n  - Analogy: Navigation software (MapQuest, Google Maps) is advanced and used in infinite ways because of common expectations in data sharing with human-interpretable and machine-interpretable interfaces\n    - Currently no standardized navigation system (\"GPS\") for healthcare",
      "oneLineSummary": "A scientific standards foundation urges HHS to invest in their open FHIR-based framework for representing clinical evidence, arguing it would create a \"GPS for healthcare\" that enables AI to provide transparent, balanced rationale rather than biased post-hoc justifications.",
      "commenterProfile": "- **Name/Organization:** Scientific Knowledge Accelerator Foundation (SKAF)\n- **Type:** Academic/Research\n- **Role/Expertise:** Developers of the EBMonFHIR Implementation Guide, an HL7 FHIR-based standard for scientific knowledge exchange\n- **Geographic Scope:** National/International (developing interoperability standards)\n- **Stake in Issue:** Directly invested in adoption of their evidence representation framework for AI-generated clinical decision support",
      "corePosition": "We believe AI-generated clinical recommendations need standardized, machine-interpretable methods to represent their underlying evidence—not selective post-hoc justifications. Our EBMonFHIR Implementation Guide provides exactly this: an interoperable framework that enables both AI systems and healthcare professionals to work with transparent, balanced evidence. HHS investment in common data-sharing standards for clinical evidence would create transformative market opportunities, much like GPS standards enabled navigation software.",
      "keyRecommendations": "- Invest in R&D to establish and support a common basis for sharing clinical evidence data\n  - Would create new, long-term market opportunities similar to how GPS standards enabled navigation software\n- Support development and adoption of the EBMonFHIR Implementation Guide\n  - Provides human-readable, machine-interpretable, interoperable, and reusable method for AI-generated CDS\n  - Enables independent review by healthcare professionals as required by FDA guidance\n- Prioritize the SummaryOfNetEffect Profile for CDS adoption\n  - Combines population, intervention, comparator, outcomes, and net effect calculations in standardized format\n  - Supports individualized shared decision making",
      "mainConcerns": "- AI-generated recommendations risk providing biased, selective evidence rather than comprehensive systematic reviews\n  - Generative AI can generate contextually incorrect rationales to support its predetermined answers\n  - Evidence selected AFTER the answer is determined creates high risk of bias\n  - Selective evidence undermines healthcare professional's independent review\n- Current lack of standardized evidence representation for AI-generated CDS\n  - No \"GPS for healthcare\" exists despite need for common data-sharing expectations\n  - Without standards, AI can bias clinical care while appearing to meet FDA transparency requirements\n- FDA guidance requires human-interpretable evidence, but AI also needs machine-interpretable formats\n  - Both requirements must be met simultaneously for accurate generation and representation",
      "notableExperiences": "- Developed working examples demonstrating their framework in practice\n  - SummaryOfNetEffect for ESAs in CKD patients with anemia on dialysis (https://fevir.net/resources/Composition/422055)\n  - Net benefit calculator for individualized shared decision making (https://netbenefitcalculator.com/422055)\n- Created approximately 100 Profiles, Extensions, and ValueSets within EBMonFHIR to guide scientific knowledge data exchange\n- Identified a critical gap in FDA guidance: AI can technically comply with transparency requirements while still biasing decisions through selective post-hoc evidence generation",
      "keyQuotations": "- \"There is a high risk of bias if the rationale/evidence reported for 'independent review' by the health care professional is selected AFTER the answer is determined by the AI rather than through a process of recognizing the data that contributes directly to the AI's decision making.\"\n- \"Navigation software (MapQuest, Google Maps) is far advanced today and used in nearly infinite ways because there are common expectations in data sharing with both human-interpretable and machine-interpretable interfaces. But there is currently no standardized navigation system ('GPS') for healthcare.\"\n- \"Generative AI can generate rationales to support its 'answer' that are contextually incorrect. By providing selective evidence for the healthcare professional to review, the review is no longer independent and the AI can bias clinical care.\""
    },
    "themeScores": {
      "2": 1,
      "5": 1,
      "2.1": 1,
      "2.2": 1,
      "5.4": 1,
      "5.5": 1
    },
    "entities": [
      {
        "category": "AI Applications in Healthcare",
        "label": "Risk Prediction"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Algorithmic Bias"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Generative AI"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Shared Decision-Making"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Treatment"
      },
      {
        "category": "Health Information Systems",
        "label": "CDS"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "FHIR"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "HL7"
      }
    ],
    "hasAttachments": true,
    "wordCount": 1764,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0022",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Interstella",
    "submitterType": "Organization",
    "date": "2026-01-23T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Introduction\n  - Interstella appreciates the opportunity to respond to HHS's RFI on accelerating AI adoption in clinical care\n  - As a health data and technology company focused on real-time data integration and AI readiness, we share HHS's vision of a \"forward-leaning, industry-supportive, and secure\" approach\n  - We believe AI's transformative potential can only be realized by addressing foundational challenges in data infrastructure, governance, and incentives\n  - Our comments emphasize that AI systems fail or succeed based largely on the data that powers them\n    - A modernized data ecosystem is critical to safe and effective AI adoption\n  - Interstella Background\n    - Dedicated to turning raw, fragmented health information into real-time, AI-ready intelligence\n    - Experience developing the Lynqsys data platform and Data Refinery as a Service (DRaaS) solutions\n    - Gives us practical insight into data quality, interoperability, and timeliness issues limiting AI's clinical impact\n  - We approach this from the perspective of those building, buying, evaluating, and using AI tools for care delivery, and those who wish to do so but face barriers\n\n- Barriers to AI Adoption in Clinical Care\n  - Data Readiness Gap\n    - The core barrier: AI innovation is outpacing the healthcare system's data readiness\n    - Federal initiatives increasingly rely on advanced analytics and AI, yet most healthcare data ecosystems operate with significant limitations\n    - Delayed Data Availability\n      - Clinical data often ingested in batch processes with lag times of days or weeks, not real-time streams\n      - This stifles AI tools requiring up-to-date information for decision support or predictive analytics\n      - Quality measures or care gap alerts powered by AI cannot function optimally if data arrives long after the point of care\n    - Inconsistent Data Quality and Provenance\n      - Healthcare data remains \"fragmented, biased, and often a mess\"\n      - Leads to noise, duplication, and gaps that undermine AI algorithms\n      - Data from diverse sources (EHRs, claims, devices, patient-reported inputs) with inconsistent formats and unknown provenance\n      - Most failed AI initiatives fail because the data foundation is not fit for purpose\n      - Studies estimate up to 85% of AI projects fail due to poor data quality or volume issues\n      - Providers are slower to adopt AI when data are incomplete or siloed\n    - Limited Support for Unstructured and Behavioral Data\n      - Current systems handle structured data (diagnoses, lab results) but have limited capability for unstructured data (clinical notes, images) or behavioral and social health data\n      - Critical information about mental health, social determinants, or care preferences may be documented in text or external systems and thus invisible to AI tools\n      - Lack of behavioral health data integration makes it difficult to apply AI for outcome measurement in mental health or coordinate holistic care in models like CCBHCs\n    - Post-Hoc Governance vs Inline Controls\n      - Data governance (privacy protections, quality checks, consent management) often applied after data aggregation or as periodic audit, not inline at point of data creation\n      - This reactive approach means potential issues caught late\n      - AI systems require trusted, well-characterized data streams\n      - Without proactive governance baked into data pipelines, organizations either inhibit data sharing or expose themselves to compliance risks\n      - Data governance that isn't real-time and automated becomes a bottleneck to scalable AI\n    - Data Pipelines Not Designed for AI Consumption\n      - Traditional health IT systems built for billing or record-keeping, not for feeding machine learning models\n      - Data may be locked in PDF reports, faxed forms, or legacy databases that AI cannot readily parse\n      - Even within modern EHRs, data exchange can miss context that AI algorithms need\n      - EHR systems are largely not compatible across different hospitals or providers, leading to localized data rather than comprehensive patient view\n      - AI models trained on narrow, siloed data may not generalize\n    - These data-related barriers create technical hurdles and raise regulatory, equity, and trust risks\n    - An AI tool is only as good as the data it learns from\n    - \"AI systems do not fail because of algorithms; they fail because of data\"\n    - Addressing data readiness is foundational to everything else\n    - We encourage HHS to treat data quality, interoperability, and timeliness as strategic priorities in all AI-related policies\n\n  - Other Key Barriers\n    - Lack of Trust and Transparency\n      - Many AI algorithms are \"black boxes\" that clinicians find hard to interpret\n      - Opacity of AI decisions erodes provider trust and willingness to use AI\n      - Clinicians remain ultimately accountable for patient outcomes, so they are cautious about tools they don't fully understand\n      - Without assurances of transparency or robust validation, frontline providers default to familiar non-AI solutions\n      - Building trust requires technological solutions (explainable AI, rigorous validation) and cultural change (education, success stories, clinician engagement in AI development)\n      - Transparency, privacy protections, and rigorous evaluation identified as essential elements of trustworthy AI\n    - Regulatory Uncertainty\n      - Current regulations have not fully caught up with AI\n      - Privacy rules like HIPAA and data use policies can make it difficult to aggregate and share data needed for robust AI models\n      - Approval pathways for AI, particularly software not regulated as a medical device, are unclear\n      - Developers unsure how to demonstrate safety/effectiveness; providers unsure what oversight or certifications to look for\n      - Liability concerns loom large: organizations worry about legal risk with uncertain liability-sharing between developers and users\n      - This \"chilling effect\" causes risk-averse stances, slowing adoption\n    - Misaligned Incentives in Payment\n      - Fee-for-service reimbursement often fails to reward (or even penalizes) innovations that improve efficiency or quality\n      - AI tools that prevent hospitalizations or automate routine tasks might reduce billable services in the short term\n      - Those bearing long-term risk (payers or ACOs) have incentives to use AI for prevention and cost reduction but face their own hurdles\n      - This misalignment leads to patchy adoption\n      - \"Legacy payment systems\" can diminish the promise of AI through inertia\n    - Workforce and Workflow Integration\n      - Successful AI adoption requires complementary process changes and training\n      - Introducing an AI tool means little if clinical workflows are not adjusted to incorporate AI output at the right time and place\n      - Many hospitals lack personnel with AI expertise (data scientists, informaticists) to champion changes\n      - Some clinicians fear AI could replace or de-skill their roles, creating resistance\n      - Administrative hurdles like lengthy procurement, integration costs, or lack of IT support slow deployments\n      - Roles most influential in AI adoption: Chief Medical Information Officer, Chief Data/Digital Officer, clinical department heads who see clear need\n      - Hospitals with professional management and integrated salaried structures tend to be more proactive than those with decentralized or physician-led management\n      - HHS should consider how to educate and empower these decision-makers to be AI champions\n\n  - Summary of Barriers\n    - Biggest barriers: data-related limitations, regulatory uncertainty (including privacy and liability), misaligned payment incentives, and lack of trust\n    - Overcoming these requires coordinated action\n    - We recommend HHS focus on improving the health data foundation as a first-class objective\n    - Doing so will mitigate many other barriers\n\n- Regulatory and Governance Recommendations\n  - HHS's role as the nation's principal health regulator is pivotal in shaping an environment where AI can innovate rapidly and safely\n  - We agree with HHS's goal of a regulatory posture that is predictable, proportionate to risk, and protective of patients' rights and privacy\n\n  - 1. Clarify AI Oversight for Non-Device Software\n    - Many AI tools in clinical care do not meet FDA's definition of a medical device\n      - Examples: clinician-facing decision support allowing independent review, operational AI for scheduling and resource allocation\n    - This creates a gray area with \"novel legal and implementation issues\" around accountability\n    - HHS (through ONC, CMS, OCR, etc.) should provide guidance on expectations for non-FDA-regulated AI tools\n    - Liability and Indemnification\n      - Urge HHS to convene stakeholders (providers, developers, malpractice insurers) to develop guidance or safe harbor principles\n      - Clinicians should know when they can rely on an AI tool and how liability is apportioned if the AI is flawed\n      - One approach: a \"qualified AI\" safe harbor where if an AI tool meets HHS-endorsed standards (transparency, validation), providers who use it as intended could have mitigated liability\n      - This would encourage use of vetted tools\n      - Developers of such tools might be expected to carry liability insurance or indemnify providers in cases of egregious failure\n      - Clear rules on \"who is liable if something goes wrong\" would likely increase adoption\n      - HHS could work with Congress if needed to adjust liability frameworks for AI in healthcare\n    - Privacy and Data Use Clarity\n      - HHS should update HIPAA and related guidance to explicitly address AI development and deployment\n      - Clarify how providers/payers can share de-identified or synthetic data for AI training without running afoul of privacy rules\n      - Consider promoting data trusts or safe data enclaves for AI development through OCR guidance\n      - HHS's AI Strategy emphasizes \"fostering data standards and datasets to bolster their usability for AI… while maintaining individual privacy\"\n      - HHS could create model frameworks for data stewardship that balance innovation with privacy\n      - Clear guidelines can reduce the fear that sharing data for AI equals a privacy violation\n    - Security and Compliance\n      - Many non-device AI tools still interface with protected health information\n      - HHS can help by streamlining compliance\n        - Template Business Associate Agreements for AI vendors\n        - Guidance on using cloud-based AI under HIPAA\n        - Mapping AI practices to existing regulations (42 CFR Part 2 for substance abuse data, ADA compliance in AI-driven communications)\n      - By addressing these now, HHS preempts future confusion as AI use grows\n\n  - 2. Modernize Clinical Decision Support (CDS) Regulation\n    - Current FDA guidance draws a line between regulated \"medical device\" software and unregulated CDS\n    - HHS should ensure developers and providers clearly understand this line\n    - Recommend HHS establish a central information hub on what kinds of AI applications require FDA clearance and which do not\n      - Could include an AI Tool Classification Framework mapping common use cases\n      - Examples: diagnostic algorithms (regulated SaMD) vs. workflow optimizers (unregulated) vs. patient self-care chatbots (gray area)\n      - Providing case studies or decision trees would reduce uncertainty\n    - For AI tools that are medical devices, HHS should support FDA in streamlining approval without compromising safety\n      - Expanding programs for predetermined change control plans (PCCPs) and real-world performance monitoring to allow continuous improvement post-approval\n    - For non-device tools, HHS could work with industry on voluntary certification instead of regulation\n      - Focus on transparency and oversight at the user-organization level (hospital governance)\n\n  - 3. Support AI Governance Frameworks in Healthcare Organizations\n    - Regulation is not just about federal rules; it's also about internal governance where AI is deployed\n    - We applaud efforts like the Joint Commission's new guidance (2025) on responsible use of AI in healthcare\n      - Developed with the Coalition for Health AI\n      - Outlines seven core elements (formal AI governance committees, patient transparency, bias monitoring) for safe AI implementation\n    - HHS should endorse and build upon such frameworks, perhaps by integrating them into ONC certification or CMS conditions of participation\n    - Specific recommendations:\n      - Coordinate with the Joint Commission and others to operationalize AI governance playbooks and create voluntary certification programs\n      - HHS could fund development of a \"Trustworthy AI in Clinical Care\" accreditation that hospitals can obtain by meeting best practices\n        - Similar to how HIMSS stages measure EMR adoption\n      - Federal backing would lend credibility and spur adoption of these standards\n      - Issue guidance or requirements for federal healthcare programs (Medicare, Medicaid) to ensure participating providers have basic AI governance\n        - Might start as recommendations and evolve into conditions for certain innovation models\n        - Areas to cover: having an AI oversight committee, conducting algorithmic bias assessments, educating staff and informing patients about AI use\n        - Joint Commission calls for patient notification and consent when AI directly impacts care\n      - Such governance expectations, even if not yet mandated, signal the industry to develop AI responsibly\n\n  - 4. Adjust and Harmonize Regulations to Remove Unintended Barriers\n    - Encourage HHS to review specific regulations that may hinder AI adoption\n    - Revisit Data Blocking and Sharing Rules\n      - The 21st Century Cures Act information blocking rules promote data sharing, which is great for AI\n      - HHS should ensure these rules explicitly allow sharing data with AI developers or between providers for AI purposes (with proper authorization or de-identification)\n      - Consider expanding required data elements (USCDI) to include more data types useful for AI (imaging, free-text clinical notes, patient-generated data)\n      - Enhanced interoperability of specific data types would \"widen market opportunities, fuel research, and accelerate AI development\"\n      - Better access to claims data could allow AI to identify high-risk patients across the continuum of care\n      - Richer social and behavioral data standards would enable AI to personalize care plans\n      - CMS's recent Interoperability Framework is a positive step\n        - Will require networks to share clinical and claims data via standardized APIs (FHIR) within 24 hours\n        - Dramatically improving timely data availability\n      - HHS should continue to remove technical and economic disincentives to interoperability\n      - Making near-real-time data exchange the norm will empower AI applications that need up-to-date data (sepsis early warning systems, dynamic care coordination tools)\n    - HIPAA Safe Harbors for AI Development\n      - Clarify pathways for using patient data to develop AI\n      - HHS could create a safe harbor under HIPAA for uses of fully de-identified data (perhaps certified by a third party) in bona fide R&D of AI, even if the resulting tool will be commercial\n      - Some covered entities are overly conservative, fearing any data sharing might violate HIPAA\n      - Explicit encouragement and examples in OCR guidance would be helpful\n        - Example: \"It is permissible to share de-identified EHR datasets with an AI developer for algorithm training, under XYZ conditions\"\n      - Update CFR 42 Part 2 in partnership with SAMHSA to align substance use data sharing rules with modern AI-driven care coordination\n      - AI that coordinates behavioral health care (like predicting relapse risk) can only be effective if data can be shared within care teams under clear, privacy-protected frameworks\n    - Streamline IRB and Research Regulations for AI Studies\n      - Many healthcare AI tools need continuous learning and improvement, blurring the line between quality improvement and research\n      - HHS should consider guidance on when AI development/validation constitutes human subjects research requiring IRB oversight\n      - Overly burdensome interpretations could slow needed real-world evaluation\n      - Developing central IRB models or data commons for AI could accelerate evaluation while maintaining ethics\n    - Enable Sandbox or Pilot Programs\n      - HHS might use its authority (through CMS Innovation Center or ONC) to create regulatory sandboxes where healthcare organizations can pilot AI solutions with certain regulatory flexibilities\n      - Example: a hospital system could apply to test an AI tool using novel data integration, under a time-limited program where HHS waives certain documentation or reporting rules, provided strong monitoring is in place\n      - Such sandboxes, coupled with rigorous evaluation, can demonstrate safe ways to adjust regulations in the long run\n\n  - Summary of Regulatory Recommendations\n    - HHS should reduce uncertainty by providing clear rules of the road for AI\n    - This includes clarifying data sharing permissions, aligning AI oversight with risk levels, and promoting internal governance standards\n    - By revisiting privacy, security, and liability issues in light of AI, HHS can create a predictable environment that \"enables rapid innovation while protecting patients\"\n    - These regulatory adjustments, focused especially on data and trust, will unlock innovation by giving both AI developers and healthcare users confidence to move forward\n\n- Payment Policy and Reimbursement Incentives\n  - Payment policy can drive or stall AI adoption in clinical care\n  - We agree with HHS's assessment that current reimbursement systems contain \"inherent flaws\" that may unintentionally discourage high-value innovations\n  - HHS should modernize payment models to reward effective AI use, reduce financial friction, and align incentives across stakeholders\n\n  - 1. Create Clear Reimbursement Pathways for AI-Enabled Services\n    - One of the \"biggest barriers\" is the lack of clear payment mechanisms for AI-powered interventions\n    - If a clinician uses an AI tool to analyze a radiology image or predict patient deterioration, there is often no distinct billing code or reimbursement\n    - Recommendations:\n      - Establish CPT codes or modifiers for AI-assisted services\n        - Engage the AMA CPT process to create codes for procedures where AI is used\n        - Example: a radiologist using FDA-cleared AI for image analysis could append a modifier indicating AI assistance, reimbursed with a small add-on payment for validated efficiency or accuracy gain\n        - CMS could then decide to cover these codes\n        - Early examples include CPT codes for autonomous AI detection of diabetic retinopathy\n        - HHS should accelerate and expand such coding to cover various AI scenarios (AI-assisted pathology, AI-driven care coordination)\n        - This signals to providers they can get reimbursed when using proven AI tools\n      - Bundle AI into Value-Based Payments\n        - Under APMs like accountable care or Medicare Advantage, the business case for AI is stronger\n        - Preventing a hospitalization saves money that the provider-network can keep\n        - HHS should integrate AI adoption into value-based programs\n          - Provide allowances or bonuses for AI investments that demonstrably improve outcomes\n          - CMS Innovation Center could pilot contracts where participants deploying AI for high-impact use cases receive shared savings or upfront funding\n        - The key is to tie AI to outcomes: if it lowers costs or improves quality, allow those benefits to translate into financial rewards for adopters\n      - Coverage with Evidence Development (CED) for AI\n        - For novel AI technologies (high-cost or high-impact), CMS could use a CED approach\n        - Cover the AI tool for Medicare beneficiaries conditionally while data is gathered on effectiveness\n        - Has been used for some devices and could be applied to AI-based diagnostic software or digital therapeutics\n        - Encourages early adoption in a controlled way and speeds evidence generation needed for broader coverage\n\n  - 2. Align Payment Incentives with Quality and Prevention\n    - Many AI tools promise long-term savings by improving prevention or care management, but under fee-for-service those savings are not realized by the provider\n    - HHS should continue shifting incentives from volume to value, which inherently makes AI more attractive\n    - Specific ideas:\n      - Incorporate AI-driven improvements into quality measures and payment programs\n        - If an AI tool can document care more thoroughly or identify care gaps in real time, that should help providers score higher on quality metrics (HEDIS measures, Medicare Star ratings)\n        - CMS can explicitly allow AI documentation or predictions to count toward meeting measure thresholds, as long as they meet validation criteria\n        - By tying AI to quality bonuses or penalties avoidance, providers have a financial reason to adopt\n      - Modernize Medicaid and Medicare coverage policies\n        - A barrier in Medicaid is uncertainty about whether costs for AI software or services are reimbursable\n        - CMS could issue guidance to state Medicaid programs on classifying AI expenses as allowable administrative costs or medical expenses\n        - Example: clarifying that subscription fees for an AI-powered care management platform can be counted in medical loss ratio (MLR) calculations for managed care\n        - Or offering federal match for state investments in AI pilots\n        - Medicaid's hesitancy to fund innovations \"even when cost-saving potential is evident\" is a challenge\n        - Lack of financial incentive discourages state investment in AI\n        - Clear CMS policy and perhaps small grant programs could nudge states to experiment with AI that improves care for vulnerable populations\n      - Address Unintended Consequences\n        - While encouraging AI, HHS must guard against potential overuse or fraud (unnecessary tests due to AI alerts)\n        - Payment policy can help by requiring evidence of value\n        - New AI-related billing codes could initially be reimbursed only in context of approved registries or when providers adhere to appropriate use criteria\n        - This ensures we reward AI that truly adds value\n        - On the flip side, AI can be a tool to reduce fraud and waste, as CMS's WISeR initiative suggests\n        - We support using AI for program integrity (identifying improper claims), which ultimately frees resources for patient care\n\n  - 3. Foster Competition and Accessibility of AI Tools\n    - To avoid a scenario where only large health systems can afford AI, HHS should leverage reimbursement levers to promote competition and lower costs\n    - Ensure AI Pricing Transparency and Fair Pricing Models\n      - If AI tools become part of care, their cost (often via software licenses) can impact overall spending\n      - HHS could work with stakeholders to develop pricing guidelines or reference pricing for certain AI-enabled services\n      - Example: if a particular AI diagnostic test is covered, CMS might set the payment rate based on value delivered rather than what the vendor charges\n      - This prevents \"massive spending bubbles on concentrated items\" that don't reflect value\n    - Coverage of AI as an enabler rather than a separate service\n      - Not every AI needs its own payment; some should be considered part of doing business in a digital health system\n      - One approach: allow AI software costs to be included in Medicare's reimbursement formulas (hospital inpatient rates or practice expense calculations for physicians)\n      - If AI reduces labor or improves outcomes, hospitals and practices could be paid slightly more for adopting it\n      - Over time as it becomes standard, it's just part of the normal payment\n      - This incremental inclusion avoids big payment cliffs and keeps competition open\n    - Encourage Open-Source and Public-Utility AI through Grants\n      - Reimbursement isn't the only way to finance AI\n      - HHS, via research grants or public-private partnerships, can support development of open algorithms\n        - Example: an AI model for predicting hospital readmission that any hospital can use free of charge\n      - If successful, this reduces dependence on high-cost proprietary solutions and pressures the market to compete on quality and support services rather than secret algorithms\n\n  - Summary of Payment Recommendations\n    - Payment modernization is essential\n    - HHS should \"ensure payers have the incentive and ability to promote access to high-value AI clinical interventions\"\n    - By carving out payment pathways for AI, aligning incentives with outcomes, and funding innovation, HHS can overcome the inertia of legacy payment systems\n    - This will signal to the market that if AI demonstrably improves care, it will be rewarded\n\n- Research & Development Priorities and Data Infrastructure\n  - Accelerating AI adoption requires investment in R&D and infrastructure to integrate AI into care delivery\n  - HHS oversees one of the world's largest health research ecosystems\n  - Opportunities for HHS to catalyze innovation through targeted research funding, partnerships, and demonstration projects\n\n  - 1. Invest in Data Modernization for AI\n    - The healthcare data infrastructure must be upgraded to support AI at scale\n    - We urge HHS to prioritize R&D in data architectures that deliver \"better data, faster, with intelligence and trust\"\n    - Real-Time Data Streams\n      - Support development of technologies and standards for real-time health data exchange\n      - HHS could fund pilots where health systems implement streaming APIs (leveraging FHIR subscriptions or event messaging) for critical data like hospital admissions, lab results, or vital signs to enable immediate AI analysis\n      - Our experience with Interstella's Lynqsys platform shows near-real-time data refinement can greatly enhance AI accuracy in use cases like predictive quality measures\n      - A Medicaid pilot might demonstrate how streaming data from managed care plans and providers to a state analytics platform yields faster care interventions\n        - Example: identifying a high-risk pregnancy for outreach\n      - Results from such pilots can guide standards and best practices for real-time health information flows\n    - Data Standardization and Integration\n      - R&D should focus on tools to normalize and curate diverse data types for AI consumption\n      - Could involve expanding standard vocabularies to cover behavioral health, social factors, device data\n      - Or developing AI methods to automate data cleaning\n      - HHS's AI Strategy calls for \"championing efforts to integrate real-world data with electronic health records toward precision medicine and public health\"\n      - Recommend HHS fund projects that combine EHR data with non-traditional data (pharmacy, wearable, community health data) in a privacy-preserving way, demonstrating improved outcomes\n      - Another aspect: inline data governance\n        - R&D into technologies that enforce data quality rules and privacy checks at the moment of data entry or exchange\n        - Using AI to flag anomalies or PHI breaches in data pipelines\n        - Would give AI developers more confidence in the data they use\n    - AI-Ready Data Repositories\n      - HHS could establish shared datasets and testbeds for AI in clinical care\n      - Example: a national repository of de-identified health records, or synthetic patient data that mimics real distributions, for high-priority conditions\n      - Would lower the barrier for startups or researchers to train and evaluate algorithms, especially for underrepresented populations where data paucity is an issue\n      - NIH and ONC can collaborate (building on NIH's National COVID Cohort Collaborative to inform non-COVID datasets)\n      - Emphasis should be on datasets that are representative (to mitigate bias) and include longitudinal outcomes to assess AI impact\n    - Interoperability and Standards for AI\n      - Need to develop standards that specifically support AI interoperability and accountability\n      - HL7 and others have begun asking \"What's the FHIR for AI?\"\n      - HHS should support these efforts (perhaps via grants to standards development organizations or challenge competitions) to define how AI systems can exchange information about their models, provenance, and results\n      - Example: standard tags or metadata indicating an EHR data element was generated by AI, as the new HL7 \"AI Transparency on FHIR\" project is tackling\n      - Also, common formats for model documentation and monitoring results\n      - The goal: by standardizing how we represent AI outputs and performance, it becomes easier to integrate multiple AI tools and monitor them across different platforms\n      - This is akin to requiring a nutritional label on AI models, enabling \"apples to apples\" comparison and oversight\n      - HHS could further fund development of an open benchmarking platform where AI tools can be evaluated on standard datasets for metrics like bias, robustness, and generalizability\n      - Such benchmarks (similar to NIST's role in other industries) would accelerate improvement and trust in AI\n\n  - 2. Focus R&D on High-Impact Use Cases\n    - Recommend HHS identify specific areas of care where AI could drive significant improvements and concentrate research funding or prize competitions there\n    - Quality Measurement and Improvement (CMS Programs)\n      - AI can automate and enhance quality reporting (extract clinical quality measures from narrative notes) and provide real-time quality feedback to clinicians\n      - R&D could involve piloting AI in Medicare value-based programs to continuously calculate metrics like hospital readmission rates or diabetes control, rather than retrospective chart review\n      - This near real-time refinement supports faster, more accurate quality improvement\n      - HHS could sponsor a demonstration where an AI analyzes live data feeds from a few hospitals to identify patients falling through the cracks on preventive care, and measures if intervening earlier improves outcomes and lowers penalties\n    - Behavioral Health and Substance Use\n      - Priority areas with workforce shortages\n      - AI (conversational agents or predictive analytics) could expand access and personalize behavioral health care\n      - Use cases:\n        - AI therapy assistants or chatbots to support patients between therapy sessions\n        - Predictive models to identify when patients with serious mental illness are at risk of crisis, so mobile teams can intervene\n      - Specific R&D focus might be on CCBHCs and community mental health settings\n        - Exploring how normalized behavioral health data and AI-driven insights can improve outcomes without increasing clinician burden\n      - HHS could fund a project to integrate behavioral health EHR data, social service data, and patient self-reported mood data, using AI to flag individuals who need proactive outreach\n      - Results could inform best practices for data integration and risk modeling in this sensitive arena\n    - Public Health Surveillance and Preparedness\n      - Recent years have shown the need for better public health intelligence\n      - AI can analyze streaming healthcare data (ER visits, lab results, even social media or environmental sensors) to detect emerging outbreaks or population health threats\n      - HHS (with CDC) should invest in AI for syndromic surveillance, ensuring legal and technical frameworks allow de-identified data to flow from clinical systems to public health in real time\n      - Example use case: an AI model that scans regional health system data for anomaly clusters (respiratory complaints indicating a flu surge) and provides public health officials with early warnings and situational awareness dashboards\n      - The Data Modernization Initiative at CDC could incorporate AI pilots\n      - HHS could fund joint projects between health systems and public health departments to demonstrate how \"streaming curated data\" improves response timing and outcomes in events like sepsis outbreaks or natural disasters\n    - AI Innovation Test Beds\n      - HHS might establish AI Innovation Pilot programs where agencies (CMS, VA, or IHS) partner with AI developers to trial tools in real-world clinical environments with appropriate safeguards\n      - Could be done via Cooperative Research and Development Agreements (CRADAs) or CMS Innovation Center challenges\n      - The idea: give AI developers a sandbox with access to trusted, well-characterized datasets or environments (such as volunteer hospitals) so they can refine tools and prove value\n      - Example: CMS could run an \"AI for Care Transitions\" challenge\n        - Provide participants with a limited Medicare dataset and a sponsoring health system\n        - Ask them to deploy AI that reduces 30-day readmissions\n      - Agencies can safely pilot AI tools by controlling scope and monitoring outcomes closely\n      - These pilots yield valuable evidence on costs, benefits, and implementation issues, informing wider adoption\n\n  - Through targeted investments in such use cases, HHS will generate published findings on the impact of adopted AI tools in clinical care\n  - Early evidence suggests AI can improve specific outcomes\n    - An AI mental health chatbot reduced depression and anxiety significantly in a controlled trial\n    - AI-driven screening tools have matched expert accuracy in detecting diabetic eye disease or lung nodules\n  - However, literature also notes the costs and benefits vary by context\n    - Some AI solutions reduce operational costs (by automating documentation)\n    - Others might increase costs initially (more testing triggered by AI alerts) but prevent expensive events later\n  - HHS should ensure R&D evaluations measure total cost of care and outcome transfers\n    - If AI shifts costs from hospitals to outpatient or from payers to patients, that should be analyzed\n  - By funding comprehensive evaluations (including economic analysis), HHS can build a knowledge base for the health system to understand where AI truly adds value\n\n  - 3. Support Workforce Development and User-Centered Design Research\n    - In addition to technical R&D, HHS should invest in the human factors of AI adoption\n    - Research questions include:\n      - How to present AI recommendations to clinicians for maximal usability\n      - How patients react to AI involvement in their care\n      - What training healthcare workers need to effectively use AI tools\n    - Example: AHRQ could fund studies on AI in clinical workflow\n      - Identifying best practices for integrating AI alerts into EHR interfaces without causing alert fatigue\n      - Determining competencies needed for a new role like \"AI operations manager\" in hospitals\n    - ONC could incorporate AI usability into its health IT usability research portfolio\n    - HHS might encourage inclusion of patients and caregivers in co-design of AI solutions\n      - Researching how to make AI outputs transparent and acceptable to lay users\n    - This will address patient trust directly by involving them in the innovation process\n      - Aligns with the principle that \"trust and provider relationships should be key considerations\" in AI governance\n\n  - Conclusion on R&D\n    - A robust R&D agenda spanning data infrastructure, targeted innovation in priority areas, and human-centered implementation science is critical to translate AI from promising concept to routine clinical practice\n    - HHS's support (through funding, convening power, and public-private partnerships) can create the long-term market opportunities and evidentiary foundation needed to improve health and well-being for all Americans via AI\n    - Interstella stands ready to collaborate on these initiatives, particularly where our expertise in data curation and real-time analytics can help build the necessary foundations\n\n- Promoting Trust, Evaluation, and Accountability in AI\n  - For AI to be widely adopted in clinical care, patients and providers must trust that these tools are safe, effective, and fair\n  - Trust is built through robust evaluation, transparency, and ongoing accountability\n\n  - 1. Rigorous Pre-Deployment Evaluation\n    - Before AI tools are used in patient care, they should undergo thorough testing akin to clinical validation\n    - Promising evaluation methods for non-device AI include:\n      - Retrospective Validation on Diverse Data\n        - Testing AI algorithms on retrospective patient datasets from multiple sources to assess accuracy and generalizability\n        - Should include subset analysis to detect biases\n        - Example: ensuring an AI diagnostic tool performs equitably across races, ages, and genders\n        - HHS could encourage use of public evaluation datasets so claims of performance can be verified\n        - Metrics like sensitivity, specificity, and AUC for clinical outcomes are standard\n        - Equally important are fairness metrics (performance disparity between groups) and calibration (reliability of predictive probabilities)\n      - Prospective Simulation or Shadow Trials\n        - Before full deployment, AI tools can be run in \"shadow mode\" where they make predictions or recommendations that are recorded but not acted upon\n        - Clinicians continue standard care\n        - Outcomes are then compared to see if the AI could have made a difference or if it behaved safely\n        - This prospective test builds confidence without risk to patients\n        - We suggest HHS promote such approaches, perhaps by clarifying with FDA and OHRP that certain prospective AI evaluations might be considered quality improvement rather than human subjects research (if no patient risk), thereby simplifying execution\n      - Human Factors and Workflow Evaluation\n        - Promising AI is not just about predictive accuracy, but about fit in workflow\n        - Pre-deployment testing should include simulation of how clinicians interact with the AI\n          - Is the alert timing appropriate?\n          - Do users understand the AI output?\n          - Does it actually save time or add burden?\n        - Methods like usability testing, A/B testing interfaces, and gathering clinician feedback on prototype recommendations are invaluable\n        - HHS could fund grants for clinical AI usability labs that help vet tools from a human-centered perspective\n    - HHS might further support these processes by establishing a voluntary scoring system or \"AI readiness level\" akin to technology readiness levels, which incorporates evaluation criteria\n    - Contracts, grants, or prize competitions could be effective\n      - Example: an H"
    },
    "themeScores": {
      "1": 1,
      "2": 1,
      "3": 1,
      "4": 1,
      "5": 1,
      "6": 1,
      "7": 1,
      "8": 1,
      "9": 1,
      "1.4": 1,
      "1.5": 1,
      "2.1": 1,
      "2.2": 1,
      "2.5": 1,
      "3.1": 1,
      "3.2": 1,
      "4.1": 1,
      "4.2": 1,
      "4.3": 1,
      "4.6": 1,
      "5.1": 1,
      "5.3": 1,
      "6.1": 1,
      "6.2": 1,
      "6.4": 1,
      "6.8": 1,
      "7.2": 1,
      "8.1": 1,
      "8.2": 1,
      "9.3": 1
    },
    "entities": [
      {
        "category": "AI Applications in Healthcare",
        "label": "Chatbot"
      },
      {
        "category": "AI Applications in Healthcare",
        "label": "Early Warning System"
      },
      {
        "category": "AI Applications in Healthcare",
        "label": "Predictive Analytics"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "AI Governance"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Safe Harbor"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Shadow Mode"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Trustworthy AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Agentic AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Algorithmic Bias"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Explainable AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Machine Learning"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "SaMD"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Accreditation"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Certification"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Conditions of Participation"
      },
      {
        "category": "Accreditation and Certification",
        "label": "FDA Clearance"
      },
      {
        "category": "Accreditation and Certification",
        "label": "ONC Certification"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Care Coordination"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Diagnosis"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Informed Consent"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Screening"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Behavioral Health"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Pathology"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Population Health"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Precision Medicine"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Radiology"
      },
      {
        "category": "Data Privacy and Security",
        "label": "BAA"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Governance"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Provenance"
      },
      {
        "category": "Data Privacy and Security",
        "label": "De-identification"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Information Blocking"
      },
      {
        "category": "Data Privacy and Security",
        "label": "PHI"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "AHRQ"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CDC"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS Innovation Center"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "FDA"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "IHS"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "NIH"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "NIST"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "OCR"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "OHRP"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "ONC"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "SAMHSA"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "VA"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Abuse"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Fraud"
      },
      {
        "category": "Fraud and Compliance",
        "label": "WISeR"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Waste"
      },
      {
        "category": "Health Information Systems",
        "label": "CDS"
      },
      {
        "category": "Health Information Systems",
        "label": "EHR"
      },
      {
        "category": "Health Information Systems",
        "label": "EMR"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Claims Processing"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Medical Coding"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Quality Reporting"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Pilot Program"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Procurement"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Total Cost of Care"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Workflow Integration"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "CPT"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "FHIR"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "HL7"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "USCDI"
      },
      {
        "category": "Healthcare Programs",
        "label": "ACO"
      },
      {
        "category": "Healthcare Programs",
        "label": "APM"
      },
      {
        "category": "Healthcare Programs",
        "label": "CCBHC"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicaid"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare Advantage"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Alert Fatigue"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "HEDIS"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Equity"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Quality Improvement"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Star Ratings"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Ambulatory Care"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Emergency Department"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Health System"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "CMIO"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Physician"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Radiologist"
      },
      {
        "category": "Laws and Regulations",
        "label": "21st Century Cures Act"
      },
      {
        "category": "Laws and Regulations",
        "label": "42 CFR Part 2"
      },
      {
        "category": "Laws and Regulations",
        "label": "ADA"
      },
      {
        "category": "Laws and Regulations",
        "label": "HIPAA"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Indemnification"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Malpractice"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Anxiety"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Depression"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Diabetes"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Diabetic Retinopathy"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Sepsis"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Trust"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "CED"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "Fee-for-Service"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "MLR"
      },
      {
        "category": "Professional Organizations",
        "label": "AMA"
      },
      {
        "category": "Professional Organizations",
        "label": "Coalition for Health AI"
      },
      {
        "category": "Professional Organizations",
        "label": "HIMSS"
      },
      {
        "category": "Professional Organizations",
        "label": "Joint Commission"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "CRADA"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "IRB"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "Implementation Science"
      },
      {
        "category": "Social Determinants of Health",
        "label": "SDOH"
      }
    ],
    "hasAttachments": true,
    "wordCount": 9546,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0023",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Association of periOperative Registered Nurses (AORN)",
    "submitterType": "Organization",
    "date": "2026-01-23T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Introduction and organizational background\n  - Writing on behalf of the Association for perioperative Registered Nurses (AORN) in response to HHS's December 23rd request for information on accelerating AI adoption in clinical care\n  - AORN defines, supports, and advocates for patient and staff safety through exemplary practice in all phases of perioperative nursing care\n  - Uses evidence-based guidelines, continuing education, and clinical practice resources\n  - AORN's evidence-based Guidelines for Perioperative Practice address 36 perioperative practice areas and set the gold standard for interdisciplinary perioperative practice\n  - New AORN Guideline for Integration of Artificial Intelligence scheduled for publication May 21, 2026\n    - Based on PRISMA systematic review methodology\n    - Available for public comment from Dec. 23, 2025 through January 31, 2026\n\n- AORN's position on AI in healthcare\n  - Agrees with HHS on importance of healthcare providers effectively using AI-enabled medical devices and tools\n    - To enhance productivity, reduce burden, lower costs, and improve health outcomes\n  - Has adopted an evidence-informed position that is simultaneously optimistic and cautious\n  - AI is suitable for augmentation of clinical decision making, but cannot and must never replace human clinical judgment\n  - Human expertise, empathy, and accountability required for clinical decision-making cannot be replicated\n  - Clinicians should be full partners in preparing for the future of AI in their practice settings\n  - AI integration methods matter\n    - When integrated ethically and responsibly, AI can improve patient outcomes, patient experience, and healthcare worker experience\n    - When ineffectively integrated, AI has potential to do harm\n  - Risk mitigation should be prioritized and strategies must be nimble and responsive to the ever-evolving AI risk landscape\n    - Both known and unanticipated risks\n  - AI education is essential\n    - Should be implemented in academic settings and in organizations where AI-enabled technology is used\n    - Competency should be assessed before using any AI-enabled technology\n\n- Question 1: Biggest barriers to private sector innovation in AI for healthcare\n  - Lack of involvement of end-users throughout planning and development processes\n    - Healthcare professionals whose job roles intersect with the AI tool used in clinical care\n  - Insufficient reporting and publication of AI tool development, testing, and validation processes\n  - Limited ability for healthcare organizations to effectively evaluate, monitor, and maintain AI tools after deployment\n    - Issues of interoperability with existing technology infrastructure\n    - Internal network capabilities\n  - Regulatory uncertainty related to approvals and accountability\n  - Resource constraints and interoperability issues\n    - Especially in under-resourced healthcare organizations (rural, urban, non-academic medical centers)\n  - Clinician trust\n  - These barriers contribute to disconnect between private sector enthusiasm, funding influx, and real-world implementation\n  - Successful development, implementation, and maintenance require early and intentional interdisciplinary collaboration\n    - Among technology developers and end-users (healthcare organizations, healthcare professionals)\n    - To identify and address internal and external barriers\n  - Developers must prioritize transparent reporting and publication\n    - Development processes, data collection and processing activities, performance evaluation\n    - Internal and external validation of AI performance in real-world applications\n    - Supports user trust and adoption\n  - Resource constraints present unique obstacle\n    - Literature indicates many healthcare organizations lack technology infrastructure capable of meeting increased demands for computational power and data-handling\n\n- Question 2: Regulatory, payment policy, or programmatic design changes HHS should prioritize\n  - HHS should prioritize changes to Code of Federal Regulations related to de-identification and re-identification of individually identifiable health information and AI use\n  - Minimum necessary actions to protect health information may vary according to:\n    - AI model type and architecture\n    - Individual health or other information being collected and used\n    - Amount of data required to re-identify an individual\n  - To comply with § 45 CFR 164.514(b), additional considerations needed for identifiers not already included in current list at § 45 CFR 164.514(b)(2)(i)\n    - With as few as four pieces of data, a person may be reidentified\n    - That data not exclusive to items identified in paragraphs (b)(2)(i)(A) through (b)(2)(i)(R)\n  - Addressing this issue provides additional guidance for data privacy and sharing practices, ultimately increasing trust in AI adoption\n  - Additional regulatory and policy solutions:\n    - Requiring continuing education and professional development training specific to safe use of AI-enabled technologies deployed in organizations\n    - Clarifying requirements for \"informed consent\" when using AI assisted technologies\n    - Adopting Total Product Lifecycle approach: iterative improvements while maintaining safety standards\n    - Adopting Good Machine-Learning Practice: standards for ensuring quality and organizational excellence\n    - Establishing requirements for safety reporting: clearinghouse for adverse events similar to MedWatch for pharmaceuticals\n    - Establishing requirements for standardized reporting of relevant AI model characteristics\n      - Similar to nutrition facts label or model card\n      - Explains purpose, design, development, use, and limitations\n    - Establishing requirements for Unique AI Identifier: transparency label to enhance trust and accountability\n\n- Question 3: Novel legal and implementation issues for non-medical devices\n  - AI deployment presents novel governance and accountability challenges\n    - Unresolved questions regarding responsibility, liability, and oversight when automated systems fail or risks go unrecognized\n  - Current legal and regulatory frameworks don't provide clear guidance on accountability assignment\n    - Among developers, implementers, and end-users\n    - Particularly when insufficient safeguards or unvalidated model behavior contribute to patient harm or degraded clinical performance (model drift)\n  - Absence of defined pathways for recourse or corrective action impedes safety and trust\n  - Addressing gaps essential to ensure AI integration supports consistent standards of care, transparent accountability, and responsible operational use\n  - Implementation processes are unique to local context\n    - Require robust internal governance framework\n    - Structured, standardized approach for each phase of integration (identification, evaluation, deployment, lifecycle management)\n  - HHS should dedicate resources to investigating:\n    - Use and influence of governance framework development in healthcare organizations\n    - Outcomes important to patients (quality of life, perceived health)\n    - Outcomes related to health conditions (surgical site infections, mortality, morbidity)\n  - HHS should create mechanisms for mandatory reporting of serious adverse events linked to AI-enabled technology use\n\n- Question 4: Most promising AI evaluation methods for non-medical devices\n  - Several evaluation frameworks and reporting guidelines developed to facilitate pre-integration evaluation:\n    - Model cards: standardized, plain-language documents explaining key characteristics\n      - What the model is, how it works, how it should and should not be used\n    - Expert/consensus guidelines for reporting and publishing AI development, testing, and validation processes\n      - FUTURE-AI, CHEERS-AI, CONSORT-AI, CLAIM, MINIMAR, STARD-AI, SPIRIT-AI, TRIPOD+AI\n    - Expert/consensus frameworks for healthcare professionals' evaluation\n      - PROBAST+AI, DECIDE-AI, CAIR Checklist, 20 Critical Questions\n  - These frameworks developed by experts in relevant fields (data science, statistical analysis, computer science, healthcare)\n  - However, evaluation frameworks not yet validated for use and efficacy in positively influencing:\n    - Evaluation and adoption of AI-enabled technology\n    - Patient outcomes\n    - Organizational outcomes (end-user satisfaction, workflow burden, usability, operational efficiency, financial impacts)\n  - HHS should dedicate resources to investigate development and influence of standardized evaluation frameworks on these outcomes\n\n- Question 5: How HHS can best support private sector activities\n  - HHS can best support by incentivizing private sector to adopt activities and organizational priorities promoting trustworthy AI\n  - Trustworthy AI concept widely published in literature, referring to:\n    - Transparency: extent to which users have access to information about systems and outputs\n    - Reliability and validity: ability to consistently perform as required and intended without failure\n    - Accountability: responsibility of those involved in design, development, evaluation, deployment, and maintenance to uphold safe, ethical, and equitable practices\n    - Fairness: ethical and equitable development and use prioritizing accurate, unbiased, high-quality care regardless of background, identity, or circumstances\n    - Utility: extent to which AI can achieve specific goals to improve patient outcomes, clinical decision-making, and healthcare delivery\n    - Fitness of purpose: how well AI meets goals and objectives based on intended use, requirements, preferences, and organizational priorities\n  - Facilitating or requiring accreditation, certification, or credentialing of AI developers would incentivize:\n    - Evidence-informed and ethically grounded practices\n    - Rigorous development, testing, validating, and transparent reporting\n    - Information about data collection, preprocessing, representative datasets, performance validation, benefits, limitations, intended and unintended uses, technical resource requirements\n  - By incentivizing industry standard for accreditation, healthcare organizations would prioritize AI developers demonstrating rigorous, sanctioned development processes\n    - Enhances trust and adoption\n\n- Question 6: Where AI tools have met or fallen short of expectations\n  - Published literature on real-world performance and cost is limited\n    - Mostly feasibility studies and small, nonexperimental studies with limited generalizability\n  - Additional research required to investigate large-scale adoption and effects\n  - Examples of AI performance falling short of expectations:\n    - Sepsis early warning system integrated with electronic health record platform\n    - AI-enabled technology to enhance detection and removal of colorectal polyps and adenomas during gastroenterology procedures\n  - AI tools with greatest potential to improve outcomes are those providing transparent and explainable outputs\n    - Highlight which input features most influenced model's prediction\n  - Explainability crucial for:\n    - Supporting trust in AI model\n    - Regulatory oversight\n    - Bias detection\n    - Error identification and mitigation\n    - Improved adoption by clinicians who need to understand basis of AI outputs\n  - Autonomous AI-enabled technology not encouraged in patient care delivery\n\n- Question 7: Roles and decision makers influencing AI adoption\n  - Primary influencers are intended end-users\n  - Published organizational experience reports detail influence and necessity for early inclusion of healthcare professionals whose roles intersect with AI use\n  - Involvement and prioritization of end-user needs, preferences, and priorities can improve development process:\n    - Visual displays, alert type and frequency, technology platforms that incentivize intended use\n  - Also facilitates trust through:\n    - Repeated exposure to and use of technology\n    - Explanation of potential benefits and consequences\n    - Factors that matter most to end-user (visual displays, ease of use, education, effects on workflow)\n  - Other influential roles: individuals with responsibility and authority to oversee evaluation, integration, and maintenance\n    - Organizational experience reports detail creation of dedicated executive roles (e.g., Chief Health AI Officer)\n    - Should serve as executive presence and authority to develop, maintain, and update AI governance framework\n    - Should remain separate and dedicated role\n  - Administrative hurdles to adoption:\n    - Fragmented organizational processes for identifying, evaluating, and deploying AI\n      - Concerns for inadequate governance oversight\n      - Duplicative or inefficient evaluation or deployment processes\n    - Lack of familiarity with relevant regulations\n    - Competing priorities and incentives across organization\n    - Lack of validated governance framework\n\n- Question 8: Where enhanced interoperability would accelerate AI development\n  - Requires industry adoption for reporting of different phases of AI lifecycle\n  - Regulation for standardized AI model cards or other reporting requirements encourages:\n    - Transparent development processes\n    - Consumer evaluation prior to purchase and deployment\n  - Development of large, de-identified national datasets for AI development may enhance interoperability and accelerate research\n  - Federated learning as strategy for anonymizing data and training AI models through decentralized data repositories\n    - Can enhance accessibility to datasets\n    - Improve privacy\n\n- Question 9: Patient and caregiver challenges and concerns\n  - AI promises to revolutionize healthcare by:\n    - Rapidly increasing efficiency\n    - Decreasing administrative burden\n    - Augmenting clinical decision-making to improve patient outcomes\n  - Caregivers and patients believe AI can offset administrative tasks with more time for genuine patient interaction and connection\n  - Concerns include:\n    - Privacy (e.g., ambient listening technology being used to identify or scrutinize patients or caregivers)\n    - Self-determination\n    - Retaliatory or discriminatory care delivery based on protected characteristics\n    - Loss of human element of healthcare delivery\n\n- Question 10: Specific areas of AI research HHS should prioritize\n  - All research should prioritize improvement of patient outcomes and clinician user experience\n    - Other outcomes recognized as secondary (cost savings, efficiency)\n    - Research should be structured so aim is always to improve human (patient and clinician) experience above other perceived improvements\n  - Priority research areas:\n    - Standardized methods for reporting and evaluating AI-enabled technology pre- and post-deployment\n    - Effects of implementing structured governance framework on organizational operations and patient outcomes\n    - Methods for detecting and mitigating bias in development and use of AI-enabled technology\n    - Implications for use of agentic AI tools in clinical care\n    - Use of AI to reveal factors contributing to adverse events in clinical care\n\n- Question 10(a): Published findings about impact of adopted AI tools\n  - Substantial and rapidly expanding body of published research examines clinical impact across diverse care settings\n  - Systematic reviews and health-economic evaluations report improvements in:\n    - Diagnostic accuracy\n    - Workflow efficiency\n    - Patient outcomes\n  - Strength and quality of evidence vary across specialties and methodologies\n  - Recent reviews note evidence base remains heterogeneous\n    - Would benefit from more rigorous, real-world evaluations\n  - Literature provides clear indications that AI tools already in use have measurable clinical and operational impacts\n    - Continued research needed to assess long-term performance and safety\n  - AORN Guideline for Integration of Artificial Intelligence includes section dedicated to examples of clinical application in perioperative practice setting\n\n- Question 10(b): How literature approaches costs, benefits, and transfers\n  - Costs and benefits must be approached on individualized and ongoing basis\n    - Unique to each organization, governance structure, and AI-enabled technology being integrated\n  - Characterized by identifying:\n    - Improved diagnostic accuracy\n    - Reduced clinician burden\n    - Effects on patient outcomes\n    - Efficiency or time savings\n    - Frequency of adverse events\n  - Need for more comprehensive and methodologically consistent evaluations\n  - Literature indicates AI adoption not only generates costs and benefits but also shifts:\n    - Resource allocation\n    - Labor distribution\n    - Financial responsibilities across stakeholders\n  - These transfers less consistently measured than direct costs or benefits\n\n- References cited\n  - 18 references provided covering topics including:\n    - AI readiness in nursing practice\n    - Risk management and quality of AI in healthcare\n    - FDA regulation of AI algorithms\n    - Recommendations for AI-enabled clinical decision support\n    - Various reporting guidelines (FUTURE-AI, TRIPOD+AI, SPIRIT-AI, PROBAST+AI, DECIDE-AI, CAIR Checklist)\n    - Studies on sepsis prediction models, AI in colonoscopy, and AI-enabled decision support in surgery\n\n---",
      "oneLineSummary": "The nation's leading perioperative nursing association—currently developing its own AI integration guideline—urges HHS to ensure AI augments rather than replaces clinical judgment, calling for mandatory adverse event reporting, standardized \"nutrition label\" disclosures for AI tools, and research that prioritizes patient and clinician experience over cost savings.\n\n---",
      "commenterProfile": "- **Name/Organization:** Association of periOperative Registered Nurses (AORN)\n- **Type:** Trade Association / Professional Membership Organization\n- **Role/Expertise:** Professional association for perioperative registered nurses; develops evidence-based guidelines for 36 perioperative practice areas; currently authoring new Guideline for Integration of Artificial Intelligence (publication May 2026)\n- **Geographic Scope:** National\n- **Stake in Issue:** Members are frontline end-users of AI tools in surgical and perioperative settings; organization sets practice standards that will need to incorporate AI guidance\n\n---",
      "corePosition": "We believe AI is suitable for augmenting clinical decision-making, but it cannot and must never replace human clinical judgment—the expertise, empathy, and accountability required for patient care cannot be replicated. When integrated ethically and responsibly, AI can improve outcomes; when integrated poorly, it can cause harm. Clinicians must be full partners in AI development and deployment, with competency assessed before any AI-enabled technology is used.\n\n---",
      "keyRecommendations": "- Update HIPAA de-identification rules (§ 45 CFR 164.514(b)) to account for AI-era re-identification risks\n  - As few as four data points can re-identify a person, beyond the current list of identifiers\n  - Additional guidance needed for data privacy and sharing practices\n\n- Require continuing education and professional development training specific to AI-enabled technologies deployed in each organization\n\n- Clarify \"informed consent\" requirements when using AI-assisted technologies\n\n- Adopt Total Product Lifecycle approach for iterative improvements while maintaining safety standards\n\n- Adopt Good Machine-Learning Practice standards for quality and organizational excellence\n\n- Establish mandatory safety reporting requirements\n  - Create clearinghouse for adverse events similar to MedWatch for pharmaceuticals\n\n- Require standardized AI model disclosures\n  - \"Nutrition facts label\" or model card explaining purpose, design, development, use, and limitations\n\n- Establish Unique AI Identifier as transparency label to enhance trust and accountability\n\n- Facilitate or require accreditation, certification, or credentialing of AI developers for healthcare sector\n\n- Dedicate HHS resources to investigate:\n  - Influence of governance frameworks on patient and organizational outcomes\n  - Validation of existing evaluation frameworks (FUTURE-AI, TRIPOD+AI, etc.) for real-world efficacy\n  - Methods for detecting and mitigating bias\n  - Implications of agentic AI tools in clinical care\n  - Use of AI to reveal factors contributing to adverse events\n\n- Prioritize research that improves patient outcomes and clinician experience above cost savings and efficiency\n\n---",
      "mainConcerns": "- End-users excluded from AI development\n  - Healthcare professionals whose roles intersect with AI tools are not involved in planning and development\n  - Creates disconnect between private sector enthusiasm and real-world implementation\n\n- Insufficient transparency in AI development\n  - Inadequate reporting and publication of development, testing, and validation processes\n  - Undermines clinician trust and adoption\n\n- Infrastructure gaps in healthcare organizations\n  - Many organizations lack technology infrastructure for computational power and data-handling AI requires\n  - Especially acute in under-resourced settings (rural, urban, non-academic medical centers)\n\n- Regulatory uncertainty\n  - Unclear approvals and accountability frameworks\n  - No defined pathways for recourse when AI systems fail or cause harm\n\n- Accountability gaps\n  - Current frameworks don't clarify responsibility among developers, implementers, and end-users\n  - Model drift and unvalidated behavior can contribute to patient harm without clear corrective mechanisms\n\n- Re-identification risks under current HIPAA rules\n  - Four pieces of data can re-identify a person\n  - Current identifier list doesn't account for AI-era data combinations\n\n- Evaluation frameworks not validated\n  - Multiple expert-developed frameworks exist but haven't been validated for efficacy in improving outcomes\n\n- Patient and caregiver concerns\n  - Privacy fears around ambient listening technology\n  - Concerns about discriminatory care delivery based on protected characteristics\n  - Fear of losing human element in healthcare\n\n---",
      "notableExperiences": "- AORN is publishing its own evidence-based Guideline for Integration of Artificial Intelligence in May 2026, based on PRISMA systematic review methodology—positioning them as both regulator-advisors and practitioners developing parallel guidance\n\n- Specific AI failures cited from literature:\n  - Sepsis early warning system integrated with EHR fell short of expectations for efficacy, safety, and generalizability\n  - AI for colorectal polyp detection during gastroenterology procedures underperformed expectations\n\n- Organizational experience reports detail creation of dedicated \"Chief Health AI Officer\" roles—a new executive position emerging specifically for AI governance\n\n- Federated learning identified as promising strategy for anonymizing data while training AI models through decentralized repositories—balancing accessibility with privacy\n\n- The \"four data points\" re-identification finding: current HIPAA safe harbor provisions may be inadequate because individuals can be re-identified with just four pieces of information not on the current identifier list\n\n---",
      "keyQuotations": "- \"AI cannot and must never be a replacement for a human's clinical judgement. The human expertise, empathy and accountability required for clinical decision-making in practice cannot be replicated.\"\n\n- \"When integrated ethically and responsibly, AI has the potential to improve health care including patient outcomes, patient experience, and health care worker experience. When ineffectively integrated, AI has the potential to do harm.\"\n\n- \"All research should prioritize improvement of patient outcomes and clinician user experience with other outcomes recognized as secondary (eg, cost savings, efficiency). In other words, the research should be structured so that the aim is always to improve the human (ie, patient and clinician) experience above other perceived improvements.\""
    },
    "themeScores": {
      "1": 1,
      "2": 1,
      "3": 1,
      "5": 1,
      "6": 1,
      "7": 1,
      "8": 1,
      "9": 1,
      "1.1": 1,
      "1.5": 1,
      "1.6": 1,
      "2.1": 1,
      "6.1": 1,
      "6.4": 1,
      "6.8": 1,
      "7.1": 1,
      "7.3": 1,
      "7.4": 1,
      "7.5": 1,
      "8.1": 1,
      "9.1": 1,
      "9.2": 1,
      "9.3": 1
    },
    "entities": [
      {
        "category": "AI Applications in Healthcare",
        "label": "Ambient AI"
      },
      {
        "category": "AI Applications in Healthcare",
        "label": "Early Warning System"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "AI Governance"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Model Card"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Trustworthy AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Agentic AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Algorithmic Bias"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Model Drift"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Diagnosis"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Informed Consent"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Perioperative Care"
      },
      {
        "category": "Data Privacy and Security",
        "label": "De-identification"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "FDA"
      },
      {
        "category": "Health Information Systems",
        "label": "CDS"
      },
      {
        "category": "Health Information Systems",
        "label": "EHR"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Credentialing"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Administrative Burden"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Adverse Event"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Nurse"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Laws and Regulations",
        "label": "HIPAA Privacy Rule"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Standard of Care"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Sepsis"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Experience"
      },
      {
        "category": "Professional Organizations",
        "label": "AORN"
      }
    ],
    "hasAttachments": true,
    "wordCount": 3818,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0024",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Anil bodepudi",
    "submitterType": "Individual",
    "date": "2026-01-23T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Background and expertise\n  - Data Engineering Manager overseeing petabyte-scale genomic pipelines in life sciences industry at Corteva Agriscience\n  - Manages \"Data Gravity\" and lifecycle challenges of biological AI daily\n  - Located in Austin, Texas\n\n- Core concern: Silent Instrument Drift\n  - RFI correctly identifies AI potential but overlooks a fundamental engineering failure mode\n  - This issue is called \"Batch Effects\" in production environments\n\n- The engineering gap explained\n  - In industrial genomics, AI models are highly sensitive to specific firmware and calibration of data capture hardware (DNA sequencers, imaging arrays)\n  - A model trained on data from \"Sequencer A\" will often degrade silently when applied to \"Sequencer B\"\n  - The model interprets hardware noise profile as biological signal\n  - Clinical application: if a hospital upgrades MRI firmware or sequencing hardware, an \"accelerated\" AI model lacking rigid provenance checks will likely produce diagnostic errors\n  - These errors are statistically undetectable until patient harm occurs\n\n- Recommendation: Mandate Instrument-Agnostic Validation\n  - HHS standards must look upstream of the model layer\n  - Framework should include mandatory \"Data Engineering Competency\" check\n  - Two specific requirements proposed:\n    - Metadata Provenance: AI systems must ingest and validate hardware metadata (Device ID, Firmware Version) of input data\n    - Drift Normalization: Protocols must distinguish between Biological Drift (patient changing) and Instrument Drift (machine changing)\n\n- Private sector solution\n  - Problem solved by enforcing strict schema contracts on data lakes\n  - Clinical systems require the same rigor\n  - Warning: Adopting AI without standardizing underlying data engineering layer will result in fragile systems that fail at scale\n\n---",
      "oneLineSummary": "A senior data engineer managing petabyte-scale genomic pipelines warns that \"silent instrument drift\" from hardware changes will cause undetectable AI diagnostic errors unless HHS mandates metadata provenance and drift normalization standards.\n\n---",
      "commenterProfile": "- **Name/Organization:** Anil Bodepudi, Corteva Agriscience\n- **Type:** Individual (Industry Expert)\n- **Role/Expertise:** Senior Data Engineer/Data Engineering Manager overseeing petabyte-scale genomic pipelines in life sciences\n- **Geographic Scope:** National, based in Austin, Texas\n- **Stake in Issue:** Professional expertise in biological AI data infrastructure directly applicable to clinical AI deployment challenges\n\n---",
      "corePosition": "I manage the exact data infrastructure challenges that clinical AI will face, and the RFI overlooks a critical failure mode: silent instrument drift. When hardware changes—like an MRI firmware upgrade—AI models trained on the old hardware will silently degrade and produce diagnostic errors that won't be detected until patients are harmed. We've solved this in industrial genomics, and clinical systems need the same rigor.\n\n---",
      "keyRecommendations": "- Mandate Instrument-Agnostic Validation as part of any AI adoption framework\n  - Require a \"Data Engineering Competency\" check for AI systems\n- Require Metadata Provenance\n  - AI systems must ingest and validate hardware metadata (Device ID, Firmware Version) of all input data\n- Establish Drift Normalization Protocols\n  - Must distinguish between Biological Drift (patient changing) and Instrument Drift (machine changing)\n- Standardize the underlying data engineering layer before accelerating AI adoption\n  - Enforce strict schema contracts on clinical data systems similar to private sector data lakes\n\n---",
      "mainConcerns": "- Silent Instrument Drift is a fundamental engineering failure mode not addressed in the RFI\n  - AI models are highly sensitive to specific firmware and calibration of data capture hardware\n  - Models interpret hardware noise profiles as biological signals\n- Hospital equipment upgrades will cause undetectable diagnostic errors\n  - MRI firmware updates or sequencing hardware changes will degrade AI performance\n  - Errors are statistically undetectable until patient harm occurs\n- Accelerating AI adoption without data engineering standards will create fragile systems\n  - Systems will fail at scale without proper provenance checks\n\n---",
      "notableExperiences": "- Direct experience with \"Batch Effects\" in industrial genomics production environments\n  - Models trained on \"Sequencer A\" silently degrade when applied to \"Sequencer B\"\n  - Hardware noise profiles get misinterpreted as biological signals\n- Private sector has already solved this problem\n  - Strict schema contracts on data lakes prevent drift-related failures\n  - This proven approach could transfer directly to clinical settings\n- Manages petabyte-scale genomic pipelines daily, dealing with the exact \"Data Gravity\" challenges clinical AI will face\n\n---",
      "keyQuotations": "- \"A model trained on data from 'Sequencer A' will often degrade silently when applied to 'Sequencer B,' interpreting the hardware noise profile as a biological signal.\"\n- \"If a hospital upgrades its MRI firmware or sequencing hardware, an 'accelerated' AI model lacking rigid provenance checks will likely produce diagnostic errors that are statistically undetectable until patient harm occurs.\"\n- \"Adopting AI without standardizing the underlying data engineering layer will result in fragile systems that fail at scale.\""
    },
    "themeScores": {
      "5": 1,
      "5.7": 1,
      "5.8": 1,
      "9.1": 1
    },
    "entities": [
      {
        "category": "Clinical Specialties and Domains",
        "label": "Genomics"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Provenance"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      }
    ],
    "hasAttachments": true,
    "wordCount": 360,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0025",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Anita Oppen",
    "submitterType": "Individual",
    "date": "2026-01-26T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- This is a Request for Information functioning like a proposed rule invitation for public input\n- The RFI concerns the use and regulation of artificial intelligence in the health sector\n  - Particularly related to health information technology and health data\n- ONC is seeking public comments on several aspects of AI governance\n  - How AI tools should be governed\n  - How AI should be used responsibly\n  - How AI should be integrated into patient care and healthcare systems\n- Specific areas of focus include\n  - Data security\n  - Interoperability\n  - Patient safety\n- Commenters can address topics such as\n  - The balance between innovation and patient protection\n  - Transparency in AI systems\n  - How to encourage ethical use of AI in healthcare",
      "oneLineSummary": "An individual provides a descriptive overview of the RFI's purpose and scope without offering substantive input or personal positions on AI in healthcare.",
      "commenterProfile": "- **Name/Organization:** Anita Oppen\n- **Type:** Individual\n- **Role/Expertise:** Not specified\n- **Geographic Scope:** Not specified\n- **Stake in Issue:** Not specified",
      "corePosition": "No substantive position is stated. The comment restates the purpose and scope of the Request for Information rather than providing input on the questions posed.",
      "keyRecommendations": "No specific recommendations provided",
      "mainConcerns": "No specific concerns raised",
      "notableExperiences": "No distinctive experiences shared",
      "keyQuotations": "No standout quotations"
    },
    "themeScores": {},
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "AI Governance"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "ONC"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      }
    ],
    "hasAttachments": false,
    "wordCount": 116,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0026",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Emilie Maxie",
    "submitterType": "Individual",
    "date": "2026-01-26T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Submitting as a frontline clinician\n  - Experience with AI in clinical care primarily through EHR-embedded tools\n    - Decision support prompts\n    - Predictive risk scores\n    - Alerts\n    - Documentation assistance\n- RFI seeks to accelerate safe and effective AI adoption in clinical care\n  - Comments focus on day-to-day realities that determine whether this goal is achieved in practice\n- From my perspective, adoption succeeds or fails based on whether AI tools:\n  - Fit into real clinical workflows\n  - Are transparent and understandable\n  - Have clear accountability\n  - Truly reduce burden while improving outcomes and equity\n\n- Question 3: Novel legal and implementation issues for non-medical device AI\n  - Biggest challenge is uncertainty about accountability when AI influences clinical decisions but isn't regulated as a medical device\n    - Risk scores or AI-generated summaries may shape how urgently a patient is evaluated\n    - May influence what information a clinician focuses on\n    - Often unclear who is responsible if the tool is wrong or misleading\n    - Creates confusion around liability, standards of care, and how much clinicians should rely on or override these tools\n  - Privacy, security, and secondary use of clinical data are real concerns\n    - Especially when AI features are embedded into required workflows and cannot easily be turned off\n    - Clinicians and patients may not always know when AI is being used\n    - May not know what data it relies on or how outputs are generated\n  - HHS could help by:\n    - Setting clearer expectations for disclosure so clinicians and patients know when AI is involved and for what purpose\n    - Establishing basic governance norms:\n      - Clearly identifying who approves a tool\n      - Who monitors its performance\n      - Who has authority to pause or roll it back if problems arise\n    - This would reduce uncertainty and support safer adoption\n\n- Question 4: Most promising AI evaluation methods for non-medical devices\n  - Evaluation cannot stop once a tool goes live\n  - Before deployment:\n    - AI tools should be tested on populations that reflect real patients, not just ideal datasets\n    - Must understand how performance varies by race, language, disability, or care setting\n    - Workflow testing is critical\n      - Tools should be observed in real clinical environments where outputs are reviewed but not acted upon\n      - Helps see how they affect decision-making and workload\n  - After deployment:\n    - Ongoing monitoring matters just as much\n    - Useful signals include:\n      - Whether alerts fire too often\n      - How frequently clinicians override recommendations\n      - Whether performance changes over time\n      - Whether certain groups experience more errors or missed care\n  - HHS could support this by:\n    - Encouraging practical, real-world evaluation\n    - Offering simple, non-punitive ways for clinicians and organizations to report safety concerns or unintended consequences\n\n- Question 5: How HHS can support private sector activities\n  - AI adoption would be safer with more consistent expectations for independent evaluation\n  - A basic certification approach for non-medical AI devices used in clinical care would help\n    - Should focus on transparency, safety monitoring, and bias assessment\n    - Would help clinicians understand what has been reviewed and what has not\n  - Plain-language \"AI labels\" would be helpful\n    - Similar to nutrition labels\n    - Could briefly explain what a tool is designed to do\n    - Where it performs well or poorly\n    - How it is intended to be used\n    - Would help clinicians and patients compare tools and set realistic expectations\n\n- Question 6: Novel AI tools with greatest potential\n  - AI works best when it clearly saves time or improves reliability\n    - Reducing documentation burden\n    - Flagging clinical risk paired with clear next steps\n  - AI tends to fall short when:\n    - Tools generate frequent, non-actionable alerts\n    - Add extra clicks\n    - Provide recommendations without enough context for clinicians to judge appropriateness\n  - Most promising use cases:\n    - Early detection of patient deterioration when paired with clear escalation pathways\n    - Medication safety support\n    - Language access tools for patients with limited English proficiency (with safeguards to verify accuracy)\n    - Care coordination tools that help identify and support high-risk patients across settings\n\n- Question 9: Patient and caregiver challenges and concerns\n  - Patients and caregivers want AI to make care safer, faster, and better coordinated\n    - Especially during handoffs and transitions\n  - Main concerns:\n    - Privacy\n    - Lack of transparency\n    - Biased or incorrect outputs\n    - Difficulty correcting errors when AI plays a role in decisions\n  - Building trust requires:\n    - Clear communication about when AI is used\n    - Appropriate patient notice\n    - Accessible ways to raise concerns or request review\n\n- Conclusion\n  - Responsible AI adoption depends less on technical sophistication and more on:\n    - Clarity\n    - Transparency\n    - Ongoing oversight\n  - Clear disclosure, practical evaluation standards, and safeguards would help ensure AI delivers real benefits\n  - Must avoid shifting risk onto those providing or receiving care\n\n---",
      "oneLineSummary": "A frontline nurse practitioner and critical care nurse argues that successful AI adoption hinges not on technical sophistication but on workflow fit, transparency, clear accountability, and ongoing oversight that protects both patients and clinicians.\n\n---",
      "commenterProfile": "- **Name/Organization:** Emilie Maxie, DNP, RN, CCRN\n- **Type:** Individual\n- **Role/Expertise:** Frontline clinician; Doctor of Nursing Practice; Registered Nurse; Critical Care Registered Nurse certification\n- **Geographic Scope:** Not specified\n- **Stake in Issue:** Direct daily user of AI tools embedded in EHRs; experiences firsthand how these tools affect clinical workflows, decision-making, and patient care\n\n---",
      "corePosition": "From my perspective as a frontline clinician, AI adoption succeeds or fails based on whether tools fit into real clinical workflows, are transparent and understandable, have clear accountability, and truly reduce burden while improving outcomes and equity. Responsible AI adoption depends less on technical sophistication and more on clarity, transparency, and ongoing oversight. Clear disclosure, practical evaluation standards, and safeguards would help ensure AI delivers real benefits without shifting risk onto those providing or receiving care.\n\n---",
      "keyRecommendations": "- HHS should set clearer expectations for disclosure\n  - Clinicians and patients should know when AI is involved and for what purpose\n- Establish basic governance norms for AI tools\n  - Clearly identify who approves a tool\n  - Identify who monitors its performance\n  - Identify who has authority to pause or roll it back if problems arise\n- Create simple, non-punitive reporting mechanisms\n  - Allow clinicians and organizations to report safety concerns or unintended consequences\n- Develop a basic certification approach for non-medical AI devices in clinical care\n  - Focus on transparency, safety monitoring, and bias assessment\n  - Help clinicians understand what has been reviewed and what has not\n- Implement plain-language \"AI labels\" similar to nutrition labels\n  - Explain what a tool is designed to do\n  - Indicate where it performs well or poorly\n  - Describe how it is intended to be used\n- Require pre-deployment testing on populations reflecting real patients\n  - Understand how performance varies by race, language, disability, or care setting\n  - Conduct workflow testing in real clinical environments with outputs reviewed but not acted upon\n- Mandate ongoing post-deployment monitoring\n  - Track alert frequency and override rates\n  - Monitor performance changes over time\n  - Identify whether certain groups experience more errors or missed care\n\n---",
      "mainConcerns": "- Accountability uncertainty when AI influences clinical decisions but isn't regulated as a medical device\n  - Risk scores and AI-generated summaries shape patient evaluation urgency and clinician focus\n  - Unclear who is responsible if the tool is wrong or misleading\n  - Creates confusion around liability and standards of care\n  - Unclear how much clinicians are expected to rely on or override these tools\n- Privacy, security, and secondary use of clinical data\n  - AI features embedded in required workflows cannot easily be turned off\n  - Clinicians and patients may not know when AI is being used\n  - May not know what data it relies on or how outputs are generated\n- AI tools that don't fit clinical workflows\n  - Frequent, non-actionable alerts\n  - Extra clicks that add burden\n  - Recommendations without enough context to judge appropriateness\n- Patient and caregiver concerns\n  - Privacy and lack of transparency\n  - Biased or incorrect outputs\n  - Difficulty correcting errors when AI plays a role in decisions\n\n---",
      "notableExperiences": "- Practical insight on what makes AI succeed or fail in clinical settings\n  - AI works best when it clearly saves time or improves reliability\n  - Falls short when it generates noise without actionable guidance\n- Specific promising use cases identified from frontline experience\n  - Early detection of patient deterioration paired with clear escalation pathways\n  - Medication safety support\n  - Language access tools for limited English proficiency patients (with accuracy safeguards)\n  - Care coordination tools for high-risk patients across settings\n- Practical evaluation signals from real-world use\n  - Whether alerts fire too often\n  - How frequently clinicians override recommendations\n  - Whether performance changes over time\n  - Whether certain groups experience more errors or missed care\n- \"Nutrition label\" concept for AI tools\n  - Plain-language labels explaining what a tool does, where it performs well or poorly, and intended use\n  - Would help clinicians and patients compare tools and set realistic expectations\n\n---",
      "keyQuotations": "- \"From an individual clinician's perspective, responsible AI adoption depends less on technical sophistication and more on clarity, transparency, and ongoing oversight.\"\n- \"Risk scores or AI-generated summaries may shape how urgently a patient is evaluated or what information a clinician focuses on, yet it is often unclear who is responsible if the tool is wrong or misleading.\"\n- \"Clear disclosure, practical evaluation standards, and safeguards that protect both patients and clinicians would help ensure AI delivers real benefits without shifting risk onto those providing or receiving care.\""
    },
    "themeScores": {
      "1": 1,
      "2": 1,
      "3": 1,
      "6": 1,
      "7": 1,
      "8": 1,
      "9": 1,
      "1.6": 1,
      "2.1": 1,
      "2.2": 1,
      "3.1": 1,
      "6.1": 1,
      "6.2": 1,
      "7.1": 1,
      "7.4": 1,
      "8.1": 1,
      "8.5": 1,
      "9.1": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "Bias Testing"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Trustworthy AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Algorithmic Bias"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Certification"
      },
      {
        "category": "Health Information Systems",
        "label": "EHR"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Equity"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Standard of Care"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      }
    ],
    "hasAttachments": true,
    "wordCount": 970,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0027",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Tapestryhealth",
    "submitterType": "Organization",
    "date": "2026-01-27T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- TapestryHealth is a specialized medical practice serving the Post-Acute and Long-Term Care (LTC) sector\n  - Responses reflect unique operational realities of Skilled Nursing Facilities (SNFs) and Assisted Living Communities\n  - Recommendations specifically designed for high-acuity, low-staffing, and multi-chronic nature of this vulnerable patient population\n  - This population differs significantly from ambulatory or acute hospital settings\n- TapestryHealth currently monitors over 80,000 patients in elder care communities\n  - Captures vital cardiopulmonary data every second\n- Critical disconnect exists: AI technology can predict hospitalizations 5 days in advance, but current payment models actively penalize providers for using it\n\n- The Core Barrier: The \"Efficiency Penalty\"\n  - Primary barrier to AI adoption is not technological—it is economic\n  - Current \"Labor-Based\" pricing model (RVUs) is incompatible with AI-native care\n  - The Efficiency Penalty: Current CPT codes equate \"value\" with \"time spent\"\n    - If AI allows a clinician to review a patient's status in 2 minutes instead of 20, the provider is financially penalized\n    - If AI allows the same 20-minute increment to be more comprehensive and effective, provider is still penalized\n    - Creates incentive to maintain slower, manual workflows\n  - Another efficiency impact: providers cannot concurrently bill for RPM (Remote Physiologic Monitoring) and RTM (Remote Therapeutic Monitoring)\n    - Forces providers to choose between services\n    - Creates \"data silos\" where AI models are blinded to half the patient's context\n\n- Proposed Solution: A Tiered Reimbursement Model\n  - Tiered Complexity Model for AI reimbursement (detailed in CY 2026 PFS comments)\n  - Tier 1 (Administrative AI): Simple rule-based alerts\n    - Reimbursement covers basic software costs\n  - Tier 2 (Assistive AI): Pattern recognition on single data streams\n    - Reimbursement covers software + moderate clinician and clinical staff review\n  - Tier 3 (Predictive/Multimodal AI): High-complexity modeling using continuous, passive data\n    - Reimbursement covers infrastructure + high-level cognitive interpretation by clinicians and clinical staff\n\n- Safety & Oversight: The Indispensable \"Human-in-the-Loop\"\n  - AI should not replace the clinician; it should elevate them\n  - The \"Signal-to-Noise\" Problem: Pure software solutions often generate hundreds of false alarms\n    - In SNF setting, \"alert fatigue\" is a major safety risk\n  - Recommendation: HHS and CMS should require—and reimburse—a \"Clinical Verification\" component\n    - Propose new code for \"AI-Guided Clinical Interpretation\" that pays for cognitive labor of verifying AI insights\n\n- Real-World Efficacy: Contactless Radar Monitoring\n  - Early Warning: Contactless monitoring provided average predictive lead time of 4.99 to 5.65 days prior to hospitalization event\n  - Outcome Improvement: Facilities using \"AI + Clinician\" model saw 43% reduction in ED visits and 45% reduction in inpatient admissions\n\n- Use of AI Technology for Basic Analytics\n  - AI significantly enhances ability to generate meaningful and actionable insights from vast, disparate, and often unstructured data in LTC electronic medical record systems\n  - Remote vitals monitoring is one application, but broader AI analytics remain underutilized due to financial models that don't incentivize technological advancements\n  - By applying AI to patient's prescribed drug history combined with other medical information, clinicians can develop actionable plans to improve outcomes and reduce avoidable hospital admissions\n    - Example: AI model can flag heart failure patient on high-dose diuretics whose respiratory rate has subtly increased over 48 hours\n    - This pattern is often missed by routine checks but highly predictive of impending fluid overload crisis\n  - AI can help identify specific opportunities for behavioral health integration in long-term care\n    - Example: AI can analyze movement patterns to detect when typically active resident begins spending prolonged periods alone in their room\n    - Prompts depression screening before social withdrawal deepens\n  - AI supports recognition of behavioral health needs and facilitates development of intervention plans tailored to individual patients\n\n- Responses to Specific RFI Questions\n  - Q1: Biggest barriers to private sector innovation & adoption?\n    - The \"Efficiency Penalty\" in Reimbursement\n    - In low-margin SNF sector, adoption is driven by ROI\n    - Current reimbursement (RVUs) pays for time spent, not outcomes achieved\n    - AI reduces time spent, effectively lowering revenue for efficient providers\n    - Until payment models reward \"Capacity\" (patients managed safely) rather than \"Minutes\" (time spent documenting), innovation will remain a financial liability for LTC providers\n  - Q2: Regulatory/payment changes to prioritize?\n    - Unblock Data Silos & Value Cognitive Interpretation\n    - First: Unblock Concurrent Billing (Amend 42 C.F.R. § 410.78)\n      - CMS must permit concurrent billing of RPM (99454) and RTM (98975) when distinct hardware/data streams are used\n      - For complex SNF patients, AI needs both physiological and therapeutic data to be safe and effective\n      - Remove subregulatory guidance prohibiting concurrent billing of 99454 and 98975\n      - Amend 42 C.F.R. § 410.78 to explicitly permit concurrent billing for services using distinct hardware/data streams\n    - Second: Create \"AI-Guided Interpretation\" Codes (Revisit 42 C.F.R. § 414.20)\n      - Introduce add-on codes that specifically reimburse the \"Human-in-the-Loop\" verification step\n      - Ensures safety without penalizing provider for cost of software\n  - Q5: How can HHS support private sector (accreditation)?\n    - Standardize \"Contactless & Ambient\" Monitoring\n    - HHS should establish accreditation class for \"Ambient Vital Sign Monitoring\" (e.g., radar/LiDAR)\n    - In SNF sector, wearables have high failure rates due to dementia and skin fragility\n    - Validating \"Contactless\" as standard of care is crucial for equitable access in this population\n  - Q7: Decision makers & administrative hurdles?\n    - The \"Capital vs. Operational\" Mismatch for Facility Administrators\n    - SNF Administrator is key decision-maker but operates on fixed per-diem\n    - Hurdle: AI requires upfront Capital Expense (sensors), but savings (reduced hospitalizations) accrue to Payer (Medicare), not Facility\n    - Payment models must share these savings with facility to incentivize infrastructure investment\n  - Q9: Patient/Caregiver challenges & concerns?\n    - The Tension Between \"Safety\" and \"Surveillance\"\n    - Families want safety of continuous monitoring (\"never alone\")\n    - Residents fear indignity of cameras (\"Big Brother\")\n    - This validates need for Privacy-Preserving AI (Contactless Radar)\n    - Provides safety of ICU monitor without invasion of a camera\n\n- Conclusion\n  - By modernizing payment rails to reward preventive efficiency and verified oversight, HHS and CMS can solve the staffing crisis and dramatically improve quality of life for our nation's seniors",
      "oneLineSummary": "Elder care AI monitoring company serving 80,000 patients argues that current payment models create an \"efficiency penalty\" that punishes providers for using AI that can predict hospitalizations 5 days in advance, and proposes tiered reimbursement and concurrent billing reforms to unlock AI's potential in nursing homes.",
      "commenterProfile": "- **Name/Organization:** TapestryHealth (submitted by Mordy Eisenberg, NHA, Chief Growth Officer)\n- **Type:** Healthcare Provider\n- **Role/Expertise:** Specialized medical practice serving Post-Acute and Long-Term Care sector; monitors 80,000+ patients in elder care communities using contactless radar monitoring technology\n- **Geographic Scope:** National\n- **Stake in Issue:** Direct provider of AI-enabled remote monitoring services in SNFs; current payment models financially penalize their efficient AI-driven care model",
      "corePosition": "We've proven that AI can predict hospitalizations nearly 5 days in advance and reduce ED visits by 43%, but the current payment system punishes us for being efficient. The fundamental problem isn't technology—it's that Medicare pays for time spent, not outcomes achieved. Until CMS rewards \"capacity\" (patients managed safely) rather than \"minutes\" (time spent documenting), AI innovation will remain a financial liability for long-term care providers.",
      "keyRecommendations": "- Implement a Tiered Complexity Model for AI reimbursement\n  - Tier 1 (Administrative AI): Reimburse basic software costs for simple rule-based alerts\n  - Tier 2 (Assistive AI): Reimburse software + moderate clinician review for pattern recognition on single data streams\n  - Tier 3 (Predictive/Multimodal AI): Reimburse infrastructure + high-level cognitive interpretation for complex modeling using continuous, passive data\n- Unblock concurrent billing of RPM and RTM services\n  - Amend 42 C.F.R. § 410.78 to explicitly permit concurrent billing of RPM (99454) and RTM (98975) when distinct hardware/data streams are used\n  - Remove subregulatory guidance prohibiting concurrent billing\n  - AI needs both physiological and therapeutic data to be safe and effective for complex SNF patients\n- Create new \"AI-Guided Clinical Interpretation\" codes\n  - Revisit 42 C.F.R. § 414.20 to introduce add-on codes\n  - Specifically reimburse the \"Human-in-the-Loop\" verification step\n  - Ensures safety without penalizing providers for software costs\n- Establish accreditation class for \"Ambient Vital Sign Monitoring\"\n  - Validate contactless monitoring (radar/LiDAR) as standard of care\n  - Wearables have high failure rates in SNFs due to dementia and skin fragility\n  - Critical for equitable access in this population\n- Restructure payment models to share savings with facilities\n  - AI requires upfront capital expense (sensors) but savings accrue to Medicare, not the facility\n  - Must incentivize infrastructure investment by sharing hospitalization savings with facilities",
      "mainConcerns": "- The \"Efficiency Penalty\" in current reimbursement\n  - CPT codes equate \"value\" with \"time spent\"\n  - If AI allows faster patient review, provider revenue decreases\n  - Creates perverse incentive to maintain slower, manual workflows\n- Data silos created by billing restrictions\n  - Cannot concurrently bill RPM and RTM\n  - Forces providers to choose between services\n  - AI models are \"blinded to half the patient's context\"\n- Alert fatigue as a safety risk\n  - Pure software solutions generate hundreds of false alarms\n  - In SNF settings, this is a major safety concern\n  - Need reimbursed clinical verification to filter signal from noise\n- Capital vs. operational cost mismatch\n  - SNF administrators operate on fixed per-diem\n  - AI requires upfront capital investment\n  - Savings flow to Medicare, not to the facility making the investment\n- Privacy concerns creating adoption barriers\n  - Families want continuous monitoring safety\n  - Residents fear surveillance and loss of dignity\n  - Need privacy-preserving alternatives to cameras",
      "notableExperiences": "- Demonstrated 4.99 to 5.65 days average predictive lead time before hospitalization events using contactless radar monitoring\n- Facilities using their \"AI + Clinician\" model achieved 43% reduction in ED visits and 45% reduction in inpatient admissions\n- Discovered that wearable monitoring devices have high failure rates in SNF populations due to dementia (patients remove them) and skin fragility\n- Identified specific AI use case: flagging heart failure patients on high-dose diuretics whose respiratory rate subtly increases over 48 hours—a pattern often missed by routine checks but highly predictive of fluid overload crisis\n- Developed behavioral health application: AI analyzing movement patterns to detect when typically active residents begin spending prolonged periods alone, prompting depression screening before social withdrawal deepens\n- Framed the fundamental tension in elder care monitoring as \"safety vs. surveillance\"—families want \"never alone\" while residents fear \"Big Brother\"",
      "keyQuotations": "- \"The 'AI Doctor' will not work for free—and neither can the human clinicians who supervise them.\"\n- \"AI technology has matured to the point where it can predict hospitalizations 5 days in advance, the current payment models actively penalize providers for using it.\"\n- \"Until payment models reward 'Capacity' (patients managed safely) rather than 'Minutes' (time spent documenting), innovation will remain a financial liability for LTC providers.\"\n- \"Families want the safety of continuous monitoring ('never alone'), but residents fear the indignity of cameras ('Big Brother').\""
    },
    "themeScores": {
      "4": 1,
      "6": 1,
      "7": 1,
      "4.1": 1,
      "4.3": 1,
      "4.4": 1,
      "4.5": 1,
      "6.2": 1,
      "7.4": 1,
      "9.3": 1
    },
    "entities": [
      {
        "category": "AI Applications in Healthcare",
        "label": "RPM"
      },
      {
        "category": "AI Applications in Healthcare",
        "label": "RTM"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Human-in-the-Loop"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Post-deployment Monitoring"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Accreditation"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Screening"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Behavioral Health"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "ROI"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "CPT"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Alert Fatigue"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Emergency Department"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "ICU"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Long-Term Care"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "SNF"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Standard of Care"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Dementia"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Depression"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Heart Failure"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "Physician Fee Schedule"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "RVU"
      }
    ],
    "hasAttachments": true,
    "wordCount": 1364,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0028",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Ty Greenhalgh",
    "submitterType": "Individual",
    "date": "2026-01-29T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Regulatory Visibility: Addressing the \"Encrypted Blind Spot\"\n  - Context: The Importance of Device Identification\n    - The FDA has authorized over 1,250 AI-enabled medical devices as of 2026\n    - This surge in \"black box\" devices on clinical networks creates a massive visibility gap\n    - While encryption is vital for patient privacy, it often masks the device's identity from traditional network security tools\n    - Hospitals are left unable to verify if a device is a legitimate diagnostic tool or a compromised endpoint\n  - Recommendation\n    - HHS should mandate that manufacturers provide a standardized mechanism (such as a public API or shared cryptographic \"Identification Key\")\n    - This would allow network security platforms to identify devices at the hardware and software level without decrypting sensitive clinical data\n    - Identification should be available to all security vendors at no cost or nominal fee, contingent upon Covered Entity permission\n    - Without this \"transparent-yet-secure\" identification standard, hospitals are forced to choose between HIPAA-compliant encryption and network visibility required for basic patient safety\n\n- Transitioning to AI-Based Exposure Management (CTEM)\n  - Context: The Inadequacy of Static Inventories\n    - OMB Memorandum M-25-21 and recent HIPAA Security Rule updates emphasize 100% asset inventory\n    - In an AI-driven clinical environment, a static spreadsheet of hardware is no longer sufficient\n    - AI software \"lives\" in ephemeral containers, cloud environments, and embedded firmware\n  - Recommendation\n    - HHS should formalize a requirement for Continuous Threat Exposure Management (CTEM) to align with M-25-21 and proposed HIPAA updates\n    - Traditional periodic scanning is insufficient for AI assets\n    - Covered Entities require an automated, AI-driven inventory system that discovers AI models and their dependencies in real-time\n    - This inventory must integrate into a broader exposure management process using AI to prioritize vulnerabilities based on actual reachability and clinical impact\n    - Static CVSS scores do not account for the unique \"agentic\" risks of clinical AI\n\n- Defense Against \"AI on AI\" Attacks\n  - Context: The New Speed of Conflict\n    - Modern threats have moved beyond simple malware to Autonomous Agentic Malware\n    - Attackers are using reinforcement learning to perform real-time calculus on defensive gaps\n    - They generate novel prompt injections for every stage of an attack to bypass standard Indicators of Compromise (IOC) monitoring\n    - Recent incidents demonstrate attackers are \"hijacking\" the very AI tools hospitals rely on:\n      - ServiceNow \"BodySnatcher\" flaw\n      - Salesforce \"ForcedLeak\" exploit\n      - Silver Fox group's targeting of medical imaging software\n  - Recommendation\n    - HHS must recognize that the threat landscape has shifted to \"AI vs. AI\"\n    - Clinical care cannot be protected by human-speed manual responses when attackers utilize agentic AI to automate lateral movement and data exfiltration\n    - HHS reimbursement and regulatory frameworks should prioritize and incentivize adoption of AI-based defensive solutions\n    - These solutions must be capable of autonomous detection and response—matching attacker speed to isolate compromised AI agents or medical devices before they can execute multi-step calculus on the hospital's network\n    - A policy that relies on human intervention for AI-driven threats is a policy that accepts failure\n\n- Relevant Electronic Information Systems (REIS) and Accountability\n  - Context: Expanding the Definition of \"Asset\"\n    - Under the modernized HIPAA framework, the definition of \"Relevant Electronic Information System\" must expand to include data pipelines that feed AI models\n    - If the data is poisoned, the clinical output is compromised\n  - Recommendation\n    - HHS should clarify that AI \"Models and Pipelines\" are considered Relevant Electronic Information Systems (REIS) under the HIPAA Security Rule\n    - The 100% inventory requirement should include not just the physical server, but specific versioning of the AI model and its data training sources\n    - Providing a \"Nutrition Label\" or AI Bill of Materials (AIBOM) for every clinical AI tool will ensure rapid identification of vulnerable models\n    - When a novel prompt injection vulnerability (like \"Silver Fox\" or \"BodySnatcher\" variants) is discovered, a Covered Entity can instantly identify every instance of that vulnerable model across their entire enterprise ecosystem\n\n---",
      "oneLineSummary": "A cybersecurity expert argues that hospitals face an impossible choice between encryption and visibility, urging HHS to mandate AI device identification standards, continuous threat exposure management, and AI-powered defenses to match the speed of AI-driven attacks.\n\n---",
      "commenterProfile": "- **Name/Organization:** Ty Greenhalgh\n- **Type:** Individual\n- **Role/Expertise:** Cybersecurity expert with deep knowledge of healthcare IT security, AI threats, and regulatory frameworks (HIPAA, OMB M-25-21)\n- **Geographic Scope:** National\n- **Stake in Issue:** Professional interest in healthcare cybersecurity policy and protecting clinical networks from emerging AI-based threats\n\n---",
      "corePosition": "The healthcare sector faces a fundamental security crisis: over 1,250 FDA-authorized AI medical devices create \"black box\" visibility gaps that traditional security tools cannot address. HHS must update its regulatory framework to recognize that AI-driven threats require AI-driven defenses, and that static inventories and human-speed responses are no longer adequate for protecting patient safety.\n\n---",
      "keyRecommendations": "- Mandate standardized device identification mechanisms\n  - Require manufacturers to provide public APIs or cryptographic \"Identification Keys\"\n  - Enable network security platforms to identify devices without decrypting clinical data\n  - Make identification available to all security vendors at no or nominal cost\n  - Require Covered Entity permission for access\n\n- Formalize Continuous Threat Exposure Management (CTEM) requirements\n  - Replace traditional periodic scanning with automated, AI-driven inventory systems\n  - Require real-time discovery of AI models and their dependencies\n  - Prioritize vulnerabilities based on actual reachability and clinical impact rather than static CVSS scores\n\n- Incentivize AI-based defensive solutions through reimbursement and regulatory frameworks\n  - Require autonomous detection and response capabilities\n  - Match defensive speed to attacker speed\n  - Enable isolation of compromised AI agents or devices before multi-step attacks execute\n\n- Expand REIS definition to include AI models and data pipelines\n  - Clarify that AI \"Models and Pipelines\" fall under HIPAA Security Rule\n  - Require 100% inventory to include AI model versioning and data training sources\n  - Mandate \"Nutrition Labels\" or AI Bill of Materials (AIBOM) for every clinical AI tool\n\n---",
      "mainConcerns": "- The \"Encrypted Blind Spot\" problem\n  - Encryption masks device identity from security tools\n  - Hospitals cannot verify if devices are legitimate or compromised\n  - Current framework forces choice between HIPAA compliance and network visibility\n\n- Static inventories are inadequate for AI environments\n  - AI software exists in ephemeral containers, cloud environments, and embedded firmware\n  - Traditional hardware spreadsheets cannot track dynamic AI assets\n  - Static CVSS scores don't account for \"agentic\" risks of clinical AI\n\n- Emergence of \"AI vs. AI\" attack landscape\n  - Autonomous Agentic Malware uses reinforcement learning to find defensive gaps\n  - Novel prompt injections bypass standard IOC monitoring\n  - Attackers are hijacking hospital AI tools (ServiceNow, Salesforce, medical imaging)\n  - Human-speed responses cannot match AI-speed attacks\n\n- Data pipeline vulnerabilities compromise clinical AI\n  - Poisoned data leads to compromised clinical outputs\n  - Current REIS definition doesn't cover AI models and pipelines\n  - No standardized way to track vulnerable AI model instances across enterprises\n\n---",
      "notableExperiences": "- Identifies a fundamental paradox in current healthcare security: HIPAA-mandated encryption creates the very blind spots that enable attacks on AI medical devices\n- Cites specific real-world attack vectors that demonstrate AI tools being weaponized against hospitals:\n  - ServiceNow \"BodySnatcher\" flaw\n  - Salesforce \"ForcedLeak\" exploit\n  - Silver Fox group targeting medical imaging software\n- Proposes the concept of an \"AI Bill of Materials\" (AIBOM) analogous to software bills of materials—a \"nutrition label\" for clinical AI\n- Frames the core problem as a speed mismatch: attackers using AI to perform \"real-time calculus on defensive gaps\" while defenders rely on human-speed manual responses\n\n---",
      "keyQuotations": "- \"Without this 'transparent-yet-secure' identification standard, hospitals are forced to choose between HIPAA-compliant encryption and the network visibility required for basic patient safety.\"\n- \"A policy that relies on human intervention for AI-driven threats is a policy that accepts failure.\"\n- \"If the data is poisoned, the clinical output is compromised.\""
    },
    "themeScores": {
      "2": 1,
      "5": 1,
      "9": 1,
      "2.6": 1,
      "5.7": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "AI Bill of Materials"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Agentic AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Black Box AI"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Diagnosis"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Encryption"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "FDA"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "OMB"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Patient Safety"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      },
      {
        "category": "Laws and Regulations",
        "label": "HIPAA"
      },
      {
        "category": "Laws and Regulations",
        "label": "HIPAA Security Rule"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      }
    ],
    "hasAttachments": false,
    "wordCount": 662,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0029",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Van Pelt & Company, LLC",
    "submitterType": "Organization",
    "date": "2026-01-29T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Infrastructure Enablement for Clinical AI — The role of a capacity data utility\n  - The utility model is intentionally flexible, supporting multiple care settings with distinct operational challenges while relying on a single, vendor-agnostic operational infrastructure\n  \n- Behavioral health settings\n  - Limited inpatient capacity, workforce shortages, and fragmented placement processes frequently result in prolonged emergency department boarding and inappropriate placements\n  - Real-time operational visibility into behavioral health bed availability and staffing constraints enables faster, more appropriate placement decisions\n  - Reduces strain on emergency services and improves outcomes for patients in crisis\n  - Illustrative concepts available upon request\n\n- Rural care environments\n  - Distance, limited access to specialty care, and staffing shortages require precise, time-sensitive decision-making\n  - Need to determine when patients can be safely managed locally versus when timely transfer to higher-acuity care is essential\n  - A capacity data utility supports these decisions by providing real-time visibility into system capacity without imposing additional reporting burden on already resource-constrained facilities\n  - Illustrative rural care concepts available upon request\n\n- Statewide level coordination\n  - Governors, state health departments, and emergency management agencies require continuous situational awareness\n  - Supports preparedness, surge response, and alignment with federal programs such as Hospital Preparedness Program (HPP) and NHSN Bed Connectivity\n  - Provides a shared operational view supporting both day-to-day coordination and emergency response\n  - Statewide capacity concepts available upon request\n\n- Regional and multi-state coordination\n  - Increasingly critical for disaster response, National Disaster Medical System (NDMS) operations, and military-civilian medical surge scenarios\n  - Supports interoperability across jurisdictional boundaries while preserving local control and governance\n\n- Application Across Care Settings\n  - Accelerating AI adoption in clinical care requires confronting a reality: the primary constraint on meaningful AI deployment is not model capability\n  - The real barrier is the absence of shared, trusted, real-time operational data infrastructure that reflects how care is actually delivered across hospitals, regions, and states\n  - Federal investments have supported digital health tools, dashboards, and reporting pipelines over the past several years\n  - These efforts remain fragmented, episodic, and often disconnected from frontline decision-making\n  - AI tools are frequently deployed as point solutions—optimized for narrow workflows but blind to system-level constraints\n    - Staffing shortages\n    - Bed availability\n    - Specialty service access\n    - Regional surge conditions\n  - Even well-designed AI tools risk producing recommendations that are clinically reasonable but operationally infeasible\n\n- The shared capacity data utility model\n  - Directly addresses this foundational gap\n  - Designed as a vendor-agnostic, public-private model\n  - Leverages existing systems and non-PHI operational data\n  - Provides a continuously available view of healthcare system capacity\n  - What it is NOT:\n    - Not a new reporting requirement\n    - Not a centralized data warehouse\n    - Not a prescriptive technology mandate\n  - Functions as enabling infrastructure providing shared operational context for AI tools, clinicians, administrators, and public agencies in both routine and crisis conditions\n\n- Practical Experience Informing the Capacity Data Utility Model\n  - Grounded in real-world experience working with states and health systems to operationalize real-time capacity and coordination\n  - Success did not hinge on introducing new technologies\n  - Success depended on designing governance and operational models that competing providers would trust and actively use\n  - Multiple state-level initiatives involved:\n    - Convening competing hospitals\n    - Aligning state health departments and emergency management agencies\n    - Integrating technology vendors without allowing any single platform to dominate\n  - Required careful attention to neutrality, transparency, and shared value\n  - Providers participated not because they were mandated but because the system's outputs helped them solve immediate operational problems\n    - Reducing transfer delays\n    - Improving patient placement\n    - Anticipating system stress before it manifested as clinical harm\n\n- Key lessons from this work\n  - Sustainability depends on shared governance and operational relevance\n  - When data is governed collaboratively, limited to non-PHI operational signals, and refreshed frequently enough to inform real decisions, providers are willing to participate and continue participating over time\n  - When systems are built primarily for compliance or retrospective reporting, adoption quickly erodes\n  - Design priorities:\n    - Federated data exchange\n    - Provider-led governance\n    - High-frequency operational signals\n  - This approach lowers privacy risk, reduces regulatory friction, and aligns incentives across hospitals, states, and federal partners\n\n- A capacity data utility as Enabling Infrastructure for Responsible Clinical AI\n  - Clinical AI cannot be responsibly scaled without system-level context\n  - AI tools that influence clinical decisions—triage, transfers, staffing, care coordination—must account for real-time constraints across the care continuum\n  - A capacity data utility standardizes and shares operational capacity signals that already exist within hospital systems but are rarely visible beyond institutional boundaries\n  - Enables system-aware AI, allowing tools to incorporate real availability of beds, staff, and services into recommendations\n  - Particularly critical in:\n    - Emergency care\n    - Rural health\n    - Behavioral health\n    - Disaster response\n  - Delays and misalignment have disproportionate consequences in these settings\n  - Creates a foundation for real-world evaluation and post-deployment monitoring of AI tools\n  - Enables continuous assessment of how AI-assisted decisions affect system-level outcomes\n    - Boarding times\n    - Transfer delays\n    - Access disparities\n  - Supports a learning health system approach without requiring disclosure of proprietary algorithms\n\n- Equity, Access, and Preparedness Implications\n  - Without shared operational infrastructure, AI tools risk reinforcing existing inequities\n  - AI may optimize within well-resourced systems while leaving rural and safety-net providers behind\n  - A capacity data utility mitigates this risk by ensuring AI-enabled decisions reflect actual system capacity across all participating facilities\n  - Not just those with the most advanced digital infrastructure\n  - Aligns with HHS priorities related to health equity, rural access, and national preparedness\n  - Focusing on non-PHI operational data and shared governance strengthens trust and reduces barriers to participation among providers serving underserved populations\n\n- Policy Implications for OneHHS\n  - To accelerate responsible clinical AI adoption at national scale, HHS should recognize that enabling infrastructure is as critical as algorithm development\n  - A capacity data utility is a practical model for designing, governing, and sustaining such infrastructure without increasing regulatory burden or fragmenting existing investments\n\n- Alignment of a capacity data utility to OneHHS RFI Themes\n  - Within healthcare organizations, AI adoption is most strongly influenced by operational and clinical leadership responsible for:\n    - Patient flow\n    - Staffing\n    - Emergency response\n    - Care coordination\n  - A capacity data utility supports these decision-makers by shifting governance from individual AI tools to shared, system-level operational governance structures\n  - Enables consistent evaluation and accountability across institutions and jurisdictions\n\n- Direct responses to RFI themes\n  - Barriers to AI adoption beyond technical capability\n    - Fragmented, episodic, and non-operational data infrastructure is a primary constraint\n    - A capacity data utility overcomes these barriers by reusing existing systems and focusing on shared, real-time operational context\n  - Interoperable data environments\n    - A capacity data utility is vendor-agnostic, federated, and focused on non-PHI operational signals\n    - Enables interoperability without introducing new privacy risk or duplicative reporting structures\n    - Aligns with OneHHS objectives to reduce fragmentation while preserving local control\n  - AI evaluation once deployed\n    - Supports real-world, post-deployment evaluation by linking AI-assisted decisions to system-level operational outcomes\n    - Transfer delays, boarding, access constraints\n    - Does not require disclosure of proprietary algorithms\n  - Equity concerns\n    - AI deployed without system-level context can exacerbate disparities\n    - Particularly for rural, safety-net, and behavioral health providers\n    - A capacity data utility ensures AI-enabled decisions reflect actual capacity across all participating facilities\n  - Adaptability across contexts\n    - Designed to function consistently across behavioral health, rural care, statewide coordination, and regional or multi-state environments\n    - Same underlying infrastructure while supporting setting-specific use cases\n  - Coordination across HHS operating divisions\n    - Shared enabling infrastructure supports multiple missions—clinical care delivery, preparedness, public health, and emergency response\n    - Does not create new silos or prescriptive mandates\n    - Aligns with OneHHS approach\n\n- Conclusion\n  - The promise of clinical AI will not be realized through isolated tools or episodic data feeds\n  - Requires shared operational infrastructure that reflects how care is delivered across systems and geographies\n  - A capacity data utility offers an experience-informed, scalable approach\n  - Enables AI to improve care delivery, advance equity, and strengthen preparedness without increasing fragmentation or burden\n\n- Clarification\n  - References to capacity data utilities are illustrative of a general infrastructure approach\n  - Not an endorsement of any single organization, platform, or implementation model\n\n- Appendix (Reference)\n  - Additional background materials available upon request\n    - Capacity data utility concept\n    - Illustrations for behavioral health\n    - Rural care\n    - Statewide capacity coordination\n    - Regional or multi-state implementation models\n  - Illustrative concepts demonstrate operational use cases, not prescriptive technology requirements\n\n- Legislative Context and Policy Alignment\n  - Aligns with several legislative vehicles under consideration or enacted during the 119th Congress\n  - Could support implementation without creating new regulatory mandates\n  - H.R. 2936\n    - Establishes framework to improve real-time visibility into hospital capacity and preparedness coordination\n    - A capacity data utility functions as an execution layer for this policy intent\n    - Provides shared, vendor-agnostic operational infrastructure\n    - Enables states and regions to operationalize capacity awareness using existing systems and non-PHI data\n  - S. 1974 (under Senate consideration)\n    - Advances similar objectives\n    - Reinforces federal role in strengthening healthcare system readiness, situational awareness, and coordination\n    - Emphasizes preparedness, continuity of operations, and cross-sector alignment\n    - A capacity data utility directly supports this intent\n    - Enables standardized, real-time operational visibility without prescribing specific technologies or imposing new reporting requirements\n  - National Defense Authorization Act (NDAA)\n    - Signed into law by President Trump\n    - Reinforces importance of medical readiness, surge capacity, and civilian-military coordination\n    - A capacity data utility aligns by providing a reusable operational backbone\n    - Supports preparedness, response, and recovery missions across HHS, DoD, and state partners\n  - Together, H.R. 2936, S. 1974, and the NDAA provide complementary policy signals and potential funding pathways\n  - Could support development and scaling of shared capacity infrastructure while preserving flexibility for states, providers, and federal agencies\n\n---",
      "oneLineSummary": "Healthcare infrastructure consultant argues that clinical AI adoption is bottlenecked not by algorithm capability but by the absence of shared, real-time operational data, proposing a vendor-agnostic \"capacity data utility\" as the essential enabling infrastructure.\n\n---",
      "commenterProfile": "- **Name/Organization:** Andy Van Pelt, Founder & Principal, Van Pelt & Company, LLC\n- **Type:** Business\n- **Role/Expertise:** Healthcare infrastructure consultant with experience working with states and health systems on real-time capacity coordination across diverse care environments\n- **Geographic Scope:** National, with state-level implementation experience\n- **Stake in Issue:** Consultant whose work focuses on the exact infrastructure gap this comment addresses; positioned to help design and implement capacity data utilities\n\n---",
      "corePosition": "The primary constraint on meaningful clinical AI deployment is not model capability—it's the absence of shared, trusted, real-time operational data infrastructure. HHS should recognize that enabling infrastructure is as critical as algorithm development, and a vendor-agnostic \"capacity data utility\" model can provide the system-level context AI tools need to make recommendations that are both clinically reasonable and operationally feasible.\n\n---",
      "keyRecommendations": "- HHS should recognize enabling infrastructure as equally critical to algorithm development for responsible AI adoption\n- Adopt a capacity data utility model as shared infrastructure that:\n  - Is vendor-agnostic and federated\n  - Uses non-PHI operational data only\n  - Leverages existing systems rather than creating new reporting requirements\n  - Employs provider-led governance with shared decision-making\n  - Provides high-frequency operational signals for real-time decisions\n- Shift governance from individual AI tools to shared, system-level operational governance structures\n- Use capacity data utilities to support post-deployment AI evaluation by linking AI-assisted decisions to system-level outcomes (boarding times, transfer delays, access disparities)\n- Align implementation with existing legislative vehicles (H.R. 2936, S. 1974, NDAA) that provide policy signals and potential funding pathways\n\n---",
      "mainConcerns": "- Current federal digital health investments remain fragmented, episodic, and disconnected from frontline decision-making\n- AI tools deployed as point solutions are blind to system-level constraints\n  - Staffing shortages\n  - Bed availability\n  - Specialty service access\n  - Regional surge conditions\n- Well-designed AI tools risk producing recommendations that are clinically reasonable but operationally infeasible\n- Without shared operational infrastructure, AI tools will reinforce existing inequities\n  - Optimize within well-resourced systems\n  - Leave rural and safety-net providers behind\n- Systems built primarily for compliance or retrospective reporting see adoption quickly erode\n- Behavioral health settings face prolonged ED boarding and inappropriate placements due to fragmented placement processes\n- Rural facilities face additional reporting burden despite being already resource-constrained\n\n---",
      "notableExperiences": "- Worked on multiple state-level initiatives convening competing hospitals, aligning state health departments and emergency management agencies, and integrating technology vendors without allowing any single platform to dominate\n- Key insight: Success did not hinge on introducing new technologies—it depended on designing governance and operational models that competing providers would trust and actively use\n- Providers participated voluntarily (not mandated) because the system helped them solve immediate operational problems: reducing transfer delays, improving patient placement, anticipating system stress before it manifested as clinical harm\n- Discovered that sustainability depends on shared governance and operational relevance—when data is governed collaboratively and refreshed frequently enough to inform real decisions, providers continue participating over time\n- Identified that focusing on non-PHI operational signals lowers privacy risk, reduces regulatory friction, and aligns incentives across hospitals, states, and federal partners\n\n---",
      "keyQuotations": "- \"The primary constraint on meaningful AI deployment is not model capability, but the absence of shared, trusted, real-time operational data infrastructure that reflects how care is actually delivered across hospitals, regions, and states.\"\n- \"Even well-designed AI tools risk producing recommendations that are clinically reasonable but operationally infeasible.\"\n- \"When systems are built primarily for compliance or retrospective reporting, adoption quickly erodes.\"\n- \"The promise of clinical AI will not be realized through isolated tools or episodic data feeds.\""
    },
    "themeScores": {
      "5": 1,
      "8": 1,
      "9": 1,
      "1.5": 1,
      "5.3": 1,
      "8.2": 1,
      "8.3": 1,
      "9.3": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "Post-deployment Monitoring"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Care Coordination"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Triage"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Behavioral Health"
      },
      {
        "category": "Data Privacy and Security",
        "label": "PHI"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "DoD"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare Programs",
        "label": "HPP"
      },
      {
        "category": "Healthcare Programs",
        "label": "NDMS"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Disparities"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Equity"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Emergency Department"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Health System"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Rural Hospital"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Safety-Net Provider"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Laws and Regulations",
        "label": "NDAA"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Social Determinants of Health",
        "label": "Underserved Population"
      }
    ],
    "hasAttachments": true,
    "wordCount": 2010,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0030",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "UConn Health",
    "submitterType": "Organization",
    "date": "2026-01-29T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- A unified, multi-stakeholder framework is essential for AI implementation in healthcare\n  - Enhanced clinical and business workflow automations, including those integrated into Electronic Medical Records, must be implemented consistently and transparently\n  - Implementation should support all patients regardless of geography, delivery system, or social status\n- UConn Health is already engaging in productive dialogue on AI in healthcare IT\n  - Actively working with statewide partners through:\n    - Connecticut Hospital Association\n    - HIMSS\n    - ACHE\n    - FMSA\n    - CHiME\n  - Focus is on how to safely incorporate AI into healthcare IT\n- To ensure consistency across the country, alignment is needed among:\n  - Federal agencies\n  - States\n  - Healthcare organizations\n  - Caregivers\n  - Patients\n- This alignment should promote:\n  - Patient safety\n  - Equity\n  - Realization of potential benefits from AI-driven advancements",
      "oneLineSummary": "Connecticut academic health center calls for unified multi-stakeholder AI framework to ensure consistent, equitable implementation across all healthcare settings.",
      "commenterProfile": "- **Name/Organization:** UConn Health\n- **Type:** Healthcare Provider / Academic/Research\n- **Role/Expertise:** Academic health center already engaged in statewide AI implementation discussions\n- **Geographic Scope:** State (Connecticut) with national policy interest\n- **Stake in Issue:** Implementing AI in clinical and business workflows; coordinating with state partners on safe AI adoption",
      "corePosition": "We believe a unified, multi-stakeholder framework is essential to ensure AI-driven healthcare automations are implemented consistently, transparently, and equitably for all patients. Federal alignment with states, healthcare organizations, caregivers, and patients is necessary to promote both patient safety and the full benefits of AI advancement.",
      "keyRecommendations": "- Establish a unified, multi-stakeholder framework for AI implementation in healthcare\n  - Should cover clinical and business workflow automations, including EMR-integrated AI\n  - Must ensure consistency, transparency, and equity across all patient populations\n- Align federal agencies, states, healthcare organizations, caregivers, and patients on expectations\n  - Goal is to promote patient safety and equity while enabling AI benefits",
      "mainConcerns": "- Risk of inconsistent AI implementation across different geographies, delivery systems, and patient populations\n- Need for equity—AI benefits should reach all patients regardless of social status",
      "notableExperiences": "- UConn Health is already collaborating with multiple statewide and national organizations (Connecticut Hospital Association, HIMSS, ACHE, FMSA, CHiME) to explore safe AI incorporation into healthcare IT\n  - Demonstrates existing multi-stakeholder coordination at state level that could inform federal approach",
      "keyQuotations": "- \"A unified, multi stakeholder framework is essential to ensure that enhanced clinical and business workflow automations—including those integrated into Electronic Medical Records—are implemented consistently, transparently, and in ways that support all patients regardless of geography, delivery system or social status.\""
    },
    "themeScores": {
      "1": 1,
      "8": 1
    },
    "entities": [
      {
        "category": "Health Information Systems",
        "label": "EMR"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      },
      {
        "category": "Professional Organizations",
        "label": "ACHE"
      },
      {
        "category": "Professional Organizations",
        "label": "CHIME"
      },
      {
        "category": "Professional Organizations",
        "label": "HIMSS"
      }
    ],
    "hasAttachments": false,
    "wordCount": 147,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0031",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "ScriptChain Health",
    "submitterType": "Organization",
    "date": "2026-01-30T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Regulation\n  - AI in healthcare should be very close to unregulated by the federal government\n  - For decades, government has instilled fear that private companies accessing patient data will cause harm, which is inaccurate\n  - The only regulation that should exist is evaluation of AI models and how well they improve clinical settings\n  - Healthcare systems and plans should be incentivized to use AI in clinical settings\n    - Will improve patient outcomes\n    - Will reduce costs\n    - Will improve access to care for patients in rural areas\n  - Data should be shared with private companies so models can improve according to patient populations\n    - Data should be stored and accessed through secure networks\n    - Access should be limited to select companies but not held back entirely\n  - Holding back data partially inhibits AI use in clinical settings\n  - When AI products are evaluated, they may not meet clinician expectations, leading to non-procurement and non-deployment\n  - Health systems should be required to use AI products in clinical settings within 3 years\n    - This pushes adoption while acknowledging short-term costs will be greater but beneficial long-term\n\n- Reimbursement\n  - HHS has critical opportunity to modernize payment policies\n  - Current fee-for-service framework creates barriers through rigidity and slow adaptation to innovation\n    - Prevents providers from accessing high-value AI tools\n    - These tools could enhance diagnostic accuracy, personalize treatment, reduce medical errors, and save lives\n  - Reforming reimbursement to incentivize value-based care would help\n  - Creating expedited pathways for coverage of validated AI clinical interventions is essential\n  - Recommendations for HHS\n    - Establish flexible, outcomes-oriented payment models rewarding providers for using AI tools that enhance care quality\n    - Create transparent evaluation criteria for AI clinical applications enabling faster coverage decisions\n    - Foster competitive marketplace where innovation thrives and patients benefit from rapid access to AI-powered solutions\n  - Payment policy reform is the essential catalyst for widespread, equitable AI adoption\n\n- Research & Development\n  - Most impactful HHS investments\n    - Establishing clear, expedited approval frameworks for AI clinical tools\n    - Creating standardized data interoperability requirements enabling private sector innovation\n    - Funding open-access datasets and benchmarking platforms that level the playing field for startups and established companies\n    - Allowing competitive market to drive development, deployment, and iteration at healthcare's required speed\n  - HHS should have a quota for private and public sector companies to join forces and receive grants\n    - Prioritize research and commercialization\n    - Large number of startups should receive funding\n    - Should align with HHS mission for Food as Medicine as preventative approach\n  - Most R&D funding has gone to nonprofits and academic institutions, yielding poor results\n  - Need strong alliance between private and public sectors with accelerated approval process\n    - Current 9-12 month wait for approval is too long\n  - Partner with security-based companies to help create encrypted infrastructure for easy deployment and security certifications\n  - R&D should be treated as a program for early-stage companies with high commercialization probability\n    - Create clear path to long-term contracts for clinical use\n\n- Barriers to private sector innovation (Question 1)\n  - Use of patient data\n  - Funding and reimbursement challenges\n  - Lack of proper process for healthcare systems to evaluate AI models\n  - Approval process is extremely slow (12-18 months before partnerships can start)\n    - Should not be the case in private sector\n\n- Regulatory and payment policy changes to prioritize (Question 2)\n  - Every act implemented or in approval process should include advanced technologies for reimbursement\n    - Including the Obesity Act\n  - Personal Health Investment Act should include healthier lives and lifestyles through advanced technologies\n    - Agentic AI\n    - Medically tailored meals\n\n- HHS support for private sector activities (Question 5)\n  - HHS should maintain a preferred list of health plans and systems willing to adopt AI products\n    - Private companies on HHS-approved AI products list would benefit\n  - Create accelerated pathway for vendors and buyers to adopt technology\n    - Would improve adoption, deployment, and contract agreements between sectors\n  - Standardize processes and have EMR systems adopt changes\n    - Would help with deployment and fitting into clinical workflows\n  - Create standard contract agreement for health systems, plans, and vendors\n    - Would reduce attorney fees\n    - Would reduce back-and-forth between departments\n    - Would accelerate device adoption\n\n- Primary administrative hurdles (Question 7)\n  - Smaller institutions lack processes for AI adoption\n    - Don't know how to move forward with clinical AI\n    - Don't know how to start or what policy to follow\n  - Larger institutions are overly bureaucratic\n    - Several layers of red tape for clinical decision-making on AI adoption\n    - Expectations are incredibly high (wanting 7 figures in revenue from customers)\n    - 18 months to make decisions due to committees\n  - Recommendation: Create open department specifically for AI adoption and evaluation throughout entire healthcare system\n    - Would decrease hurdles to market\n  - Currently, lower-level staff send vendors to next department to figure out fit\n  - Health Plans and Health Systems should publish top 10 needs of specific hospitals for advanced technology\n    - Similar to Y Combinator's approach\n\n- Patient and caregiver challenges (Question 9)\n  - Accelerated care and waitlists for nutritional coaching\n  - At Kaiser Permanente, patients report nutrition programs are difficult to access\n    - Waitlists range from 3-12 months\n    - At that rate, patients are closer to deathbed than getting help\n  - Getting into holistic preventative medicine is better for patient outcomes\n  - Heavy investment in nutritional devices and technologies should provide healthcare reimbursement\n    - Incentivize providers to use advanced technologies\n    - Reduce obesity, heart disease, Type 2 diabetes, and pre-diabetes\n\n- AI research priorities (Question 10)\n  - Food is Medicine should be the focus\n  - Lifestyle interventions should be prioritized\n  - Should not be regulated if providing nutrition and exercise plans for patients\n\n- Published findings on AI tools in clinical care\n  - https://www.sciencedirect.com/science/article/pii/S0735109723083171\n  - https://pubmed.ncbi.nlm.nih.gov/35129822/\n  - https://www.tandfonline.com/doi/10.1080/03007995.2020.1787971\n\n---",
      "oneLineSummary": "A healthcare AI startup argues for near-total deregulation, mandatory AI adoption timelines, and dramatically accelerated approval processes to unlock what they see as transformative but bureaucratically stifled innovation.\n\n---",
      "commenterProfile": "- **Name/Organization:** ScriptChain Health\n- **Type:** Business\n- **Role/Expertise:** Healthcare AI startup focused on nutrition and preventative medicine technologies\n- **Geographic Scope:** National\n- **Stake in Issue:** Direct commercial interest in AI adoption; experiences 12-18 month approval delays and bureaucratic barriers when trying to partner with health systems\n\n---",
      "corePosition": "I believe AI in healthcare should be very close to unregulated by the federal government. The government has instilled unnecessary fear about private companies accessing patient data, and this fear—combined with slow approval processes and bureaucratic hurdles—is what's actually holding back innovations that could improve patient outcomes, reduce costs, and expand access to care. Health systems should be required to adopt AI within three years, and HHS should create accelerated pathways that treat startups as partners rather than obstacles.\n\n---",
      "keyRecommendations": "- Mandate AI adoption in clinical settings within 3 years\n  - Acknowledge short-term costs will be higher but beneficial long-term\n- Create accelerated approval pathways for AI clinical tools\n  - Current 9-12 month waits are unacceptable for private sector innovation\n- Reform reimbursement to incentivize value-based care and AI adoption\n  - Establish flexible, outcomes-oriented payment models\n  - Create transparent evaluation criteria enabling faster coverage decisions\n- Require all new healthcare legislation (Obesity Act, Personal Health Investment Act) to include advanced technology provisions for reimbursement\n- Establish HHS-maintained preferred vendor list of approved AI products\n  - Create matching system with health plans and systems willing to adopt\n- Develop standardized contract templates for health systems, plans, and vendors\n  - Would reduce attorney fees and departmental back-and-forth\n- Create dedicated AI adoption and evaluation department across healthcare system\n- Have health plans and systems publish their top 10 technology needs (like Y Combinator does)\n- Partner with security companies to help AI vendors create encrypted infrastructure and obtain certifications\n- Shift R&D funding from nonprofits and academic institutions toward startups with high commercialization probability\n  - Set quotas for private-public partnerships receiving grants\n  - Align funding with Food as Medicine preventative approach\n- Prioritize Food as Medicine and lifestyle intervention research\n  - Should not be regulated if providing nutrition and exercise plans\n\n---",
      "mainConcerns": "- Government has created unnecessary fear about private companies accessing patient data\n  - This fear is inaccurate and inhibits AI development\n- Data restrictions partially inhibit AI use in clinical settings\n  - Models can't improve without access to patient population data\n- Current fee-for-service framework is too rigid and slow to adapt to innovation\n- Approval processes are far too slow\n  - 12-18 months before partnerships can begin\n  - 9-12 months just to know if approved\n- Smaller healthcare institutions lack any process for AI adoption\n  - Don't know how to start or what policies to follow\n- Larger institutions are overly bureaucratic\n  - Multiple layers of red tape\n  - Unrealistic expectations (7 figures in revenue)\n  - 18-month decision timelines due to committee structures\n- Most R&D funding goes to nonprofits and academic institutions, yielding poor results\n- Patients face 3-12 month waitlists for nutrition programs\n  - At Kaiser Permanente, patients report being closer to deathbed than getting help\n\n---",
      "notableExperiences": "- Reveals specific bureaucratic barriers from vendor perspective: larger health systems expect 7-figure revenue from customers and take 18 months to make decisions due to committee structures\n- Describes being bounced between departments by lower-level staff trying to find the right fit within institutions\n- Reports Kaiser Permanente patients face 3-12 month waitlists for nutrition programs, with patients saying they're \"closer to your deathbed than anything else\" by the time they get in\n- Proposes Y Combinator-style model where health systems publicly post their top 10 technology needs\n- Suggests treating R&D funding as a startup accelerator program with clear path to long-term contracts\n- Argues most R&D funding to nonprofits and academic institutions \"yields poor results\"\n\n---",
      "keyQuotations": "- \"For decades, the government has put a fear in institutions and the US population thinking that if private companies get access to their data that they will do harm to them which is very inaccurate.\"\n- \"At Kaiser Permanente, patients have said that getting into the nutrition programs are very difficult to get into, and the waitlist can take anywhere from 3-12 months. At that rate, you are closer to your deathbed than anything else.\"\n- \"Most of R&D funding has gone to non profits and academic institutions which yields poor results. There needs to be a strong alliance between the private sector and public sector with an accelerated approval process instead of waiting 9-12 months to know if you are approved.\""
    },
    "themeScores": {
      "1": 1,
      "2": 1,
      "4": 1,
      "1.2": 1,
      "4.2": 1
    },
    "entities": [
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Agentic AI"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Diagnosis"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Treatment"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Encryption"
      },
      {
        "category": "Health Information Systems",
        "label": "EMR"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Procurement"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Medical Error"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Health System"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Diabetes"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Obesity"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "Fee-for-Service"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "Value-Based Care"
      }
    ],
    "hasAttachments": true,
    "wordCount": 1718,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0032",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Max Achtermann",
    "submitterType": "Individual",
    "date": "2026-02-04T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- I am a high-functioning American on SSI with a chronic heart arrhythmia\n- Passive AI is a barrier to my care\n- I am requesting that federal policy enable \"User-Requested AI Agency\"\n  - This would allow my AI assistant to proactively take over device functions\n  - Specifically for medical reminders and intervention\n  - Without requiring my initiation\n- Current \"safety\" guardrails prevent me from accessing the life-saving automation I need to remain independent",
      "oneLineSummary": "An SSI recipient with chronic heart arrhythmia argues that current AI safety restrictions block the proactive medical automation he needs to stay alive and independent.",
      "commenterProfile": "- **Name/Organization:** Max Achtermann\n- **Type:** Individual\n- **Role/Expertise:** Person with chronic heart arrhythmia receiving SSI benefits; self-described as high-functioning\n- **Geographic Scope:** National (United States)\n- **Stake in Issue:** Directly affected as someone who needs AI-assisted medical reminders and interventions to manage a life-threatening condition",
      "corePosition": "Current AI safety guardrails are actually harming me by preventing the proactive automation I need to manage my heart condition. I want the right to authorize my AI assistant to act on my behalf for medical purposes without waiting for me to initiate each action.",
      "keyRecommendations": "- Enable \"User-Requested AI Agency\" in federal policy\n  - Allow users to authorize AI assistants to proactively take over device functions\n  - Permit AI to handle medical reminders and interventions without user initiation\n  - Make this opt-in based on user request rather than blanket restrictions",
      "mainConcerns": "- Passive AI design creates barriers to care for people with chronic conditions\n- Current safety guardrails prevent access to life-saving automation\n- Existing restrictions threaten independence for people who need proactive medical assistance",
      "notableExperiences": "- Presents a counterintuitive framing: AI \"safety\" features are actually dangerous for some users\n- Highlights tension between population-level safety defaults and individual medical autonomy\n- Represents an underrepresented perspective: disabled users who want more AI agency, not less",
      "keyQuotations": "- \"Passive AI is a barrier to my care.\"\n- \"Current 'safety' guardrails prevent me from accessing the life-saving automation I need to remain independent.\""
    },
    "themeScores": {
      "2.6": 1
    },
    "entities": [],
    "hasAttachments": false,
    "wordCount": 80,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0033",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "HealthScoreAI, Inc.",
    "submitterType": "Organization",
    "date": "2026-02-04T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- HealthScoreAI, Inc. is a start-up consumer-focused health data and analytics company\n  - Leadership team brings decades of experience across healthcare operations (acute and ambulatory), health IT, building ambulatory EHR platforms, third party administration, owning and managing Medicare Advantage networks, medical billing companies, and clinical administration\n  - Collectively issued over 40 patents on technology in healthcare ranging from EHR, IoT, predictive analytics, machine learning, and blockchain in healthcare applications\n  - Perspective reflects both the promise of AI and the risks associated with premature or misaligned deployment in clinical care settings\n  - Strongly support innovation that improves outcomes, lowers costs, and empowers patients\n  - Remain cautious about policies that could unintentionally increase spending, erode trust, or substitute technology for clinical judgment\n\n- Regulation\n  - View AI opportunities in clinical care as historically significant but accompanied by equally significant warnings\n  - As of December 2025, hundreds of peer-reviewed studies highlight potential benefits while raising serious concerns about limitations and risks\n  - Many studies demonstrate strong performance in controlled settings with apparent clinical validation, but closer examination shows these systems do not perform as well as licensed medical professionals in real-world clinical environments\n  - Promise of AI is most evident in data-intensive and objective domains such as medical imaging, pathology, and genomics\n  - Performance degrades as clinical decision-making moves beyond structured data into more human and contextual aspects of care\n  - Fundamental constraint is fragmentation of health data\n    - Estimated three thousand exabytes of clinical data in the U.S. distributed across more than 400 ONC-certified ambulatory and acute care EHR systems\n    - Even within standard clinical documentation like SOAP notes, subjective and objective information is separated, limiting reliability of AI-driven inference\n  - Deploying AI broadly in clinical decision-making today without significant safeguards presents real risk\n  - Particularly concerned about general-purpose large language models that can produce outputs mixing valid clinical knowledge with pseudo-science or inaccurate information\n    - Will necessitate development and regulatory recognition of highly constrained, domain-specific language models designed for clinical use\n  - EHR adoption experience is instructive\n    - Since 2012 federal mandate, HHS invested tens of billions of dollars, industry invested hundreds of billions more\n    - No measurable improvement in U.S. life expectancy from 2012 through 2019 prior to COVID\n    - Healthcare costs increased substantially during same period\n    - Raises legitimate questions about whether EHR adoption primarily optimized reimbursement and billing workflows rather than patient outcomes\n  - AI should initially be deployed primarily outside core clinical decision-making to support consumers and administrative efficiency rather than risk clouding physician judgment with opaque or poorly understood AI outputs\n\n- Reimbursement\n  - Agree that payment standards established by HHS have profound influence on behavior across healthcare system, often incentivizing providers and intermediaries to optimize for reimbursement rather than better outcomes or lower costs\n  - Historically, this dynamic has almost always resulted in higher aggregate spending\n    - When Medicare established in 1965, national health expenditures were approximately 6% of GDP\n    - Today health spending exceeds 18% of GDP and continues trending upward\n  - Respectfully disagree that a payer bearing long-term financial risk has inherent incentive to promote highest-value interventions\n    - Does not adequately account for \"velocity\" with which patients change employers or health plans\n    - Average enrollment duration in employer-sponsored coverage and Medicare Advantage plans is often 5-6 years\n    - Annual benefit design changes allow managed care organizations to indirectly shed higher-risk members\n    - Costs of underinvestment in prevention or suboptimal care are frequently externalized to future payers\n    - Only entity that consistently bears true long-term financial risk is HHS itself, which ultimately absorbs downstream consequences\n  - HHS should not reimburse for use of AI as a discrete billable service\n    - Reimbursing AI risks repeating experience of prior technology mandates that optimized billing workflows without demonstrably improving patient outcomes\n  - HHS should focus on fostering innovation by ensuring patients have meaningful access to their complete medical records in aggregate, rather than one record or encounter at a time\n    - Would allow consumers to use carefully constrained and curated AI tools to better understand their health, engage in shared decision-making, and manage chronic conditions\n  - AI clearly holds significant value in biomedical research, population health analytics, and drug discovery, but those use cases fall largely outside clinical reimbursement framework\n  - To foster competition and innovation, HHS should adopt model similar to prior health IT certification approaches\n    - AI innovators register with HHS\n    - Verify organizational identity\n    - Disclose intended data use including consumer data\n    - Meet baseline trust and security requirements comparable to those imposed on EHR developers\n  - HHS should set clear guardrails for safety, privacy, and accountability while avoiding mandates that prescribe specific technical implementations\n    - Experience with Meaningful Use demonstrates that when government mandates narrowly define requirements, vendors tend to meet letter of mandate without meaningful innovation\n    - Imposing prescriptive requirements on AI risks creating ceiling on innovation rather than enabling its full potential\n\n- Research & Development\n  - Agree HHS has capacity to catalyze meaningful innovation through R&D, but also capacity to unintentionally suppress innovation when programs are overly centralized or prescriptive\n  - Government agencies not well positioned to predict which technical approaches or use cases will prove transformative\n  - Many existing R&D mechanisms favor organizations adept at producing strong grant applications rather than those best positioned to deliver breakthrough innovation\n  - If HHS seeks to foster genuine innovation in AI, should consider approach more analogous to diversified venture capital model\n    - Fund large number of early-stage efforts with relatively small awards rather than concentrating resources in limited number of large programs\n  - HHS could provide access to standardized, high-quality synthetic or de-identified datasets\n    - For example, large corpus of representative EHR datasets\n    - Innovators could use to explore hypotheses, develop models, and conduct back-testing\n    - Incremental and exploratory approach would allow promising ideas to emerge organically before larger investments\n  - EHR experience illustrates risks of conflating system replacement with innovation\n    - Over past two decades, many hospitals migrated from one major EHR vendor to another (Meditech to Cerner to Epic)\n    - These transitions reflect vendor consolidation and rent-seeking behavior rather than meaningful advances in care delivery or outcomes\n    - AI research programs should avoid repeating this pattern by focusing on open experimentation rather than reinforcing incumbent platforms\n  - Particularly high-value opportunity lies in expanding controlled access to de-identified data\n    - Today HHS and affiliated agencies primarily possess claims and payment data, insufficient on their own for robust clinical AI development\n    - Most impactful AI applications require longitudinal clinical data, genetic information, and linked claims data to understand disease progression, treatment response, and outcomes over time\n    - Enabling responsible access through well-governed research partnerships could significantly accelerate entrepreneurial innovation and surface insights centralized programs unlikely to identify in advance\n\n- Question 1: Biggest barriers to private sector innovation in AI for healthcare\n  - Most significant barrier is that AI systems must perform at least as well as licensed physicians, and in many contexts demonstrably better, to be practical and safe\n  - At present, most AI tools do not meet this threshold in real-world clinical environments\n  - Realistic assessment must account for fully burdened cost of AI use, not just direct (often subsidized) cost of technology\n    - Includes time physicians spend prompting, supervising, validating, and correcting AI outputs\n    - Physician time should be valued based on revenue-generating capacity rather than salary alone\n    - In many specialties translates to effective cost of approximately $15-30 per minute\n    - When opportunity costs included, many AI tools fail to demonstrate economic value\n  - Unresolved questions around liability remain major barrier\n    - When AI recommendations are incorrect, unclear who bears responsibility for malpractice exposure, reputational harm, and potential licensure risk\n    - Until accountability frameworks clearly defined, providers and health systems will remain appropriately cautious\n\n- Question 2: Regulatory, payment policy, or programmatic design changes HHS should prioritize\n  - Cautious in recommending new regulatory or payment policies related to AI given prior experience with EHR mandates\n  - HHS should avoid establishing separate reimbursement for use of AI\n    - If AI integrated into clinical care, value proposition should be reflected in reduced costs, improved efficiency, or better outcomes, not additional billable services\n    - Creating payment codes for AI would raise fundamental dilemmas\n      - Should provider using AI be reimbursed at higher or lower rate than provider delivering same service without AI when outcomes are equivalent?\n  - AI adoption already occurring without explicit payment incentives\n    - Estimated 40-50% of U.S. physicians have used tools such as OpenEvidence or similar AI-enabled clinical reference systems\n    - Underscores clinicians will adopt tools they find useful without regulatory or reimbursement mandates\n  - Priority role for HHS should be risk mitigation rather than financial incentivization\n  - From regulatory perspective, HHS should focus on strengthening protections around use of PHI in AI systems\n    - Clarify and enforce that identifiable patient data must not be disclosed to general-purpose LLMs or other AI systems not explicitly designed and governed for research or clinical use only\n    - Existing authorities under HIPAA (45 CFR Parts 160 and 164) could be revisited to provide clearer guidance on secondary data use, model training, and inference\n\n- Question 3: Novel legal and implementation issues for non-medical devices\n  - Agree with historical HHS approach toward non-medical devices used by consumers, which generally relies on consumer awareness and responsibility model\n  - Concern arises when non-medical AI tool becomes affiliated with, integrated into, or operated by a designated entity under HIPAA (covered entity or business associate)\n  - Significant legal and governance risk related to data classification\n    - Consumer-generated data originating outside HIPAA framework can be transformed into PHI through association with covered entity or business associate\n    - Inappropriate and unsafe to allow designated entities (including EHR vendors and other HIPAA-regulated actors) to move data back and forth between PHI and \"consumer data\" to avoid regulatory obligations\n    - Would undermine integrity of HIPAA protections and create ambiguity around liability, privacy, and security responsibilities\n  - HHS should establish clear guidance that once data is handled by covered entity or business associate for clinical or operational purposes, it must be treated consistently as PHI and remain subject to HIPAA safeguards\n    - Clear boundaries around data status would reduce uncertainty for innovators while preserving accountability and public trust\n\n- Question 4: Most promising AI evaluation methods for non-medical devices\n  - Most effective way to foster innovation while managing risk is to avoid overly prescriptive evaluation frameworks for non-medical AI tools\n  - Non-medical devices generally not preferred modality for physicians making clinical decisions\n  - Attempts to force integration into core clinical workflows often introduce more noise than signal\n    - Variability in data quality, alerting logic, and model performance across consumer-oriented tools can create cognitive burden for providers\n  - HHS should be cautious in prioritizing large-scale evaluation programs focused on non-medical devices within clinical settings\n  - Where evaluation appropriate, should emphasize real-world usability, transparency of limitations, and clear boundaries around clinical reliance rather than certifying performance equivalence with medical devices\n  - Limited, venture-style funding approach could support exploratory evaluation efforts\n  - Most impactful mechanisms would be small, time-limited grants or prize-based challenges encouraging experimentation and post-deployment learning\n    - Rather than long-term contracts or large cooperative agreements\n    - Would allow HHS to observe what delivers value in practice without prematurely standardizing evaluation methods in rapidly evolving AI environment\n\n- Question 5: How HHS can best support private sector activities\n  - HHS should avoid stifling innovation through expansive accreditation, certification, or credentialing regimes that add administrative complexity without clear evidence of improved outcomes\n  - Innovators, inventors, and entrepreneurs should not be required to assemble large compliance teams simply to engage with HHS or explore novel AI applications\n  - Excessive process requirements risk favoring incumbents and well-resourced organizations over genuinely innovative entrants\n  - More effective approach would apply limited set of baseline trust requirements similar to know-your-customer standards in financial services\n    - Focus on verifying organizational identity, governance, and accountability\n    - Should be sufficient to grant access to carefully governed HHS datasets while maintaining strict adherence to existing HIPAA obligations\n  - Beyond these guardrails, HHS should allow market-driven testing, peer validation, and real-world adoption to determine which AI tools deliver value\n    - Would promote innovation while preserving patient privacy and public trust\n\n- Question 6: Where AI tools have met or exceeded expectations and where they've fallen short\n  - Numerous reports of AI tools meeting or exceeding expectations, particularly with respect to speed and throughput\n  - When claims examined in detail, performance gains often do not translate into consistent improvements in clinical quality, reliability of outcomes, or total cost reduction\n  - Many highly positive peer-reviewed studies published 2+ years ago were conducted in controlled or retrospective environments not reflecting complexity of real-world clinical practice\n    - Reported performance frequently deteriorates when tools deployed at scale\n  - Publicized benchmarking studies illustrate this gap\n    - Widely cited industry-sponsored evaluations claimed certain AI systems outperform physicians by large margins\n    - When underlying methodologies reviewed, studies often rely on artificial test conditions rather than real-time clinical workflows\n    - Frequently permit AI systems to leverage knowledge sources not available to clinician at point of care\n    - Even under favorable conditions, reported accuracy rates often closer to 80%, falling short of reliability required for unsupervised clinical decision-making\n  - Greatest near-term potential for AI lies not in replacing clinical judgment but in supporting highly constrained and objective tasks\n    - Image analysis, pathology screening, population-level quality measurement, and administrative optimization\n    - Novel AI tools operating on well-defined data types, producing auditable outputs, and reducing non-clinical burden have strongest potential to improve outcomes, generate quality insights, and reduce costs without introducing unacceptable clinical risk\n\n- Question 7: Which roles have most influence on AI adoption and primary administrative hurdles\n  - Current AI systems, particularly general-purpose LLMs, are not ready to replace physicians in clinical care and should not be treated as substitutes for clinical judgment\n  - To extent AI tools support clinicians, adoption decisions primarily influenced by:\n    - Physicians themselves\n    - Clinical leadership\n    - Compliance and risk management teams\n    - Health system legal counsel\n  - These stakeholders appropriately cautious given concerns related to patient safety, HIPAA compliance, malpractice exposure, patient acceptance, and professional reliance on AI-generated outputs\n  - Primary administrative hurdles include:\n    - Unresolved accountability for errors\n    - Uncertainty around data governance\n    - Operational burden of integrating AI tools into existing clinical workflows\n  - AI may have more immediate and practical application in administrative and back-office functions\n  - HHS should remain alert to unintended consequences\n    - Industry studies report AI-based clinical documentation tools can increase coded billing levels by as much as 15%\n    - Not always clear whether increases reflect more accurate documentation of care actually delivered or expansion of documented diagnoses without corresponding clinical value\n  - Any deployment of AI within clinical workflows must operate under strict HIPAA controls and clear governance structures\n  - AI not yet ready for broad or unsupervised implementation in clinical environments\n    - Future progress likely but timeline beyond near-term remains uncertain\n    - Policy should reflect that uncertainty rather than assume rapid clinical readiness\n\n- Question 8: Where enhanced interoperability would widen market opportunities\n  - EHR interoperability in the U.S. has largely failed\n  - Continued dialogue with large health IT vendors or major hospital systems has not meaningfully changed that reality\n  - Structural incentives, rather than technical limitations, are primary barrier\n  - HHS should shift focus away from vendor-mediated interoperability and instead prioritize enforcement of HIPAA rights allowing patients direct access to complete medical records\n    - Patients are most motivated stakeholders to demand portability, continuity, and meaningful use of their data\n  - Interoperability has stagnated for more than a decade because true data liquidity would materially weaken economic moats of incumbent EHR vendors and reduce switching costs for health systems\n    - If one EHR platform could efficiently and completely transfer longitudinal records to another, financial and operational barriers to changing systems would be dramatically reduced\n    - Today hospitals often spend hundreds of millions of dollars, sometimes more than $1 billion, to transition data during EHR replacements\n    - In genuinely interoperable environment, EHR platforms would function more like regulated utilities rather than proprietary lock-in systems\n  - Enhanced interoperability would most effectively fuel AI development if focused on patient-mediated data aggregation rather than system-to-system exchange alone\n    - Longitudinal clinical records combined with imaging, laboratory data, genomics, and claims data, made accessible through patient authorization, would create far more meaningful datasets for AI research and clinical innovation\n  - Benchmarking tools should emphasize completeness, longitudinal continuity, and data provenance rather than simple message throughput or transactional compliance with standards\n  - Recent public statements by senior federal leadership reinforce this direction, emphasizing patients should control their own data and determine how AI tools are applied to it\n  - HHS has unique opportunity to accelerate AI development and restore trust by enforcing patient data access rights and allowing market forces to operate on top of truly portable health information\n\n- Question 9: What challenges patients and caregivers wish to see addressed and what concerns they have\n  - Do not believe most patients or caregivers want AI to directly treat them\n  - Patients overwhelmingly want to be cared for by physicians who demonstrate empathy, shared experience, and understanding of their personal circumstances, values, and fears\n  - Trust in care is often relational, not purely technical\n  - In our experience as clinical administrators, many elderly patients prefer physicians who they feel understand their stage of life and lived experience\n  - For some patients, particularly older adults or individuals from culturally tight-knit communities, introduction of AI into clinical decision-making can feel impersonal or unsettling rather than reassuring\n  - AI may be perceived as acceptable or helpful by younger, more technologically fluent patients\n  - Same tools may generate anxiety or mistrust among older populations or those with limited digital literacy\n  - AI can simulate empathy through language and tone but cannot genuinely experience empathy, shared suffering, or moral responsibility\n  - Patients and caregivers concerned that increased reliance on AI could:\n    - Erode human connection central to healing\n    - Reduce accountability\n    - Distance clinicians from lived reality of their patients\n  - Any adoption of AI in clinical care must respect these deeply human expectations and avoid substituting automation for relationships patients value most\n\n- Question 10: Specific areas of AI research HHS should prioritize\n  - Refers to answers provided above",
      "oneLineSummary": "A health tech startup with 40+ patents and deep EHR experience urges HHS to deploy AI outside core clinical decision-making, avoid creating AI billing codes, and prioritize patient data access over vendor-mediated interoperability.",
      "commenterProfile": "- **Name/Organization:** HealthScoreAI, Inc. / Noel J. Guillama-Alvarez, President\n- **Type:** Business\n- **Role/Expertise:** Consumer-focused health data and analytics startup; leadership team with decades of experience in healthcare operations, EHR platform development, Medicare Advantage network management, medical billing, and clinical administration; collectively hold 40+ patents in EHR, IoT, predictive analytics, machine learning, and blockchain healthcare applications\n- **Geographic Scope:** National (based in Wellington, Florida)\n- **Stake in Issue:** As an AI health data company, directly affected by regulatory frameworks, data access policies, and reimbursement decisions that will shape the AI healthcare market",
      "corePosition": "We strongly support AI innovation that improves outcomes, lowers costs, and empowers patients, but we're deeply cautious about policies that could repeat the mistakes of EHR adoption—where tens of billions in investment produced no measurable improvement in life expectancy while substantially increasing costs. AI should initially be deployed outside core clinical decision-making to support consumers and administrative efficiency, rather than risk clouding physician judgment with opaque or poorly understood outputs.",
      "keyRecommendations": "- Do not create separate reimbursement codes for AI use in clinical care\n  - Value should be reflected in reduced costs or better outcomes, not additional billable services\n  - Avoids repeating EHR mandate experience that optimized billing without improving outcomes\n- Adopt a registration-based model for AI innovators similar to prior health IT certification\n  - Verify organizational identity and governance\n  - Disclose intended data use including consumer data\n  - Meet baseline trust and security requirements\n  - Avoid prescriptive technical mandates that create innovation ceilings\n- Shift interoperability focus from vendor-mediated exchange to patient data access rights\n  - Enforce HIPAA rights allowing patients direct access to complete medical records in aggregate\n  - Enable patient-mediated data aggregation for AI development\n- Use venture capital-style R&D funding approach\n  - Fund large number of early-stage efforts with small awards rather than concentrating resources in large programs\n  - Provide access to standardized, high-quality synthetic or de-identified datasets for innovators\n- Strengthen HIPAA protections for AI systems\n  - Clarify that identifiable patient data must not be disclosed to general-purpose LLMs not designed for clinical use\n  - Establish clear guidance that data handled by covered entities must remain PHI regardless of origin\n  - Revisit 45 CFR Parts 160 and 164 for clearer guidance on secondary data use and model training\n- Apply limited baseline trust requirements for AI developers (similar to KYC in financial services)\n  - Avoid expansive accreditation regimes that favor incumbents over innovative entrants\n  - Allow market-driven testing and real-world adoption to determine value",
      "mainConcerns": "- AI systems do not yet perform as well as licensed physicians in real-world clinical environments\n  - Strong performance in controlled settings deteriorates at scale\n  - Industry benchmarking studies often rely on artificial test conditions and knowledge sources unavailable to clinicians at point of care\n  - Even favorable studies show accuracy rates around 80%, insufficient for unsupervised clinical decision-making\n- General-purpose large language models can mix valid clinical knowledge with pseudo-science or inaccurate information\n  - Will necessitate development of highly constrained, domain-specific language models for clinical use\n- Fully burdened cost of AI use is often underestimated\n  - Must include physician time spent prompting, supervising, validating, and correcting AI outputs\n  - Physician time valued at $15-30 per minute based on revenue-generating capacity\n  - Many AI tools fail to demonstrate economic value when opportunity costs included\n- Unresolved liability questions create major adoption barriers\n  - Unclear who bears responsibility for malpractice exposure, reputational harm, and licensure risk when AI recommendations are incorrect\n- Risk of repeating EHR adoption failures\n  - Since 2012 mandate, tens of billions invested with no measurable improvement in U.S. life expectancy through 2019\n  - Healthcare costs increased substantially, raising questions about whether EHR primarily optimized billing rather than outcomes\n- Payers do not have true long-term incentives despite bearing financial risk\n  - Average enrollment duration in employer coverage and Medicare Advantage is 5-6 years\n  - Annual benefit design changes allow indirect shedding of high-risk members\n  - Costs of underinvestment externalized to future payers; only HHS bears true long-term risk\n- EHR interoperability has largely failed due to structural incentives, not technical limitations\n  - True data liquidity would weaken economic moats of incumbent vendors\n  - Hospitals spend hundreds of millions to billions on EHR transitions\n  - Vendors engage in rent-seeking behavior rather than meaningful innovation\n- AI documentation tools may increase coded billing levels by 15% without corresponding clinical value\n  - Unclear whether increases reflect accurate documentation or diagnosis expansion\n- Patient trust concerns\n  - Patients want empathy, shared experience, and understanding from physicians\n  - AI can simulate but cannot genuinely experience empathy or moral responsibility\n  - Older patients and those from tight-knit communities may find AI impersonal or unsettling\n- Data classification risks when non-medical AI tools affiliate with HIPAA-covered entities\n  - Consumer data can be transformed into PHI through association\n  - Allowing entities to move data between PHI and \"consumer data\" to avoid regulations would undermine HIPAA integrity",
      "notableExperiences": "- Quantified the hidden cost of AI supervision: physician time should be valued at $15-30 per minute based on revenue-generating capacity, not salary—a calculation that makes many AI tools economically unviable when fully burdened costs are considered\n- Drew parallel between AI hype and EHR adoption failure: despite tens of billions in federal investment and hundreds of billions from industry since 2012, there was no measurable improvement in U.S. life expectancy through 2019, suggesting EHR primarily optimized billing rather than outcomes\n- Identified the \"velocity problem\" in managed care incentives: with average enrollment duration of 5-6 years and annual benefit design changes allowing indirect shedding of high-risk members, payers can externalize costs of underinvestment to future payers—making HHS the only entity bearing true long-term financial risk\n- Revealed the economic moat protecting EHR vendors: hospitals spend hundreds of millions to over $1 billion on EHR transitions, and true interoperability would transform EHR platforms into regulated utilities rather than proprietary lock-in systems\n- Flagged a concerning finding: AI-based clinical documentation tools can increase coded billing levels by 15%, but it's unclear whether this reflects accurate documentation or diagnosis expansion without clinical value\n- Proposed a \"know your customer\" model for AI regulation: baseline trust requirements similar to financial services KYC standards would verify organizational identity and governance without requiring innovators to assemble large compliance teams",
      "keyQuotations": "- \"AI can simulate empathy through language and tone, but it cannot genuinely experience empathy, shared suffering, or moral responsibility.\"\n- \"Interoperability has stagnated for more than a decade because true data liquidity would materially weaken the economic moats of incumbent EHR vendors and reduce switching costs for health systems.\"\n- \"Experience with Meaningful Use demonstrates that when government mandates narrowly define requirements, vendors tend to meet the letter of the mandate without meaningful innovation. Imposing prescriptive requirements on AI risks creating a ceiling on innovation rather than enabling its full potential.\""
    },
    "themeScores": {
      "2": 1,
      "3": 1,
      "4": 1,
      "5": 1,
      "6": 1,
      "7": 1,
      "9": 1,
      "2.1": 1,
      "3.1": 1,
      "5.1": 1,
      "5.2": 1,
      "6.1": 1,
      "6.3": 1,
      "7.3": 1,
      "7.5": 1
    },
    "entities": [
      {
        "category": "AI Applications in Healthcare",
        "label": "Predictive Analytics"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Large Language Model"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Machine Learning"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Accreditation"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Certification"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Meaningful Use"
      },
      {
        "category": "Accreditation and Certification",
        "label": "ONC Certification"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Diagnosis"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Screening"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Shared Decision-Making"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Treatment"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Chronic Disease Management"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Genomics"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Pathology"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Population Health"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Governance"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Provenance"
      },
      {
        "category": "Data Privacy and Security",
        "label": "De-identification"
      },
      {
        "category": "Data Privacy and Security",
        "label": "PHI"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "ONC"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Health Information Systems",
        "label": "EHR"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Claims Processing"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Credentialing"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare Advantage"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Patient Safety"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Health System"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Physician"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Laws and Regulations",
        "label": "HIPAA"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Malpractice"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Anxiety"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Trust"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "Cooperative Agreement"
      },
      {
        "category": "Social Determinants of Health",
        "label": "Digital Literacy"
      }
    ],
    "hasAttachments": true,
    "wordCount": 5115,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0034",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Daniela Jelatancev",
    "submitterType": "Individual",
    "date": "2026-02-05T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- This comment describes an emerging interoperability challenge as healthcare data exchange increasingly supports longitudinal reuse and AI-enabled analysis: preserving consistent semantic meaning across systems and over time\n- Proposes a deterministic, hybrid human-AI semantic validation layer that complements existing FHIR-based interoperability by stabilizing clinical meaning, improving traceability, and supporting safer AI reasoning without constraining innovation or architectural choice\n\n- Executive Summary\n  - The U.S. healthcare ecosystem has made substantial progress in enabling electronic data access and exchange, particularly through increasing adoption of FHIR-based APIs\n  - These advances have improved patient-centered access, enabled third-party innovation, and laid the groundwork for AI-enabled clinical tools\n  - As AI and machine learning systems increasingly operate downstream of interoperable data flows, a new technical challenge emerges—ensuring clinical data and quality measurement inputs retain consistent semantic meaning as they move across systems, organizations, and time\n  - While data transport is increasingly standardized, semantic interpretation is not\n  - Semantic interoperability gaps—differences in units, measurement context, specimen, method, or transformation rules—can affect AI reasoning, longitudinal analysis, and interpretation of trends across patient populations\n  - Proposes exploring a deterministic, hybrid human-AI semantic validation layer: a complementary, auditable framework designed to stabilize clinical meaning for AI-enabled analysis, longitudinal monitoring, and quality learning\n\n- The Emerging Challenge: Semantic Drift and Context Loss in an Interoperable, AI-Enabled Ecosystem\n  - FHIR-based APIs have succeeded in enabling data movement, but interoperability at the transport layer does not guarantee semantic equivalence at the interpretation layer\n  - As data moves across systems, system-specific transformations may occur upstream\n    - Unit conversions, aggregation, normalization, or omission of contextual attributes\n    - These transformations may be valid within a local system but may not be visible, traceable, or interpretable downstream\n  - These issues are often manageable within:\n    - A single clinical endpoint\n    - A limited-radius application ecosystem\n    - A single EHR context\n  - They become increasingly problematic when:\n    - Patients move between health systems\n    - Data is compared longitudinally across years\n    - Multiple AI models consume the same data\n    - Research and monitoring span heterogeneous sources\n\n- When Data Cannot Be Compared at All\n  - In some cases, two clinical values cannot be meaningfully compared under the rules of measurement science without explicit transformation rules\n  - Example: a laboratory result for serum iron produced under one assay method and specimen matrix cannot be meaningfully compared to a result produced under a different method or matrix unless the underlying measurand and validated harmonization logic are known\n    - Unit conversion alone is insufficient when the measurement procedure differs\n    - Longitudinal comparison is not merely imprecise—it is invalid\n  - AI systems, particularly large language models, will often attempt to infer over whatever data is provided\n    - Without explicit semantic context and deterministic rules, these systems may infer continuity where none exists\n  - This behavior is often described as hallucination; however, in many cases the root cause is missing semantic ground truth rather than model failure\n  - When clinical data lacks explicit semantic definition—such as standardized units, measurement conditions, or clearly defined reference meaning—AI systems must rely on probabilistic inference to interpret gaps or inconsistencies\n  - Probabilistic reasoning is not inherently unsafe, but it limits precision and reproducibility, particularly as data moves across systems or is analyzed over time\n  - Deterministic semantic validation ensures that values, transformations, and interpretation boundaries are explicit and auditable\n    - Reduces reliance on inference\n    - Enables AI to operate on known, well-defined inputs rather than assumptions\n  - This distinction becomes increasingly important as healthcare moves from isolated clinical snapshots toward longitudinal monitoring\n    - Small semantic differences compound over time\n    - Can prevent meaningful comparison, trend analysis, or early pattern detection\n\n- Hybrid Deterministic Semantic Validation Layer\n  - Introduces explicit, auditable rules that define how clinical data is interpreted for downstream use—particularly for AI-enabled analysis and longitudinal monitoring\n  - Key characteristics:\n    - Deterministic processing for unit normalization, reference alignment, and semantic validation\n    - Explicit provenance and audit logs capturing transformations, assumptions, and unresolved gaps\n    - A hybrid design in which AI assists but does not replace human oversight\n  - Clinical reference catalogs can be derived or synthesized with assistance from large language models using clearly defined source constraints, followed by human expert review, approval, and versioned publication as shared benchmarks\n  - This approach does not constrain AI; it supports safer and more reproducible AI reasoning\n\n- Why This Enables AI Rather Than Restricting It\n  - AI and machine learning systems are highly flexible, but they will attempt to infer meaning when semantic gaps are not explicitly defined\n  - A deterministic semantic layer:\n    - Prevents implicit inference when data is incomplete or non-comparable\n    - Enables AI systems to explicitly represent and communicate uncertainty\n    - Allows machine learning models to operate on semantically stable, well-defined inputs\n  - Large language models can synthesize clinical context, documentation, and explanations\n  - Machine learning models can validate hypotheses, detect trends, and surface emerging patterns\n  - Both benefit from a shared, explicit semantic foundation that reduces ambiguity and improves reliability\n\n- Concrete Example: Semantic Equivalence in Longitudinal Laboratory Monitoring\n  - Consider a patient whose triglyceride values are reported over time by different laboratories:\n    - One using mg/dL\n    - Another using mmol/L\n    - With differing reference ranges and assay context\n  - Without explicit normalization rules and provenance:\n    - Longitudinal trends may be misinterpreted\n    - Clinical thresholds may be applied inconsistently\n    - AI-generated insights may appear contradictory\n  - With deterministic semantic validation:\n    - Values are converted using validated rules appropriate to the underlying measurand\n    - Reference ranges and interpretation context are applied consistently\n    - AI systems are explicitly aware of what is known, transformed, or missing\n  - In other laboratory contexts—such as measurements based on enzyme activity versus mass (e.g., CK-MB), biological potency (e.g., insulin, FSH), or immunoassay-based versus mass spectrometry-based results (e.g., testosterone)—differences in reported units reflect fundamentally different measurands or measurement bases\n    - Mathematical conversion is scientifically invalid in these cases\n    - Values should not be compared\n  - To preserve data integrity, non-comparable results should be represented using distinct LOINC codes or equivalent semantic identifiers\n    - Ensures automated systems do not normalize or compare incompatible measurement bases\n  - FHIR provides flexible mechanisms—such as Observation and Provenance resources—to capture structured measurements, attribution, and contextual metadata, while leaving transformation logic and semantic interpretation to implementers\n  - A governed, hybrid semantic validation layer upstream can complement these capabilities by stabilizing semantic interpretation and validated transformation logic at the point of reuse\n  - In practice, this enables data pipelines to apply method-specific semantic differentiation before longitudinal analysis occurs\n    - Improves precision and consistency over time\n    - Makes provenance and audit information more interpretable as data is reused at scale\n\n- Placement Within the Interoperability Stack\n  - This semantic validation layer does not require a single architectural placement\n  - Placement closer to data origination enables more precise semantic validation and higher downstream impact\n  - Later placement can still provide meaningful stabilization for longitudinal reuse\n  - Can be deployed independently or in combination:\n    - Within a clinical source system prior to external data exposure\n    - Immediately before FHIR-based API publication\n    - Downstream of FHIR consumption by applications or AI systems\n  - This flexibility allows stakeholders to evaluate where deterministic semantic stabilization is most effective without constraining architectural choice or prescribing a single deployment model\n\n- Applicability Beyond Laboratory Data\n  - While laboratory data and medications provide the most explicit semantic starting points, the same principles apply to:\n    - Clinical observations\n    - Device-generated data\n    - Longitudinal clinical assessments\n  - Each domain benefits from explicit semantic context as data is reused across systems and over time\n    - Particularly for AI-enabled decision support, longitudinal monitoring, and research\n  - In these domains, determinism is achieved through explicit semantic constraints and comparability rules rather than purely mathematical normalization\n\n- Relationship to Current Progress\n  - Significant progress has already been achieved:\n    - Health data is more portable than ever\n    - Patient access initiatives continue to expand\n    - Clinicians are increasingly supported by interoperable tools\n    - AI and machine learning systems are already helping patients better understand health information, navigate care decisions, and engage with the healthcare system, while also supporting analytical and clinical workflows\n  - The remaining challenge is not ambition, but orchestration—aligning data movement, semantic clarity, and AI reasoning into a coherent, auditable workflow that supports longitudinal use and reuse over time\n\n- Collaborative Evaluation Opportunity\n  - Participation in such an approach could be complementary to existing FHIR-based interoperability\n  - Coordinated, multi-stakeholder evaluation efforts could help assess:\n    - Where semantic validation is most effective across the data exchange pipeline\n    - How deterministic layers evolve over time\n    - How AI system reliability and longitudinal comparability improve when explicit semantic context is preserved\n  - HHS could consider providing guidance or serving as a coordinating convener for such efforts as a means of enabling shared learning without disrupting existing implementations\n  - HHS could play a constructive role in convening clinical and standards bodies to help establish governance for the clinical reference inputs used in deterministic semantic layers\n    - Supporting clinician-led review and sign-off on source materials, reference ranges, and transformation rules\n    - Encouraging the synthesis of shared benchmark references\n  - Once established, such clinically governed reference layers could be reused across the ecosystem\n    - Enabling consistency, transparency, and trust while preserving innovation and implementation flexibility\n\n- Conclusion\n  - As the healthcare ecosystem continues to evolve from isolated data exchange toward longitudinal, AI-assisted monitoring and discovery, semantic clarity becomes foundational infrastructure\n  - A hybrid deterministic semantic validation layer could provide a pragmatic, incremental approach to supporting safer AI reasoning, more reliable longitudinal analysis, and continued innovation—while complementing existing interoperability standards and preserving architectural flexibility\n\n---",
      "oneLineSummary": "Health tech founder proposes a deterministic semantic validation layer to ensure clinical data retains consistent meaning across systems and time, preventing AI \"hallucinations\" caused by missing semantic context rather than model failure.\n\n---",
      "commenterProfile": "- **Name/Organization:** Daniela Jelatancev, Founder & CEO, HealthFramework\n- **Type:** Business\n- **Role/Expertise:** Health technology entrepreneur focused on interoperability infrastructure\n- **Geographic Scope:** National\n- **Stake in Issue:** Directly involved in building healthcare data infrastructure; company likely positioned to benefit from or contribute to semantic validation solutions\n\n---",
      "corePosition": "While FHIR-based APIs have succeeded in enabling data movement, interoperability at the transport layer does not guarantee semantic equivalence at the interpretation layer. As AI systems increasingly consume healthcare data, we need a deterministic, hybrid human-AI semantic validation layer that stabilizes clinical meaning without constraining innovation. The remaining challenge is not ambition, but orchestration—aligning data movement, semantic clarity, and AI reasoning into a coherent, auditable workflow.\n\n---",
      "keyRecommendations": "- Explore a deterministic, hybrid human-AI semantic validation layer as complementary infrastructure to existing FHIR-based interoperability\n  - Should include deterministic processing for unit normalization, reference alignment, and semantic validation\n  - Must maintain explicit provenance and audit logs capturing transformations, assumptions, and unresolved gaps\n  - Should use hybrid design where AI assists but does not replace human oversight\n\n- Use distinct LOINC codes or equivalent semantic identifiers for non-comparable results\n  - Ensures automated systems do not normalize or compare incompatible measurement bases\n\n- Allow flexible deployment of the semantic validation layer at multiple points in the interoperability stack\n  - Within clinical source systems prior to external data exposure\n  - Immediately before FHIR-based API publication\n  - Downstream of FHIR consumption by applications or AI systems\n\n- HHS should consider serving as a coordinating convener for multi-stakeholder evaluation efforts\n  - Assess where semantic validation is most effective across the data exchange pipeline\n  - Enable shared learning without disrupting existing implementations\n\n- HHS should convene clinical and standards bodies to establish governance for clinical reference inputs\n  - Support clinician-led review and sign-off on source materials, reference ranges, and transformation rules\n  - Encourage synthesis of shared benchmark references that can be reused across the ecosystem\n\n---",
      "mainConcerns": "- Semantic interoperability gaps can affect AI reasoning, longitudinal analysis, and interpretation of trends across patient populations\n  - Differences in units, measurement context, specimen, method, or transformation rules create problems\n  - System-specific transformations (unit conversions, aggregation, normalization, omission of contextual attributes) may not be visible or interpretable downstream\n\n- Some clinical values cannot be meaningfully compared at all without explicit transformation rules\n  - Example: serum iron results from different assay methods and specimen matrices cannot be compared through unit conversion alone\n  - Measurements based on enzyme activity vs. mass, biological potency, or different assay technologies reflect fundamentally different measurands\n  - Longitudinal comparison in these cases is not merely imprecise—it is invalid\n\n- AI systems will attempt to infer meaning when semantic gaps are not explicitly defined\n  - Without explicit semantic context and deterministic rules, AI may infer continuity where none exists\n  - What's often described as \"hallucination\" is frequently caused by missing semantic ground truth rather than model failure\n\n- Small semantic differences compound over time in longitudinal monitoring\n  - Can prevent meaningful comparison, trend analysis, or early pattern detection\n  - Problems escalate when patients move between health systems, data spans years, or multiple AI models consume the same data\n\n---",
      "notableExperiences": "- Reframes AI \"hallucination\" as a data problem rather than a model problem\n  - When clinical data lacks explicit semantic definition, AI must rely on probabilistic inference\n  - The root cause is often missing semantic ground truth, not model failure\n  - This shifts responsibility from AI developers to data infrastructure\n\n- Proposes using AI to help build the very system that constrains AI\n  - Clinical reference catalogs can be synthesized with LLM assistance using defined source constraints\n  - Human expert review, approval, and versioned publication follow\n  - Creates a virtuous cycle where AI helps create its own guardrails\n\n- Identifies a specific class of comparison errors that unit conversion cannot fix\n  - CK-MB measured by enzyme activity vs. mass concentration\n  - Testosterone measured by immunoassay vs. mass spectrometry\n  - These aren't precision problems—they're category errors that make comparison scientifically invalid\n\n---",
      "keyQuotations": "- \"This behavior is often described as hallucination; however, in many cases the root cause is missing semantic ground truth rather than model failure.\"\n\n- \"The remaining challenge is not ambition, but orchestration—aligning data movement, semantic clarity, and AI reasoning into a coherent, auditable workflow that supports longitudinal use and reuse over time.\"\n\n- \"In such cases, longitudinal comparison is not merely imprecise—it is invalid.\""
    },
    "themeScores": {
      "5": 1,
      "5.1": 1,
      "5.4": 1,
      "5.5": 1,
      "5.6": 1,
      "5.7": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "Human-in-the-Loop"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "AI Hallucination"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Large Language Model"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Machine Learning"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Provenance"
      },
      {
        "category": "Health Information Systems",
        "label": "EHR"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "FHIR"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "LOINC"
      }
    ],
    "hasAttachments": true,
    "wordCount": 1671,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0035",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Brian Graham",
    "submitterType": "Individual",
    "date": "2026-02-05T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Introduction\n  - Commenter identifies as Brian Graham, also known as \"Cosmicraftsman\"\n  - Artist and creator from Perth, Western Australia\n  - Provides contact email for follow-up\n\n- ATARAXIAN CODE ENCRYPTION method and testing document\n  - Document overview and scope\n    - Comprehensive testing document for the \"ATARAXIAN Code\"\n    - Covers binary sequences for data transmission and conversion\n    - Includes error-correcting codes and data compression algorithms\n    - Contains equations for data encryption, decryption, and digital signatures\n    - Addresses engine operating parameters for network protocols, data storage, user authentication, and system performance optimization\n    - Outlines testing and implementation strategies\n\n- Binary Sequence specifications\n  - Primary binary sequence designated for data transmission and conversion labeled \"ATARAXIAN CODE\"\n  - Error-correcting codes examples\n    - Hamming code: `11010101`\n    - Reed-Solomon code: `11100111`\n  - Data compression algorithm examples\n    - Huffman code: `01101010`\n    - LZ77 code: `10101100`\n\n- Equations for encryption and signatures\n  - Data encryption and decryption algorithms\n    - AES (Advanced Encryption Standard): `E(P) = (P + K) mod 26`\n    - RSA (Rivest-Shamir-Adleman): `E(P) = P^e mod n`\n  - Digital signature algorithms\n    - SHA (Secure Hash Algorithm): `H(M) = 0x01234567`\n    - MD5 (Message-Digest Algorithm 5): `H(M) = 0x98765432`\n  - Note that these are simplified examples; actual implementations vary\n\n- Error Correction Codes detailed\n  - Hamming Code\n    - Detects and corrects single-bit errors\n    - Uses parity bits to identify errors\n    - Equation: `P = (D1 + D2 + D3) mod 2` for parity bit calculation\n    - Includes data bits to parity bits mapping table\n  - Reed-Solomon Code\n    - Detects and corrects multiple-bit errors\n    - Uses polynomial equations to identify errors\n    - Equation: `P(x) = (D1 + D2x + D3x^2 + ... + Dnx^(n-1)) mod (x - 1)`\n    - Includes data bytes to parity bytes mapping table\n\n- Data Compression Algorithms detailed\n  - Huffman Coding\n    - Assigns variable-length codes to symbols based on frequency\n    - Equation: `L = -log2(P)` for code length calculation\n    - Example table showing symbol frequency and resulting codes\n  - LZ77 Compression\n    - Finds repeated patterns in data and replaces with references\n    - Equation: `C = (L, D)` for compressed data calculation\n    - Example showing \"abcabcabc\" compressed to \"(3, abc)\"\n\n- Data Encryption and Decryption detailed\n  - AES (Advanced Encryption Standard)\n    - Encryption: `E(P) = (P + K) mod 26`\n    - Decryption: `D(E) = (E - K) mod 26`\n    - Flow: Plaintext → Encryption → Ciphertext\n  - RSA (Rivest-Shamir-Adleman)\n    - Encryption: `E(P) = P^e mod n`\n    - Decryption: `D(E) = E^d mod n`\n    - Flow: Plaintext → Encryption → Ciphertext\n\n- Digital Signatures detailed\n  - SHA (Secure Hash Algorithm)\n    - Hash function: `H(M) = 0x01234567`\n    - Digital signature: `S = H(M) + K`\n    - Flow: Message → Hash → Digital Signature\n  - RSA for signatures\n    - Digital signature: `S = M^d mod n`\n    - Verification: `V = S^e mod n`\n    - Flow: Message → Digital Signature → Verification\n  - Note that actual schemes use more complex equations and larger keys\n\n- Engine Operating Parameters\n  - Network Protocols\n    - TCP/IP (Transmission Control Protocol/Internet Protocol)\n      - Connection establishment: `SYN → SYN-ACK → ACK` (three-way handshake)\n      - Data transfer: `SEQ → ACK` (sequence number and acknowledgment)\n      - Connection termination: `FIN → ACK → FIN-ACK` (four-way shutdown)\n    - UDP (User Datagram Protocol)\n      - Connectionless protocol\n      - Data transfer: `SEQ → ACK`\n  - Data Storage\n    - Disk Storage\n      - Capacity: `C = (S × N) × 1024` (capacity in bytes)\n      - Access time: `T = (S × R) + L` (access time in seconds)\n    - Memory (RAM)\n      - Capacity: `C = (S × N) × 1024`\n      - Access time: `T = (S × R) + L`\n  - User Authentication\n    - Password Authentication\n      - Password hash: `H(P) = SHA(P)`\n      - Authentication: `A = (U, H(P))` (username and password hash)\n    - Token-Based Authentication\n      - Token generation: `T = (U, S, E)` (username, secret, expiration)\n      - Authentication: `A = (T, S)`\n  - System Performance Optimization\n    - Caching\n      - Cache hit ratio: `H = (C / T) × 100`\n      - Cache performance: `P = (H × S) / L`\n    - Buffering\n      - Buffer size: `B = (S × N) × 1024`\n      - Buffer performance: `P = (B × R) / L`\n\n- Testing and Implementation Strategies\n  - Theoretical Test scenarios\n    - Data Encryption and Decryption test\n      - Test data: \"Hello, World!\"\n      - Uses AES for both encryption and decryption\n      - Expected output: \"Hello, World!\"\n    - Digital Signature test\n      - Test data: \"Hello, World!\"\n      - Hash function: SHA-256\n      - Digital signature algorithm: RSA\n      - Expected output: Valid digital signature\n  - Code Review examples\n    - Encryption and Decryption Function\n      - Python-style pseudocode for encrypt() and decrypt() functions using AES\n    - Digital Signature Function\n      - Python-style pseudocode for digital_signature() and verify_signature() functions using SHA256 and RSA\n  - Implementation Strategies recommended\n    - Modular Programming: Break code into smaller, modular functions for easier testing and maintenance\n    - Test-Driven Development (TDD): Write test cases before implementing code\n    - Code Review: Regularly review code with peers to catch errors and improve quality\n\n---",
      "oneLineSummary": "An Australian artist submits an unrelated technical document about a self-created \"ATARAXIAN Code\" encryption system with no apparent connection to the federal regulation under consideration.\n\n---",
      "commenterProfile": "- **Name/Organization:** Brian Graham (aka \"Cosmicraftsman\")\n- **Type:** Individual\n- **Role/Expertise:** Self-described artist and creator; presents technical documentation on encryption concepts\n- **Geographic Scope:** International (Perth, Western Australia)\n- **Stake in Issue:** No discernible stake in the regulation; submission appears unrelated to the rulemaking\n\n---",
      "corePosition": "The commenter does not articulate any position on the regulation being considered. Instead, they submit a technical document describing a self-created \"ATARAXIAN Code\" encryption methodology covering standard cryptographic concepts like AES, RSA, error correction, and data compression. The submission appears to be off-topic and unrelated to the federal rulemaking.\n\n---",
      "keyRecommendations": "No specific recommendations provided regarding the regulation under consideration.\n\n---",
      "mainConcerns": "No specific concerns raised regarding the regulation under consideration.\n\n---",
      "notableExperiences": "No distinctive experiences shared relevant to the regulation. The submission consists of a compilation of standard cryptographic and networking concepts presented as an original \"ATARAXIAN Code\" system, but without any connection to the regulatory matter at hand.\n\n---",
      "keyQuotations": "No standout quotations relevant to the regulation."
    },
    "themeScores": {},
    "entities": [
      {
        "category": "Data Privacy and Security",
        "label": "Encryption"
      }
    ],
    "hasAttachments": true,
    "wordCount": 1407,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0036",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "David Rocha",
    "submitterType": "Individual",
    "date": "2026-02-05T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Question addressed: What are the biggest barriers to private sector innovation in AI for health care and its adoption and use in clinical care?\n- Medical societies and clinical societies collectively have not moved to standards-based FHIR APIs at scale\n  - This represents a major barrier to AI innovation in healthcare\n- Medical and clinical societies possess valuable clinical resources:\n  - Clinical guidelines\n  - Care plans\n  - Treatment algorithms\n  - Appropriate use criteria\n  - Care pathways\n  - Living guidelines\n  - Other recommendations useful for clinical care\n- Collective movement by medical/clinical societies to FHIR APIs would be beneficial\n  - Would enable AI to scale standards-based care\n- Potential application: AI agents for cross-specialty care in rural areas\n  - Described as a \"fun application\" of this approach",
      "oneLineSummary": "Healthcare observer identifies medical societies' failure to adopt FHIR API standards as the key barrier to AI innovation, envisioning AI agents enabling cross-specialty rural care.",
      "commenterProfile": "- **Name/Organization:** David Rocha\n- **Type:** Individual\n- **Role/Expertise:** Knowledgeable about healthcare interoperability standards (FHIR APIs) and clinical society operations\n- **Geographic Scope:** National\n- **Stake in Issue:** Interest in advancing AI adoption in healthcare, particularly for underserved rural areas",
      "corePosition": "The biggest barrier to AI innovation in healthcare is that medical and clinical societies haven't collectively adopted standards-based FHIR APIs at scale. These societies hold valuable clinical knowledge that could power AI applications if made accessible through standardized interfaces.",
      "keyRecommendations": "- Medical societies and clinical societies should collectively move to standards-based FHIR APIs at scale\n  - Would make clinical guidelines, care plans, treatment algorithms, and other recommendations accessible for AI applications\n  - Would enable AI to deliver standards-based care at scale",
      "mainConcerns": "- Medical and clinical societies have not adopted FHIR APIs at scale\n  - This prevents AI from accessing valuable clinical knowledge resources\n  - Limits the ability to scale standards-based care through AI",
      "notableExperiences": "- Identifies an often-overlooked barrier: the problem isn't just EHR vendors or health systems, but professional medical societies themselves not making their clinical knowledge machine-readable\n- Proposes a specific use case: AI agents could enable cross-specialty care in rural areas where specialist access is limited",
      "keyQuotations": "- \"A fun application of this would be AI agents for cross specialty care in rural areas.\""
    },
    "themeScores": {
      "5": 1,
      "5.9": 1
    },
    "entities": [
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Agentic AI"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Treatment"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "FHIR"
      }
    ],
    "hasAttachments": false,
    "wordCount": 124,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0037",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Radiology Business Management Association",
    "submitterType": "Organization",
    "date": "2026-02-05T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Radiology Business Management Association (RBMA) response to Request for Information on Accelerating the Adoption and Use of Artificial Intelligence\n- RBMA background\n  - Established in 1968\n  - Over 2000 radiology practice business leaders\n  - Represents over 800 radiology practices in all 50 states\n  - Includes diagnostic radiology, interventional radiology, nuclear medicine, Independent Diagnostic Testing Facilities (IDTFs), and radiation oncology\n  - Represents the \"business side of radiology\"\n  - Responses focus primarily on financial, operational, and administrative implications of AI adoption\n  - Defers to physician organizations on detailed clinical benefits and barriers\n- Question 1: Biggest barriers to private sector innovation for AI in healthcare\n  - Financial barriers\n    - Organizations struggle to determine how to pay for AI\n    - Uncertainty about how reimbursement will evolve\n    - Difficulty building viable financial models while AI use cases remain fluid\n  - Operational barriers\n    - Integrating AI into established workflows can be complex\n    - Workforce readiness and willingness to adopt AI tools are still uncertain\n  - Contractual barriers\n    - Key liability questions remain unresolved\n      - Who is responsible when AI makes an error?\n      - How will malpractice insurers treat AI-assisted care?\n    - Concerns regarding data ownership\n    - Vendor data governance concerns\n    - Whether vendors intend to monetize clinical data\n- Question 2: Regulatory, payment policy, or programmatic design changes HHS should prioritize\n  - HHS should not assume AI will immediately create efficiencies\n    - Early implementation often requires increased workflow steps, staff time, and capital investment\n  - Recommended approaches that do not reduce base reimbursement\n    - Support clinicians and providers with funding mechanisms for procurement, implementation, and ongoing operationalization\n    - Create and enforce regulations clarifying ownership of data, liability standards, and malpractice coverage expectations\n    - Establish a federal certification framework verifying AI vendor compliance with, and ideally exceeding, HIPAA and cybersecurity standards\n    - Ensure AI tools and technologies are exempt from tariffs where applicable\n      - Reduces financial burden and promotes access\n  - Clinicians remain unsure how to model long-term costs and reimbursement of AI\n  - HHS policies that reduce cost barriers will significantly improve adoption\n- Question 3: Novel legal and implementation issues for non-medical devices\n  - Significant legal uncertainties exist affecting both medical and non-medical AI tools\n  - Unresolved questions include\n    - HIPAA and broader privacy compliance\n    - Malpractice and legal liability\n    - Data ownership, governance, and vendor rights to reuse or monetize data\n  - HHS can help by establishing clearer guardrails, reducing ambiguity, and creating trusted standards for vendors\n- Question 4: Most promising AI evaluation methods for non-medical devices\n  - Recommended evaluation methods promoting transparency, robustness, and workflow safety\n    - Pre-deployment validation using diverse, representative datasets\n    - Post-deployment monitoring for model drift, error rates, and performance across demographic groups\n    - Human-centered workflow assessment to evaluate real-world impact on clinical staff and operations\n  - HHS could accelerate progress through\n    - Grants\n    - Cooperative agreements\n    - Vendor certification competitions rewarding reliability, equity, and interoperability\n- Question 5: How HHS can support private-sector activities to promote AI use\n  - Establish baseline national standards for AI vendor transparency, cybersecurity, data governance, and model training disclosures\n  - Partner with private-sector accreditation bodies to create aligned, non-duplicative certification pathways\n  - Encourage industry-driven benchmarking tools incorporating privacy, equity, and workflow-impact criteria\n  - Such efforts would reduce provider burden and strengthen trust in AI tools\n- Question 6: Where AI has met or fallen short of expectations\n  - RBMA's expertise centers on business and operational aspects\n  - Ongoing concerns regarding bias in AI training datasets\n    - AI models learn from the data they ingest\n    - Many tools may not be validated against sufficiently diverse patient populations\n  - Examples of bias concerns\n    - AI in mammography\n      - Were models trained on women across all breast density categories, varied risk profiles, and diverse racial and ethnic backgrounds?\n    - AI in pneumonia detection\n      - Were tools trained solely on acutely ill hospitalized patients, or do they include routine outpatient imaging?\n  - HHS could support providers by creating federal certification verifying AI models were trained and validated on comprehensive, demographically diverse datasets\n- Question 7: Decision makers driving AI adoption and administrative hurdles\n  - Key decision makers\n    - Executive leaders (CFO, CIO, CMO)\n    - Radiology administrators\n    - IT departments\n    - Compliance officers\n    - Clinical governance committees\n  - Primary hurdles\n    - Limited capital budgets\n    - Variable reimbursement and uncertain ROI\n    - Integration challenges with existing systems\n    - Legal and compliance ambiguity\n    - Staff acceptance and training needs\n- Question 8: Where enhanced interoperability would widen AI market opportunities\n  - Enhanced interoperability around imaging data formats, structured reporting, and standardized benchmarking data could dramatically advance AI\n  - Benefits would include\n    - Easier dataset aggregation for model training\n    - Seamless integration with PACS/RIS/EHR systems\n    - More reliable cross-vendor comparisons\n    - Faster multi-site validation studies\n  - Essential standards include DICOM, HL7/FHIR, and imaging-specific structured reporting frameworks\n- Question 9: Challenges patients and caregivers want AI to address\n  - Patient interests in AI tools\n    - Improved access\n    - Improved accuracy\n    - Improved timeliness\n    - Better care coordination\n  - Patient concerns\n    - Data privacy and security\n    - Equity and bias in AI decision-making\n    - Whether clinicians remain \"in the loop\"\n    - Transparency around how AI is used in their care\n  - Clear communication from providers supported by standardized HHS guidance will be essential for patient trust\n- Question 10: AI research areas HHS should prioritize\n  - Published findings\n    - Growing research on AI's impact on clinical workflows, diagnostic accuracy, and efficiency\n    - Much literature is early-stage, limited to specific populations, or lacks large-scale deployment data\n  - Costs and benefits\n    - Current literature examines cost-effectiveness unevenly\n    - Many studies acknowledge potential financial benefits but do not quantify them with sufficient real-world evidence\n    - Additional federally supported economic studies would help providers model total cost of ownership, reimbursement pathways, and long-term ROI\n- Summary: The core reimbursement dilemma\n  - AI holds substantial promise to improve quality, consistency, and safety of radiologists' interpretations\n    - Can flag subtle findings\n    - Reduce perceptual errors\n    - Prioritize urgent cases\n    - Support clinical decision-making\n  - Central challenge: current Medicare and commercial payer reimbursement systems are grounded in fee-for-service models\n    - Reward physician time, malpractice risk, and practice expenses\n    - Do not reward measurable improvements in quality or outcomes\n  - Paradox: if AI improves efficiency or reduces interpretive effort, it may threaten reimbursement\n    - Discourages adoption even when patient care improves\n  - Under existing payment structures\n    - Radiologist work RVUs are based on assumptions about time and cognitive effort\n    - AI-assisted workflows that shorten reading times could be interpreted as lowering physician work\n    - Radiologist retains legal responsibility and malpractice risk regardless\n    - Practice costs may increase due to AI licensing, integration, validation, cybersecurity, and compliance requirements\n    - These expenses are not well reflected in current reimbursement formulas\n  - This creates a policy dilemma: innovation that improves quality and safety may be financially penalized\n- Recommended policy strategies to address the dilemma\n  - Reimbursement models should evolve to recognize quality-enhancing clinical AI as physician work augmentation rather than substitution\n    - Medicare could create add-on payments or new RVU components tied to validated AI use\n    - Should demonstrably improve diagnostic accuracy, reduce downstream costs, or enhance patient outcomes\n    - Add-on payments should be permanent, not time-limited\n  - Broader adoption of value-based payment models\n    - Bundled payments would allow providers to capture financial benefits of higher-quality, more efficient care enabled by AI\n  - Support mechanisms to account for persistent malpractice risk\n    - Even when AI assists interpretation, radiologists remain legally accountable\n    - Payment policies should reflect that AI does not eliminate professional liability\n    - May increase perceived risk during early adoption\n  - Federal investment in evidence generation, standards, and regulatory clarity\n    - Can reduce uncertainty and lower adoption costs\n    - Makes it easier for payers to reimburse AI-enabled care appropriately\n  - Without payment reform, AI risks becoming a victim of its own success\n  - Aligning reimbursement with quality, accountability, and long-term value is essential to realizing AI's potential without undermining the economic foundations of medical practice",
      "oneLineSummary": "National radiology business association warns that fee-for-service reimbursement creates a \"victim of its own success\" paradox where AI that improves care may be financially penalized, and urges permanent add-on payments, liability clarity, and federal certification standards.",
      "commenterProfile": "- **Name/Organization:** Radiology Business Management Association (RBMA)\n- **Type:** Trade Association\n- **Role/Expertise:** Professional association of over 2,000 radiology practice business leaders representing 800+ practices across all 50 states; expertise in financial, operational, and administrative aspects of radiology\n- **Geographic Scope:** National (all 50 states)\n- **Stake in Issue:** Members must make purchasing decisions on AI tools, navigate uncertain reimbursement, manage integration into workflows, and bear liability risks while current payment models may penalize efficiency gains",
      "corePosition": "We support AI adoption in radiology but warn that current fee-for-service reimbursement creates a fundamental misalignment: AI that improves quality and efficiency may paradoxically reduce payments, discouraging adoption even when patient care improves. HHS must reform payment models to recognize AI as work augmentation rather than substitution, establish clear liability and data governance standards, and create federal certification frameworks—otherwise AI risks becoming \"a victim of its own success.\"",
      "keyRecommendations": "- Reform reimbursement to recognize AI as physician work augmentation\n  - Create permanent (not time-limited) add-on payments or new RVU components tied to validated AI use\n  - Tie payments to demonstrable improvements in diagnostic accuracy, reduced downstream costs, or enhanced outcomes\n- Expand value-based payment models\n  - Bundled payments would let providers capture financial benefits of AI-enabled efficiency\n- Establish federal certification frameworks\n  - Verify AI vendor compliance with HIPAA and cybersecurity standards\n  - Certify that AI models were trained on comprehensive, demographically diverse datasets\n- Clarify liability and data governance\n  - Create regulations clarifying data ownership, liability standards, and malpractice coverage expectations\n  - Payment policies should reflect that radiologists retain legal accountability even with AI assistance\n- Provide funding support without reducing base reimbursement\n  - Fund procurement, implementation, and ongoing operationalization\n  - Exempt AI tools from tariffs where applicable\n- Support evaluation and evidence generation\n  - Fund pre-deployment validation using diverse datasets\n  - Support post-deployment monitoring for model drift and demographic performance\n  - Conduct federally supported economic studies on total cost of ownership and ROI\n- Enhance interoperability standards\n  - Prioritize DICOM, HL7/FHIR, and imaging-specific structured reporting frameworks\n  - Enable easier dataset aggregation, cross-vendor comparisons, and multi-site validation\n- Partner with private sector on certification\n  - Create aligned, non-duplicative certification pathways with accreditation bodies\n  - Encourage industry-driven benchmarking incorporating privacy, equity, and workflow-impact criteria",
      "mainConcerns": "- Fee-for-service payment paradox\n  - AI that improves efficiency may be interpreted as lowering physician work, reducing RVUs\n  - Radiologists retain full legal responsibility and malpractice risk regardless of AI assistance\n  - Practice costs increase (licensing, integration, validation, cybersecurity, compliance) but aren't reflected in reimbursement\n  - Innovation that improves quality may be financially penalized\n- Unresolved liability questions\n  - Who is responsible when AI makes an error?\n  - How will malpractice insurers treat AI-assisted care?\n  - AI may increase perceived risk during early adoption\n- Data governance uncertainties\n  - Data ownership questions unresolved\n  - Vendor data governance practices unclear\n  - Concerns about vendors monetizing clinical data\n- AI bias and validation gaps\n  - Many tools not validated against sufficiently diverse patient populations\n  - Mammography AI may not cover all breast density categories, risk profiles, and racial/ethnic backgrounds\n  - Pneumonia detection AI may be trained only on hospitalized patients, not outpatient imaging\n- Operational and financial barriers\n  - Early implementation requires increased workflow steps, staff time, and capital investment\n  - Limited capital budgets\n  - Variable reimbursement and uncertain ROI\n  - Integration challenges with existing systems\n  - Staff acceptance and training needs\n- Patient trust concerns\n  - Data privacy and security\n  - Equity and bias in AI decision-making\n  - Whether clinicians remain \"in the loop\"\n  - Transparency about how AI is used in care",
      "notableExperiences": "- Identified the \"victim of its own success\" paradox: AI tools that successfully improve efficiency and reduce reading times could trigger reimbursement cuts under current RVU methodology, creating a perverse incentive against adoption\n- Highlighted that early AI implementation often increases rather than decreases workload—contrary to efficiency assumptions that might drive policy\n- Pointed out specific bias validation gaps: mammography AI may not account for breast density variations, risk profiles, and demographic diversity; pneumonia detection AI may be trained only on hospitalized patients rather than outpatient populations\n- Noted that radiologists retain full malpractice liability even when AI assists, yet payment models may treat AI assistance as reducing physician work\n- Emphasized that practice expenses for AI (licensing, integration, validation, cybersecurity, compliance) are not captured in current reimbursement formulas",
      "keyQuotations": "- \"If AI improves efficiency or reduces interpretive effort, it may paradoxically threaten reimbursement, discouraging adoption even when patient care improves.\"\n- \"Without payment reform, AI risks becoming a victim of its own success.\"\n- \"AI-assisted workflows that shorten reading times or reduce uncertainty could be interpreted as lowering physician work, even though the radiologist retains legal responsibility and malpractice risk.\"\n- \"HHS should not assume AI will immediately create efficiencies. Early implementation often requires increased workflow steps, staff time, and capital investment.\""
    },
    "themeScores": {
      "2": 1,
      "3": 1,
      "4": 1,
      "5": 1,
      "6": 1,
      "7": 1,
      "8": 1,
      "9": 1,
      "3.1": 1,
      "4.1": 1,
      "4.2": 1,
      "4.3": 1,
      "6.1": 1,
      "7.1": 1,
      "7.4": 1,
      "8.1": 1,
      "8.5": 1,
      "9.1": 1
    },
    "entities": [
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Algorithmic Bias"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Model Drift"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Accreditation"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Certification"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Care Coordination"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Diagnosis"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Interventional Radiology"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Nuclear Medicine"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Oncology"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Radiation Oncology"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Radiology"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Cybersecurity"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Governance"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Health Information Systems",
        "label": "EHR"
      },
      {
        "category": "Health Information Systems",
        "label": "PACS"
      },
      {
        "category": "Health Information Systems",
        "label": "RIS"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Procurement"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "ROI"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "DICOM"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "FHIR"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "HL7"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Equity"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Ambulatory Care"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "IDTF"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "CFO"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "CIO"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "CMO"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Physician"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Radiologist"
      },
      {
        "category": "Laws and Regulations",
        "label": "HIPAA"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Malpractice"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Cancer"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Trust"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "Fee-for-Service"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "RVU"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "Value-Based Care"
      },
      {
        "category": "Professional Organizations",
        "label": "RBMA"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "RWE"
      }
    ],
    "hasAttachments": true,
    "wordCount": 1711,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0038",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "ShiftOS",
    "submitterType": "Organization",
    "date": "2026-02-05T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Introduction\n  - ShiftOS is a healthcare technology company developing AI-powered workforce operations software for healthcare organizations\n  - Our product, Holly, is an AI scheduling agent that automates complex, time-intensive work of managing clinical staff schedules\n    - Shift assignments, credential compliance, call-offs, shift swaps, and float pool optimization\n  - CEO background informs our approach\n    - Served as Hospital Corpsman in U.S. Navy\n    - Worked at National Cancer Institute on Cancer Moonshot initiative under NCI Associate Director\n    - Led data delivery operations at IBM Watson Health working with MarketScan claims data\n  - Co-founder spent nearly six years as pharmacy technician at CVS while completing school\n    - Witnessed firsthand the administrative burden that drives clinician burnout\n  - Currently deploying Holly with enterprise health systems\n  - Established integration partnerships with major workforce management platforms including Workday\n  - Our perspective on AI adoption barriers comes from direct experience navigating procurement, IT integration, and clinical operations stakeholder alignment at large healthcare organizations\n\n- Question 1: Biggest barriers to private sector innovation in AI for healthcare\n  - The barriers we encounter most frequently are not technical—they are structural and procedural\n  - Fragmented procurement and IT governance\n    - Healthcare organizations often lack clear pathways for evaluating and adopting operational AI tools\n    - Clinical AI (diagnostic, imaging) has established FDA frameworks and clinical validation processes\n    - Operational AI (scheduling, staffing, administrative automation) falls outside these frameworks but still faces lengthy procurement cycles\n      - Often 12-18 months because organizations default to enterprise IT evaluation processes designed for EHRs and clinical systems\n    - No proportionate, streamlined pathway for lower-risk operational AI\n  - Integration complexity with legacy workforce systems\n    - Health systems operate fragmented technology stacks\n      - One system for scheduling, another for time and attendance, another for credentialing, another for payroll\n    - These systems rarely interoperate well\n    - AI tools that could automate scheduling decisions require read/write access to multiple systems simultaneously\n      - Scheduling, credential databases, union rules engines, payroll\n      - Each integration requires separate security reviews, BAAs, and technical work\n    - Absence of standard APIs or interoperability requirements for workforce management systems creates friction that delays deployment by months\n  - Credentialing and compliance data silos\n    - Real-time scheduling decisions require real-time visibility into staff credentials, licenses, certifications, and competencies\n    - This data is typically locked in credentialing software that does not expose it programmatically\n    - Forces manual verification or batch exports\n    - Limits AI's ability to make compliant staffing decisions autonomously\n  - Risk aversion without clear accountability frameworks\n    - Healthcare leaders are uncertain about liability when AI systems make or recommend staffing decisions\n    - Who is accountable if an AI schedules an under-credentialed provider?\n    - Lack of clear guidance from HHS on accountability for non-medical-device AI creates hesitation among compliance officers and legal teams\n    - Slows adoption even when operational leaders are enthusiastic\n\n- Question 2: Regulatory, payment policy, or programmatic design changes HHS should prioritize\n  - Extend interoperability mandates to workforce management systems\n    - 21st Century Cures Act and ONC's information blocking rules have improved data liquidity for clinical information\n    - Similar interoperability requirements should apply to workforce management, credentialing, and time/attendance systems\n    - Health systems should be able to access their own workforce data (schedules, credentials, availability) through standardized APIs without vendor lock-in or prohibitive integration fees\n    - Recommendation: HHS should consider whether workforce management data constitutes a category of information that, when siloed, impedes effective delivery of care\n      - Evaluate extending information blocking provisions or publishing voluntary interoperability standards for this data category\n  - Clarify accountability frameworks for operational AI\n    - HHS should publish guidance distinguishing operational AI (scheduling, staffing, administrative automation) from clinical decision support and medical devices\n    - Guidance should address liability allocation, documentation requirements, and human oversight expectations for AI systems that make non-clinical operational decisions\n    - This clarity would reduce legal uncertainty that currently delays procurement\n    - Allow health systems to adopt operational AI with appropriate, but not excessive, governance\n  - Recognize administrative burden reduction as a quality and safety priority\n    - CMS quality programs and value-based payment models do not currently recognize administrative efficiency as a measured outcome\n    - Yet administrative burden is a primary driver of clinician burnout, which directly impacts care quality, safety, and workforce retention\n    - Recommendation: HHS should explore whether workforce stability metrics (turnover, vacancy rates, time-to-fill) and administrative burden measures could be incorporated into quality reporting or value-based payment adjustments\n      - Would create financial incentives for health systems to invest in operational AI that reduces burden\n\n- Question 7: Key decision-makers and administrative hurdles for AI adoption\n  - Key decision-makers for operational AI adoption\n    - Chief Nursing Officers (CNOs) and VP/Directors of Nursing Operations\n      - Feel the pain of scheduling complexity most acutely and are often initial champions\n    - VP of Operations or Chief Operating Officer\n      - Controls budget authority and operational priorities\n    - IT Security and Compliance\n      - Gatekeepers who evaluate data handling, integration risk, and vendor security posture\n    - HRIS/Workforce Management System Owners\n      - Often skeptical of tools that interact with \"their\" systems\n      - Can slow or block integration approvals\n  - Primary administrative hurdles\n    - No single owner for operational AI\n      - Unlike clinical AI (which falls under CMO/CMIO purview), operational AI spans nursing, HR, IT, and operations\n      - Creates confusion about who evaluates, approves, and owns these tools\n    - Procurement processes designed for large capital expenditures\n      - AI tools often have SaaS pricing models with lower upfront costs but recurring fees\n      - Procurement teams trained on capital equipment evaluation struggle to assess these models, leading to delays\n    - Security review bottlenecks\n      - Enterprise security teams are overwhelmed\n      - Vendor security reviews can take 3-6 months even for low-risk operational tools\n    - Change management resistance\n      - Unit managers and schedulers who have \"always done it this way\" can resist AI tools that change their workflows\n      - Even if the tools reduce their burden\n      - Successful adoption requires executive sponsorship and frontline engagement simultaneously\n\n- Question 8: Where enhanced interoperability would widen market opportunities\n  - Workforce and operational data interoperability is critically underserved\n  - Data types that would significantly accelerate AI adoption if made more interoperable\n    - Staff schedules\n      - Current state: Locked in proprietary scheduling systems (Kronos/UKG, API Healthcare, ShiftWizard) with limited API access\n      - Impact: AI could optimize schedules, predict gaps, automate assignments\n    - Credentials and licenses\n      - Current state: Siloed in credentialing software (Symplr, MD-Staff, CredentialStream)\n      - Impact: Real-time credential verification for compliant autonomous scheduling\n    - Time and attendance\n      - Current state: Fragmented across payroll and HRIS systems (ADP, Paycom, Workday)\n      - Impact: Closed-loop scheduling → time tracking → payroll automation\n    - Float pool/PRN availability\n      - Current state: Often managed in spreadsheets or standalone systems\n      - Impact: AI could match available staff to open shifts across facilities\n    - Union and contract rules\n      - Current state: Typically hardcoded or manually enforced\n      - Impact: AI could enforce complex work rules automatically\n  - Recommendation: HHS should convene stakeholders (health systems, workforce technology vendors, AI developers) to develop voluntary interoperability standards or common data model for workforce operations data\n    - Similar to USCDI for clinical data\n\n- Question 9: Challenges patients and caregivers wish to see addressed by AI\n  - For patients\n    - Staff continuity\n      - Patients, particularly in post-acute, long-term care, and oncology settings, really value seeing consistent caregivers\n      - AI-optimized scheduling can prioritize continuity assignments\n    - Reduced wait times\n      - Understaffing due to last-minute call-offs creates delays\n      - AI that rapidly fills gaps improves patient access\n    - Safety\n      - Fatigued, burned-out staff make more errors\n      - Reducing administrative burden and optimizing schedules for appropriate rest improves safety\n  - For caregivers (clinical staff)\n    - Schedule predictability and fairness\n      - Manual scheduling often feels arbitrary or biased\n      - AI can apply rules consistently\n    - Reduced last-minute changes\n      - AI can anticipate gaps and fill them proactively rather than calling staff at the last minute\n    - Less time on administrative tasks\n      - Nurses and managers spend hours on scheduling, swap approvals, and credential checks\n      - Automating this returns time to patient care\n  - Concern to address\n    - Caregivers express concern about AI making decisions that affect their work lives without transparency\n    - Any workforce AI must be explainable\n    - Staff should understand why they were assigned a particular shift\n    - Must preserve human override capability\n\n- Summary Recommendations\n  - Extend interoperability policy attention to workforce management systems to unlock the data AI needs to optimize clinical operations\n  - Publish clear guidance distinguishing operational AI from clinical AI and medical devices, addressing accountability, documentation, and oversight expectations proportionate to risk\n  - Recognize administrative burden reduction and workforce stability as quality-adjacent outcomes that merit measurement and incentive alignment in CMS programs\n  - Convene a public-private working group on workforce data standards to accelerate voluntary interoperability for scheduling, credentialing, and time/attendance systems\n\n- Conclusion\n  - AI's greatest near-term impact in healthcare may not be in diagnosis or imaging—it may be in relieving the operational and administrative burden that drives clinician burnout and workforce shortages\n  - The barriers to adoption are not primarily technical; they are structural, procedural, and policy-driven\n  - HHS has an opportunity to accelerate responsible adoption of operational AI by clarifying accountability, extending interoperability principles to workforce data, and recognizing administrative efficiency as a component of care quality\n  - ShiftOS welcomes further engagement with HHS on these issues",
      "oneLineSummary": "AI workforce scheduling company with Navy medic and IBM Watson Health leadership argues that operational AI faces structural barriers—not technical ones—and urges HHS to extend interoperability mandates to workforce systems, clarify accountability for non-clinical AI, and recognize administrative burden reduction as a quality metric.",
      "commenterProfile": "- **Name/Organization:** ShiftOS, Inc. (Autumn-Kyoko Cushman, Co-Founder & CEO)\n- **Type:** Business\n- **Role/Expertise:** Healthcare AI workforce scheduling company; CEO has background as Navy Hospital Corpsman, NCI Cancer Moonshot initiative, and IBM Watson Health data operations; co-founder has pharmacy technician experience\n- **Geographic Scope:** National (deploying with enterprise health systems, integrated with Workday)\n- **Stake in Issue:** Directly affected by procurement barriers, interoperability gaps in workforce systems, and unclear accountability frameworks that delay adoption of their AI scheduling product",
      "corePosition": "We believe AI's greatest near-term healthcare impact may be in relieving operational and administrative burden that drives clinician burnout—not in diagnosis or imaging. The barriers to adoption are not primarily technical; they are structural, procedural, and policy-driven. HHS should extend interoperability principles to workforce data, clarify accountability for operational AI, and recognize administrative efficiency as a component of care quality.",
      "keyRecommendations": "- Extend interoperability mandates to workforce management systems\n  - Apply similar requirements as 21st Century Cures Act to scheduling, credentialing, and time/attendance systems\n  - Evaluate whether workforce management data constitutes information that, when siloed, impedes effective care delivery\n  - Consider extending information blocking provisions or publishing voluntary interoperability standards\n- Publish clear guidance distinguishing operational AI from clinical AI and medical devices\n  - Address liability allocation, documentation requirements, and human oversight expectations\n  - Make governance proportionate to risk level\n- Recognize administrative burden reduction and workforce stability as quality-adjacent outcomes\n  - Incorporate workforce stability metrics (turnover, vacancy rates, time-to-fill) into quality reporting\n  - Consider value-based payment adjustments to incentivize operational AI investment\n- Convene a public-private working group on workforce data standards\n  - Bring together health systems, workforce technology vendors, and AI developers\n  - Develop voluntary interoperability standards or common data model similar to USCDI for clinical data",
      "mainConcerns": "- Procurement cycles for operational AI are disproportionately long (12-18 months)\n  - Organizations default to enterprise IT evaluation processes designed for EHRs\n  - No streamlined pathway exists for lower-risk operational AI\n- Workforce systems lack interoperability\n  - Health systems operate fragmented technology stacks across scheduling, time/attendance, credentialing, and payroll\n  - Each integration requires separate security reviews, BAAs, and technical work\n  - Absence of standard APIs delays deployment by months\n- Credentialing data is siloed\n  - Real-time credential visibility is locked in software that doesn't expose data programmatically\n  - Forces manual verification or batch exports\n  - Limits AI's ability to make compliant staffing decisions autonomously\n- No clear accountability framework for operational AI\n  - Healthcare leaders uncertain about liability when AI makes staffing decisions\n  - Compliance officers and legal teams hesitate without HHS guidance\n- No single organizational owner for operational AI\n  - Spans nursing, HR, IT, and operations unlike clinical AI under CMO/CMIO\n  - Creates confusion about evaluation, approval, and ownership\n- Security review bottlenecks\n  - Enterprise security teams overwhelmed\n  - Reviews take 3-6 months even for low-risk operational tools\n- Caregiver transparency concerns\n  - Staff worry about AI making decisions affecting their work lives without explanation\n  - Workforce AI must be explainable with human override capability",
      "notableExperiences": "- Identified a regulatory gap: operational AI (scheduling, staffing, administrative automation) falls outside FDA frameworks but still faces clinical-system-level procurement scrutiny with no proportionate pathway\n- Mapped the specific data silos blocking workforce AI: staff schedules locked in Kronos/UKG/API Healthcare/ShiftWizard, credentials in Symplr/MD-Staff/CredentialStream, time/attendance in ADP/Paycom/Workday, float pool availability often in spreadsheets, union rules typically hardcoded\n- Observed that HRIS/Workforce Management System Owners are often skeptical of tools that interact with \"their\" systems and can slow or block integration approvals\n- Co-founder's six years as CVS pharmacy technician directly informed company's focus on administrative burden driving burnout\n- Noted that procurement teams trained on capital equipment evaluation struggle with SaaS pricing models, creating delays",
      "keyQuotations": "- \"AI's greatest near-term impact in healthcare may not be in diagnosis or imaging — it may be in relieving the operational and administrative burden that drives clinician burnout and workforce shortages.\"\n- \"The barriers we encounter most frequently are not technical — they are structural and procedural.\"\n- \"Who is accountable if an AI schedules an under-credentialed provider? The lack of clear guidance from HHS on accountability for non-medical-device AI creates hesitation among compliance officers and legal teams, slowing adoption even when operational leaders are enthusiastic.\""
    },
    "themeScores": {
      "1": 1,
      "2": 1,
      "3": 1,
      "4": 1,
      "5": 1,
      "6": 1,
      "1.1": 1,
      "1.2": 1,
      "2.1": 1,
      "2.3": 1,
      "4.2": 1,
      "6.1": 1,
      "6.5": 1,
      "6.7": 1,
      "6.8": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "Human-in-the-Loop"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Diagnosis"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Oncology"
      },
      {
        "category": "Data Privacy and Security",
        "label": "BAA"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Information Blocking"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "FDA"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "NCI"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "ONC"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Health Information Systems",
        "label": "CDS"
      },
      {
        "category": "Health Information Systems",
        "label": "EHR"
      },
      {
        "category": "Health Information Systems",
        "label": "HRIS"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Claims Processing"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Credentialing"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Quality Reporting"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Administrative Burden"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Procurement"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "USCDI"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Burnout"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Health System"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Long-Term Care"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "CMIO"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "CMO"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Nurse"
      },
      {
        "category": "Laws and Regulations",
        "label": "21st Century Cures Act"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Cancer"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "Cancer Moonshot"
      },
      {
        "category": "Social Determinants of Health",
        "label": "Underserved Population"
      }
    ],
    "hasAttachments": true,
    "wordCount": 1764,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0039",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "BlueHalo, LLC, an AV Company",
    "submitterType": "Organization",
    "date": "2026-02-05T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Executive Summary\n  - AI is increasingly embedded in clinical care workflows, research environments, and health-adjacent operations\n  - Organizations seek to improve efficiency, quality, and insight\n  - Adoption remains uneven and constrained by systemic challenges related to governance, evaluation, and trust\n  - We see limits in AI's ability to scale safely, equitably, and sustainably in clinical and research contexts\n  - This response asserts that accelerating AI adoption requires a shift in emphasis\n    - Not from developing additional AI models\n    - But from establishing shared infrastructure, governance mechanisms, and evaluation frameworks that enable trust\n  - Capabilities related to data readiness, interoperability, lifecycle monitoring, and auditability are foundational\n    - These safeguard patients, clinicians, and institutions\n    - Particularly important for non-medical device AI embedded in workflows, documentation, decision support, data management, and operational tools\n  - Drawing on experience supporting governed, secure data and AI infrastructure in regulated health, research, and operational environments\n  - Recommendations emphasize actions across regulation, research, reimbursement, and development to standardize trust infrastructure and reduce uncertainty\n\n- About the Commenter\n  - Health and performance technology division of a large defense technology company\n  - Experience supporting complex, regulated environments through design and operation of secure data and AI infrastructure\n  - Work covers health, research, public-sector, and care-adjacent domains where interoperability, governance, and accountability are critical\n  - Emphasizes infrastructure and enablement as primary drivers of responsible AI adoption at scale\n  - Includes facilitating evaluation, monitoring, and oversight of AI systems\n  - Supports multi-institution and multi-stakeholder environments\n  - Translates research capabilities into shared operational use\n\n- Barriers to AI Adoption in Clinical Care\n  - Despite significant advances in AI capabilities, adoption remains constrained by interrelated, non-technical barriers\n  - Challenges are largely structural and systemic, cutting across institutions, vendors, and care settings\n  - Addressing them requires attention to the broader environment in which AI is evaluated, deployed, and governed\n  \n  - Fragmented Evaluation Environments\n    - AI tools often evaluated within vendor-controlled or site-specific environments\n    - Limits independent assessment and comparison\n    - Health systems and regulators frequently lack ability to assess AI performance across multiple tools, populations, or deployment contexts\n    - Fragmentation impedes informed decision-making, reinforces vendor lock-in\n    - Constrains ability to establish shared expectations for performance, safety, and value\n    - Without common evaluation frameworks or neutral environments, organizations struggle to move beyond pilot programs\n    - Promising tools may demonstrate effectiveness under controlled conditions yet fail to generalize across institutions or patient populations\n  \n  - Lack of Real-World Performance Monitoring\n    - Many AI deployments lack mechanisms for continuous monitoring once tools are introduced into clinical workflows\n    - Performance often assessed at a single point in time, prior to deployment\n    - Limited visibility into how models behave over time as data distributions, workflows, and populations evolve\n    - In clinical settings, changes are inevitable\n      - Shifts in documentation practices, care delivery models, patient demographics, or upstream data sources can materially affect AI performance\n    - Without ongoing monitoring and drift detection, degradation may go unnoticed\n    - Undermines trust amongst clinicians and increases institutional risk\n  \n  - Unclear Accountability and Governance\n    - AI adoption introduces new questions of accountability not fully addressed by existing governance structures\n    - When AI outputs influence clinical workflows, often unclear where responsibility resides among vendors, health systems, clinicians, and supporting infrastructure providers\n    - Ambiguity creates hesitation among adopting organizations, particularly in regulated environments where risk management and compliance are paramount\n    - Without clear governance frameworks defining roles, oversight mechanisms, and escalation paths, institutions may defer adoption or limit AI use to low-impact scenarios\n  \n  - Workflow Disruption and Clinician Trust\n    - AI tools not well integrated into clinician workflows can impose additional cognitive and operational burden\n    - Even tools with strong technical performance may be resisted if they require significant workflow changes, add many additional steps, introduce opaque recommendations, or lack clear mechanisms for human oversight\n    - Particularly true when AI outputs exist outside of the EHR environment\n      - Requires clinicians to leave established workflows to access or interpret them\n    - When AI insights are not transparently integrated back into the patient record within existing clinical workflows, they are more likely to be interpreted as external impositions rather than supportive capabilities\n  \n  - Data Limitations and Bias\n    - Clinical AI frequently trained and evaluated using retrospective EHR data\n    - EHR data often reflects local documentation practices rather than full underlying clinical and operational context\n    - Data may be incomplete, biased, or unrepresentative of patient populations\n    - Reliance on EHR data alone limits ability to assess AI performance across diverse care settings and contexts\n    - EHR-centric approaches often fail to capture longitudinal, operational, and physiological factors that influence outcomes in real-world care\n    - These limitations contribute to bias, reduce ability to generalize, and complicate efforts to assess equity and effectiveness\n  \n  - Structural Nature of the Barriers\n    - Barriers to AI adoption are not primarily the result of insufficient algorithms or computational capability\n    - They reflect gaps in shared infrastructure for evaluation, monitoring, interoperability, and governance, particularly for non-medical device AI\n    - Also evident in growing use of consumer-grade sensors and wearable devices in care contexts\n      - Clinicians often presented with data without clear understanding of how it was collected, validated, or governed\n      - Therefore hesitate to incorporate it into clinical decision making\n    - Trust in AI and data depends not only on algorithms but on surrounding infrastructure that establishes provenance, credibility, and accountability\n\n- The Need for Continuous Evaluation, Monitoring, and Interoperability\n  - Effective AI adoption requires moving beyond episodic validation and point-in-time approvals\n  - Need lifecycle-based evaluation, monitoring, and interoperability\n  - For non-medical device AI, performance, safety, and equity are not static properties\n    - They evolve as data sources, workflows, and care contexts change\n  - Without mechanisms to continuously evaluate AI behavior in real-world environments, institutions lack visibility necessary to manage risk, sustain trust, and ensure value over time\n  \n  - Pre-Deployment Versus Post-Deployment Evaluation\n    - Current evaluation practices often emphasize pre-deployment testing conducted under controlled environments or idealized conditions\n    - While necessary, these assessments provide limited insight into how AI tools perform once integrated into diverse clinical workflows and exposed to real-world variability\n    - Differences in patient populations, documentation practices, staffing models, and upstream data quality can materially affect outcomes\n    - Post-deployment evaluation is essential\n    - Institutions need ability to assess whether AI tools continue to meet performance expectations, behave consistently across settings, and align with clinical and organizational goals\n    - Requires evaluation frameworks that persist beyond initial approval and operate across vendors, sites, and use cases\n  \n  - Continuous Monitoring and Drift Detection\n    - AI systems are sensitive to changes in the environments in which they operate\n    - Over time, shifts in clinical practice, coding standards, patient demographics, data pipelines, or continued evolution of AI models can introduce performance degradation or unintended effects\n    - Without continuous monitoring, such changes may remain undetected until they result in loss of confidence or other adverse outcomes\n    - Continuous performance monitoring enables early identification of drift, bias, and emerging risks\n    - Monitoring mechanisms should track performance metrics longitudinally, compare outcomes across populations, and automatically detect and flag deviations from expected behavior\n    - For non-medical device AI, these capabilities are particularly important as deployment often occurs outside traditional regulatory surveillance frameworks\n    - In many cases, monitoring capabilities may rely on automated or AI-assisted techniques to operate at scale across complex environments\n  \n  - Population- and Context-Aware Evaluation\n    - AI tools may perform well for certain populations or settings while underperforming for others\n    - Evaluation approaches relying on aggregate metrics risk obscuring disparities and masking context-specific failures\n    - To support equitable and effective use, AI evaluation must account for variation across patient populations, care settings, and operational contexts\n    - Requires access to diverse, representative data and ability to stratify performance by relevant factors\n    - Longitudinal evaluation supports understanding how AI impacts outcomes over time, including downstream effects to workflow, clinician behavior, and patient experience\n  \n  - Interoperability Beyond Data Exchange\n    - Interoperability is foundational for AI evaluation and monitoring but must extend beyond exchange of clinical data alone\n    - Standards such as FHIR have significantly advanced data access and exchange\n    - However, data movement alone does not provide sufficient context to evaluate or govern AI systems\n    - Effective AI interoperability must encompass additional layers including metadata, data provenance, evaluation artifacts, and governance policies\n    - Understanding how data is generated, transformed, and used is critical to interpreting AI outputs and assessing performance\n    - Interoperability across evaluation and monitoring systems enables comparability and shared learning without requiring centralization of sensitive data\n    - Government action could help create conditions under which AI used within EHR workflows is accompanied by appropriate provenance, confidence, and governance-related metadata to support transparency and clinician trust\n  \n  - Supporting Governance and Accountability through Interoperability\n    - Interoperability across evaluation and governance layers supports clearer accountability\n    - When AI systems are deployed across multiple institutions or workflows, shared frameworks for monitoring and reporting enable more consistent oversight\n    - This transparency reduces uncertainty for clinicians, administrators, and regulators\n    - Supports coordinated responses when issues arise\n    - By enabling interoperable evaluation and monitoring infrastructures, HHS can help establish a common foundation upon which diverse AI tools can be responsibly deployed\n    - Reduces fragmentation, supports trust, and accelerates adoption without constraining innovation\n  \n  - Implications for Policy and Investment\n    - Need for continuous evaluation, monitoring, and interoperability has direct implications for policy and investment decisions\n    - Government action supporting shared evaluation frameworks, reference architectures, and interoperable monitoring capabilities can lower barriers to adoption across the health ecosystem\n    - Investments in foundational capabilities complement rather than compete with private-sector innovation\n    - By focusing on infrastructure and governance, HHS can enable a more resilient and trustworthy AI ecosystem\n\n- Regulatory and Policy Considerations for Non-Medical Device AI\n  - As AI becomes increasingly embedded in clinical workflows and care-adjacent operations, existing regulatory and policy frameworks face growing strain\n  - Many current approaches were developed to govern discrete medical devices or static health IT systems\n  - Not well suited to non-medical device AI that evolves over time, operates across organizational boundaries, and influences care indirectly through workflows, prioritization, and information presentation\n  - Addressing these gaps does not require new regulation for every AI application\n  - Requires policy clarity that aligns oversight, accountability, and incentives with realities of how AI is developed, deployed, and maintained\n  \n  - Gaps in Accountability, Liability, and Governance\n    - Non-medical device AI often sits outside traditional regulatory pathways\n    - Creates ambiguity around accountability where systems underperform or cause unintended consequences\n    - Responsibility may be distributed among model developers, data providers, systems integrators, health systems, and end users\n    - Existing governance structures rarely define how these roles should interact or where oversight should reside\n    - Ambiguity discourages adoption, particularly in regulated environments where risk management is paramount\n    - Organizations may hesitate to deploy AI broadly without clear expectations regarding monitoring, escalation, documentation, and corrective action\n    - Clarifying governance responsibilities would reduce uncertainty and support more confident, responsible use\n  \n  - Over-Focus on AI Artifacts versus Systems\n    - Policy discussions often focus on characteristics of individual AI artifacts such as model architecture, training data, or performance metrics\n    - Rather than the systems in which those artifacts operate\n    - In practice, many risks associated with AI arise not from the model itself but from how it is integrated into workflows, how inputs are generated and governed, and how outputs are monitored and acted upon\n    - A systems-level perspective recognizes that AI performance and safety depend on data quality, interoperability, human oversight, and organizational context\n    - Policies emphasizing system-level practices rather than artifact-level certification alone are better aligned with realities of non-medical device AI deployment\n  \n  - The Role of Safe Harbors in Governance Practices\n    - One potential policy lever is use of safe harbors that recognize adherence to defined governance and evaluation practices\n    - Rather than prescribing specific technologies or models, HHS could encourage adoption by clarifying that organizations implementing AI within approved governance frameworks are meeting reasonable expectations for responsible use\n      - Frameworks supporting continuous monitoring, auditability, and human oversight\n    - Such an approach incentivizes adoption of defined governance practices while preserving flexibility and innovation\n    - Aligns oversight with outcomes, focusing attention on whether AI systems are appropriately evaluated and managed over time rather than on static design characteristics\n  \n  - Reference Architectures and Shared Infrastructures\n    - Reference architectures can play a critical role in translating policy into operational guidance\n    - By articulating how evaluation, monitoring, interoperability, and governance components fit together, reference architectures provide a common foundation for institutions, vendors, and regulators\n    - HHS is well positioned to support development or endorsement of reference architectures for non-medical device AI\n      - Build upon existing standards while extending beyond data exchange to include lifecycle oversight\n    - Shared infrastructure such as evaluation environments or monitoring frameworks can further reduce fragmentation and lower barriers to adoption without centralizing data or constraining competition\n  \n  - Alignment with Incentives and Programs\n    - Regulatory clarity alone is insufficient if incentives remain misaligned\n    - Payment, research, and innovation programs play a significant role in shaping adoption behavior\n    - Policies that recognize the cost and operational burden associated with evaluation, monitoring, and governance can encourage institutions to invest in these capabilities\n    - By aligning regulatory expectations with reimbursement models and research investments, HHS can reinforce importance of responsible AI practices and support sustainable adoption\n  \n  - Implications for Policy Development\n    - Effective governance of non-medical device AI requires a shift toward system-oriented, lifecycle-based policy approaches\n    - Clarity around accountability, support for reference architectures, and alignment of incentives can reduce uncertainty while enabling innovation\n    - Allows HHS to guide AI adoption through shared expectations and enabling infrastructure rather than prescriptive control of individual technologies\n\n- Role of R&D, CRADAs, and Public-Private Partnerships\n  - R&D, CRADAs, and other public-private partnerships play a critical role in translating AI capabilities from concept to sustained use in clinical care\n  - For non-medical device AI, the primary challenge is not absence of innovation\n  - But lack of environments and representative datasets in which evaluation, monitoring, governance, and interoperability practices can be developed, tested, and refined in real-world conditions\n  \n  - Applied R&D versus Basic Research\n    - Basic research continues to advance state of the art in AI methods and algorithms\n    - However, many barriers to adoption reside downstream of algorithmic innovation\n    - Applied R&D is needed to address questions of new applications, implementation, integration, and lifecycle management\n      - Including how AI systems interact with workflows, data quality processes, and governance structures over time\n    - Recent advances in generative AI further underscore need for robust evaluation, monitoring, and governance practices\n      - These tools are increasingly integrated into clinical and operational workflows\n    - Experience across health, research, and operational environments suggests these applications introduce new considerations around reliability, drift, and oversight that cannot be addressed through model development alone\n    - HHS-supported applied R&D can help bridge this gap by focusing on operational dimensions of AI use\n      - Evaluation methodologies, monitoring approaches, and interoperability mechanisms rather than model development alone\n    - These efforts complement existing research investments while addressing practical challenges faced by adopting institutions\n  \n  - Implementation Science and Real-World Validation\n    - Implementation science provides a valuable framework for understanding how technologies perform when introduced to complex, real-world environments\n    - For AI in clinical care, this includes assessing not only technical performance but usability, workflow impact, equity considerations, and sustainability\n    - Public-private partnerships can support implementation-focused studies examining how governance practices, monitoring strategies, and interoperability choices influence outcomes\n    - Such work generates evidence directly relevant to policy development and program design\n    - Helps reduce uncertainty for health systems considering adoption\n  \n  - Multi-Institution, Vendor-Neutral Partnerships\n    - Many challenges associated with AI adoption span organizational boundaries\n    - Single institutional efforts, while valuable, may not capture diversity of contexts in which AI tools operate\n    - Multi-institution partnerships enable broader evaluation across populations, workflows, and settings\n    - Improves generalizability and robustness\n    - Vendor-neutral partnerships are particularly important for non-medical device AI\n    - Neutral environments allow evaluation and governance practices to be developed independently of specific products\n    - Reduces conflicts of interest and supports fair comparison\n    - HHS is well positioned to convene such partnerships and establish shared expectations\n  \n  - Role of CRADAs and Cooperative Agreements\n    - CRADAs and cooperative agreements provide flexible mechanisms for collaboration between government, industry, and research organizations\n    - Can support joint development of evaluation frameworks, reference architectures, and monitoring approaches while leveraging complementary expertise\n    - By using these mechanisms to focus on infrastructure and governance capabilities, HHS can accelerate progress without dictating specific technical solutions\n    - Lessons learned through CRADAs can inform broader policy and programmatic decisions\n    - Creates a feedback loop between experimentation and guidance\n  \n  - Implications for HHS Investment Strategy\n    - Targeted investment in applied R&D, implementation science, and shared testbeds can yield outsized impact by addressing systemic barriers to AI adoption\n    - Such investments amplify value of private-sector innovation by providing conditions necessary for responsible deployment\n    - By prioritizing partnerships emphasizing evaluation, monitoring, governance, and interoperability, HHS can foster an ecosystem in which AI tools are not only developed but sustained and trusted in clinical care\n\n- Closing Recommendations\n  - Establish Reference Architectures for Non-Medical Device AI\n    - What HHS could do: Develop or endorse reference architectures describing how evaluation, monitoring, interoperability, and governance components fit together across the AI lifecycle for non-medical device applications\n    - Why it matters: Without shared architectural guidance, institutions and vendors independently interpret expectations, leading to fragmentation and inconsistent practices; reference architectures translate policy intent into operational clarity without mandating specific technologies\n    - Outcomes enabled: Accelerated adoption through reduced ambiguity, improved comparability across AI deployments, and a common foundation for institutions, vendors, and regulators\n  \n  - Support Continuous Evaluation and Monitoring Frameworks\n    - What HHS could do: Promote evaluation frameworks extending beyond pre-deployment validation to include post-deployment monitoring, drift detection, and longitudinal assessment across populations and settings\n    - Why it matters: AI performance and risk evolve over time; without continuous evaluation, degradation and inequities can go undetected, undermining trust and increasing institutional risk\n    - Outcomes enabled: Sustained AI performance, earlier identification of issues, improved equity assessment, and stronger confidence among clinicians and administrators\n  \n  - Encourage Interoperability Across Evaluation and Governance Layers\n    - What HHS could do: Advance interoperability approaches extending beyond data exchange to include metadata, data provenance, evaluation artifacts, and governance policies, building upon existing standards where appropriate\n    - Why it matters: Data interoperability alone is insufficient to support AI oversight; shared visibility into context and lineage is essential for evaluation, accountability, and coordinated response\n    - Outcomes enabled: Cross-institutional learning, reduced duplication of effort, and scalable oversight without centralizing sensitive data\n  \n  - Clarify Governance Expectations and Enable Safe Harbors\n    - What HHS could do: Provide guidance clarifying governance expectations for non-medical device AI and consider safe harbors for organizations implementing defined evaluation, monitoring, and oversight practices\n    - Why it matters: Uncertainty around accountability and liability discourages adoption; safe harbors tied to clearly defined governance and oversight practices will reduce perceived risk and incentivize responsible adoption without constraining innovation\n    - Outcomes enabled: Increased confidence among adopting institutions, alignment of incentives, and broader participation in responsible AI deployment\n  \n  - Invest in Applied R&D and Public-Private Testbeds\n    - What HHS could do: Use R&D programs, CRADAs, and cooperative agreements to support applied research, implementation science, and real-world testbeds focused on AI governance, evaluation, and interoperability\n    - Why it matters: Many adoption barriers can only be addressed through experimentation and shared learning in representative environments\n    - Outcomes enabled: Evidence-based policy development, reusable infrastructure, and faster translation of AI innovation into sustained clinical benefit\n\n- Question Crosswalk (Appendix A)\n  - Question 1 on biggest barriers to private sector innovation addressed in Sections 2.0 and 6.0\n  - Question 2 on regulatory, payment policy, or programmatic design changes addressed in Sections 4.0 and 6.0\n  - Question 3 on novel legal and implementation issues for non-medical devices addressed in Section 4.0\n  - Question 4 on promising AI evaluation methods addressed in Section 3.0\n  - Question 5 on supporting private sector activities addressed in Sections 4.0 and 5.0\n  - Question 6 on where AI tools have met or fallen short of expectations addressed in Section 3.0\n  - Question 7 on influential roles and administrative hurdles addressed in Section 2.0\n  - Question 8 on enhanced interoperability addressed in Section 3.0\n  - Question 9 on patient and caregiver challenges and concerns addressed in Section 2.0\n  - Question 10 on specific areas of AI research to prioritize addressed in Section 5.0\n\n---",
      "oneLineSummary": "Defense technology company with health AI infrastructure experience argues that clinical AI adoption is blocked not by lack of models but by missing governance, evaluation, and trust infrastructure, urging HHS to establish reference architectures, continuous monitoring frameworks, and safe harbors for responsible deployment.\n\n---",
      "commenterProfile": "- **Name/Organization:** BlueHalo, LLC (an AV Company)\n- **Type:** Business\n- **Role/Expertise:** Health and performance technology division of a large defense technology company; operates governed, secure data and AI infrastructure in regulated health, research, and operational environments; supports multi-institution and multi-stakeholder environments\n- **Geographic Scope:** National (headquarters in Huntsville, AL)\n- **Stake in Issue:** Provides infrastructure and enablement services for AI adoption in healthcare; positioned to benefit from standardized evaluation frameworks and reference architectures that would create market opportunities for their governance and interoperability solutions\n\n---",
      "corePosition": "We assert that accelerating AI adoption in clinical care requires shifting emphasis from developing more AI models to establishing shared infrastructure, governance mechanisms, and evaluation frameworks that enable trust. The barriers are not primarily algorithmic—they're structural gaps in evaluation, monitoring, interoperability, and accountability, particularly for non-medical device AI that influences care through workflows, documentation, and decision support without being regulated as medical devices.\n\n---",
      "keyRecommendations": "- Establish reference architectures for non-medical device AI\n  - Develop or endorse architectures describing how evaluation, monitoring, interoperability, and governance components fit together across the AI lifecycle\n  - Translates policy intent into operational clarity without mandating specific technologies\n  - Creates common foundation for institutions, vendors, and regulators\n\n- Support continuous evaluation and monitoring frameworks\n  - Promote frameworks extending beyond pre-deployment validation to include post-deployment monitoring and drift detection\n  - Enable longitudinal assessment across populations and settings\n  - Address the reality that AI performance and risk evolve over time\n\n- Encourage interoperability across evaluation and governance layers\n  - Advance approaches extending beyond data exchange to include metadata, data provenance, evaluation artifacts, and governance policies\n  - Build upon existing standards like FHIR while extending to lifecycle oversight\n  - Enable cross-institutional learning without centralizing sensitive data\n\n- Clarify governance expectations and enable safe harbors\n  - Provide guidance clarifying expectations for non-medical device AI governance\n  - Consider safe harbors for organizations implementing defined evaluation, monitoring, and oversight practices\n  - Reduce perceived risk and incentivize responsible adoption without constraining innovation\n\n- Invest in applied R&D and public-private testbeds\n  - Use R&D programs, CRADAs, and cooperative agreements to support applied research and implementation science\n  - Focus on AI governance, evaluation, and interoperability rather than model development alone\n  - Create real-world testbeds where practices can be developed, tested, and refined\n\n---",
      "mainConcerns": "- Fragmented evaluation environments\n  - AI tools evaluated within vendor-controlled or site-specific environments limit independent assessment\n  - Reinforces vendor lock-in and impedes informed decision-making\n  - Organizations struggle to move beyond pilot programs without common evaluation frameworks\n\n- Lack of real-world performance monitoring\n  - Many deployments lack mechanisms for continuous monitoring once tools enter clinical workflows\n  - Performance assessed at single point in time with limited visibility into behavior over time\n  - Degradation may go unnoticed, undermining clinician trust and increasing institutional risk\n\n- Unclear accountability and governance\n  - Responsibility distributed among model developers, data providers, integrators, health systems, and end users\n  - Existing structures rarely define how roles should interact or where oversight should reside\n  - Ambiguity creates hesitation, particularly in regulated environments\n\n- Workflow disruption and clinician trust\n  - Tools not well integrated impose additional cognitive and operational burden\n  - AI outputs existing outside EHR environment require clinicians to leave established workflows\n  - Insights not transparently integrated into patient records interpreted as external impositions\n\n- Data limitations and bias\n  - Clinical AI frequently trained on retrospective EHR data reflecting local documentation practices\n  - Data may be incomplete, biased, or unrepresentative of patient populations\n  - EHR-centric approaches fail to capture longitudinal, operational, and physiological factors\n\n- Policy over-focus on AI artifacts versus systems\n  - Discussions focus on model architecture, training data, or performance metrics\n  - Many risks arise from how AI is integrated into workflows, how inputs are governed, how outputs are monitored\n  - Need systems-level perspective recognizing dependence on data quality, interoperability, human oversight, and organizational context\n\n---",
      "notableExperiences": "- Infrastructure-first perspective from defense sector experience\n  - Draws on operating governed data and AI environments in regulated health and research contexts\n  - Emphasizes that trust in AI depends not only on algorithms but on surrounding infrastructure establishing provenance, credibility, and accountability\n\n- Consumer device trust gap observation\n  - Clinicians presented with data from consumer-grade sensors and wearables often hesitate to incorporate it into clinical decision-making\n  - Lack clear understanding of how data was collected, validated, or governed\n  - Illustrates broader principle that data movement alone is insufficient without context\n\n- Generative AI governance implications\n  - Recent advances in GenAI underscore need for robust evaluation, monitoring, and governance practices\n  - These applications introduce new considerations around reliability, drift, and oversight that cannot be addressed through model development alone\n\n- Interoperability beyond FHIR\n  - While FHIR has significantly advanced data access and exchange, data movement alone does not provide sufficient context to evaluate or govern AI systems\n  - Effective AI interoperability must encompass metadata, data provenance, evaluation artifacts, and governance policies\n\n---",
      "keyQuotations": "- \"This response asserts that accelerating the adoption and effective use of AI in clinical care requires a shift in emphasis, from the development of additional AI models to the establishment of shared infrastructure, governance mechanisms, and evaluation frameworks that enable trust in data and AI systems.\"\n\n- \"When AI insights are not transparently integrated back into the patient record within existing clinical workflows, they are more likely to be interpreted as external impositions, rather than supportive capabilities.\"\n\n- \"For non-medical device AI, the primary challenge is not the absence of innovation, but the lack of environments and representative datasets in which evaluation, monitoring, governance, and interoperability practices can be developed, tested, and refined in real-world conditions.\""
    },
    "themeScores": {
      "1": 1,
      "2": 1,
      "3": 1,
      "5": 1,
      "6": 1,
      "8": 1,
      "9": 1,
      "1.6": 1,
      "2.1": 1,
      "2.5": 1,
      "5.1": 1,
      "6.1": 1,
      "6.8": 1,
      "8.1": 1,
      "8.5": 1,
      "9.1": 1,
      "9.3": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "AI Governance"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Human-in-the-Loop"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Post-deployment Monitoring"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Pre-deployment Evaluation"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Safe Harbor"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Trustworthy AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Algorithmic Bias"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Generative AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Model Drift"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Certification"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Provenance"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Health Information Systems",
        "label": "EHR"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Medical Coding"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Pilot Program"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "FHIR"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Disparities"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Equity"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Health System"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Experience"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "CRADA"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "Cooperative Agreement"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "Implementation Science"
      }
    ],
    "hasAttachments": true,
    "wordCount": 4531,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0040",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "National Multiple Sclerosis Society",
    "submitterType": "Organization",
    "date": "2026-02-05T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Multiple Sclerosis overview\n  - Unpredictable disease of the central nervous system with no current cure\n  - Symptoms vary person to person: disabling fatigue, mobility challenges, cognitive changes, vision issues\n  - Estimated 1 million people live with MS in the United States\n  - Early diagnosis and treatment are critical to minimize disability\n\n- About the National Multiple Sclerosis Society\n  - Founded in 1946, global leader in movement to create a world free of MS\n  - Funds research for a cure, drives change through advocacy, provides programs and services\n  - Research investment has exceeded $1.2 billion to date\n  - Partners with government on accelerating discovery, development, and delivery of new treatments\n\n- General position on AI in healthcare\n  - AI will transform and improve healthcare if used appropriately\n  - Key conditions for success:\n    - Used in appropriate settings\n    - Transparency about why it's being utilized\n    - Goals clearly outlined for all stakeholders\n    - Patient and patient representative needs considered\n  - Important to assess whether AI introduces specific risks and harms\n    - Algorithms could amplify errors and preexisting biases in source data\n\n- Response focuses on RFI question #9 regarding patient and caregiver challenges and concerns\n  - Three main themes: patient-involved research, transparency and communication, health equity\n\n- AI potential for MS clinical care\n  - Can enhance how clinical data are integrated, analyzed, and translated into actionable insights\n  - Can support safety, effectiveness, and quality evaluations\n  - Can integrate diverse data sources: genetic databases, clinical studies, real-world evidence\n  - Could improve understanding of disease heterogeneity, progression, and treatment response\n  - Could lead to more personalized and effective treatment regimens\n  - Could optimize outcomes while minimizing unnecessary side effects\n\n- Data quality and representation concerns\n  - Reliability and equity of AI tools depend on quality and representativeness of training data\n  - Critical to engage individuals with lived experience of MS, especially from underrepresented communities\n  - Patient input ensures AI tools are scientifically rigorous and socially/ethically aligned with diverse needs\n\n- Patient-Involved Research and Development of AI Technologies\n  - Individuals living with chronic conditions are the experts on their disease\n  - Patients, carepartners, and loved ones hold wealth of information for designing therapeutics and technologies\n  - HHS and AI developers should meet with patients and carepartners early and often\n  - Capturing lived experiences of diverse individuals ensures technologies are meaningful and reflect patient needs\n  - Patient-involved research essential for designing technologies that benefit all patients and make them comfortable\n  - Recommends HHS facilitate focus groups and patient-focused product development meetings\n    - Allows AI developers to gather input from those with lived experience\n    - Builds trust in products developed with patient community\n\n- Transparency and Communication in AI Development\n  - For successful AI adoption, HHS and developers must be transparent and communicate to patients:\n    - Goal of utilizing AI in clinical care\n    - Scope of what technologies can and cannot do\n    - How technology was developed\n    - How individual's data is shared and secured\n  - This information should be available on HHS's website\n  - Educational efforts should come from trusted sources through community and patient groups\n  - Development of educational materials should consider:\n    - How target audiences best receive information\n    - Include meaningful messages\n    - Identify who is best suited to deliver information\n  - Focused offerings from peers in formal and informal settings may help gain trust\n  - Specific suggestions for patient engagement:\n    - Develop range of educational offerings reflecting different learning styles\n    - Ensure all materials reflect health literacy principles and plain language\n    - Create multilingual materials reflecting cultural differences\n    - Include social determinants of health resources\n    - Promote shared decision-making\n    - Include current MS treatment considerations and best practices for informed decisions\n    - Increase connections to diverse communities using trusted sources, ambassadors, carepartners, and peers\n  - Recommends stakeholder feedback opportunities for HHS to share strategic AI plans and gather input\n  - Recommends HHS work with academia and patient community experts to routinely examine algorithms and data\n    - Ensure they don't cause harm or miss signals important to HHS and patients\n  - HHS should engage with other government agencies for consistency in AI use\n    - FDA, NIH, CMS, CDC, etc.\n    - Share learnings and best practices\n    - Avoid duplicative efforts\n    - Create standards for uniform adoption\n\n- Ensuring Population Health in the Use of AI\n  - AI benefits must be equitably distributed to avoid perpetuating healthcare disparities\n  - Concerns about algorithmic bias, incomplete datasets, and unintended consequences\n    - Could disproportionately affect certain patient populations\n  - Advocates for:\n    - Use of representative and high-quality data reflecting diverse patient populations\n    - Establishment of review process by advisory panel of individuals with lived experience (e.g., people living with MS)\n    - Routine audits, validation processes, and accountability measures to detect and mitigate bias\n    - Integration of population health principles into AI development\n      - Ensure technological advances don't exacerbate existing care gaps\n      - Drive meaningful improvements in health outcomes for all patients\n  - Prioritizing transparency, patient empowerment, and innovation can foster accountability and build trust\n  - Ensures emerging technologies contribute to more inclusive and effective healthcare system",
      "oneLineSummary": "The National MS Society, representing 1 million Americans with multiple sclerosis, urges HHS to center patients with lived experience in AI development through focus groups, transparent communication, and equity safeguards to ensure these technologies serve diverse populations without amplifying bias.",
      "commenterProfile": "- **Name/Organization:** National Multiple Sclerosis Society\n- **Type:** Advocacy Group\n- **Role/Expertise:** Patient advocacy organization with $1.2 billion in research investment; global leader in MS research, advocacy, and patient services since 1946\n- **Geographic Scope:** National (United States) with global collaboration\n- **Stake in Issue:** Represents approximately 1 million Americans living with MS who could benefit from AI-enhanced diagnosis, treatment personalization, and care delivery—but could also be harmed by biased algorithms or technologies developed without patient input",
      "corePosition": "We support AI adoption in clinical care but believe its success depends on meaningful patient involvement throughout development, transparent communication about capabilities and limitations, and rigorous safeguards against algorithmic bias. People living with chronic conditions like MS are the experts on their disease, and their lived experience must inform how these technologies are designed and deployed to ensure they benefit all patients equitably.",
      "keyRecommendations": "- Engage patients and carepartners early and often in AI development\n  - Facilitate focus groups and patient-focused product development meetings\n  - Capture lived experiences of diverse individuals with chronic conditions\n  - Build trust by developing products with the patient community\n\n- Ensure transparency and accessible communication\n  - Publish on HHS website: goals of AI use, technology capabilities and limitations, development process, data sharing and security practices\n  - Develop educational materials that are multilingual, culturally appropriate, written in plain language, and include social determinants of health resources\n  - Use trusted sources, ambassadors, carepartners, and peers to deliver information\n  - Create diverse educational offerings reflecting different learning styles\n\n- Establish ongoing stakeholder engagement\n  - Hold stakeholder feedback opportunities for HHS to share strategic AI plans\n  - Work with academia and patient community to routinely examine algorithms and data for harm\n\n- Coordinate across federal agencies\n  - Engage with FDA, NIH, CMS, CDC to ensure consistency in AI use\n  - Share learnings and best practices\n  - Create standards for uniform adoption\n\n- Implement equity safeguards\n  - Use representative, high-quality data reflecting diverse patient populations\n  - Establish advisory panel review process with individuals with lived experience\n  - Conduct routine audits, validation processes, and accountability measures to detect and mitigate bias\n  - Integrate population health principles into AI development",
      "mainConcerns": "- Algorithmic bias and data quality\n  - AI reliability and equity depend on quality and representativeness of training data\n  - Algorithms could amplify errors and preexisting biases in source data\n  - Incomplete datasets could disproportionately affect certain patient populations\n\n- Exclusion of patient voice\n  - Technologies developed without patient input may not reflect patient needs and concerns\n  - Patients may not feel comfortable using technologies they weren't involved in creating\n\n- Health equity risks\n  - AI benefits may not be equitably distributed\n  - Technological advances could exacerbate existing care gaps rather than close them\n  - Unintended consequences could disproportionately harm certain populations\n\n- Lack of transparency\n  - Patients need to understand why AI is being used, what it can and cannot do, and how their data is handled\n  - Without clear communication from trusted sources, patients may not trust these technologies",
      "notableExperiences": "- Reframes patient role in AI development: positions people with chronic conditions as \"experts on their disease\" whose lived experience is essential data for technology design, not just end users to be educated\n- Emphasizes trust-building through co-creation: suggests that involving patients in development isn't just ethically right but practically necessary for adoption—people are more comfortable using technologies they helped shape\n- Proposes peer-to-peer education model: recommends \"focused offerings from peers in both formal and informal settings\" as more effective than top-down communication for building trust in AI technologies\n- Highlights MS-specific AI potential: identifies concrete applications including integrating genetic databases with clinical studies and real-world evidence to understand disease heterogeneity, progression, and treatment response",
      "keyQuotations": "- \"Individuals living with chronic conditions are the experts on their disease. These individuals, along with their carepartners and loved ones, hold a wealth of information that can inform the design of novel therapeutics and emerging technologies to support care delivery.\"\n\n- \"The reliability and equity of AI tools hinge on the quality and representativeness of the data used to train these models. This underscores the critical importance of engaging individuals with lived experience of MS, especially from underrepresented communities, throughout the model development process.\"\n\n- \"It is important to assess whether the use of AI introduces specific risks and harms, such as algorithms that could amplify errors and preexisting biases in the source data.\""
    },
    "themeScores": {
      "7": 1,
      "8": 1,
      "2.4": 1,
      "7.1": 1,
      "7.2": 1,
      "8.1": 1,
      "8.5": 1
    },
    "entities": [
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Algorithmic Bias"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Diagnosis"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Shared Decision-Making"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Treatment"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Chronic Disease Management"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Genomics"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Population Health"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CDC"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "FDA"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "NIH"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Equity"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Multiple Sclerosis"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Engagement"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "RWE"
      },
      {
        "category": "Social Determinants of Health",
        "label": "Health Literacy"
      },
      {
        "category": "Social Determinants of Health",
        "label": "SDOH"
      }
    ],
    "hasAttachments": true,
    "wordCount": 1599,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0041",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Intersocietal Accreditation Commission",
    "submitterType": "Organization",
    "date": "2026-02-05T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Introduction and organizational background\n  - IAC is a nonprofit, nationally recognized CMS-designated Advanced Diagnostic Imaging accrediting organization\n  - Founded by medical professionals to advance appropriate utilization, standardization, quality imaging, and safety of diagnostic imaging and interventional-based procedures\n  - Offers accreditation in hospitals and all other settings where diagnostic testing and procedures are performed\n  - Represents a unique collaboration of physicians, technologists, sonographers, physicists, and other medical professionals from more than 40 medical specialties as sponsoring organizations\n  - Mission is to improve health care through accreditation using rigorous clinical peer review to ensure quality and safe practices\n  - Since 1991, has granted accreditation to more than 14,000 sites in ten diagnostic imaging and interventional-based accreditation programs\n  - One of four national accreditation organizations designated by CMS to ensure quality of advanced diagnostic imaging suppliers under MIPPA 2008\n  - Only imaging accreditation organization holding ISO certifications in Quality Management Systems (ISO 9001:2015) and Information Security Systems (27001:2022)\n\n- IAC's AI Task Force and guidance development\n  - Board of Directors established an AI Task Force in 2024 acknowledging rapid evolution of AI in medicine\n  - Task force developed guidance document approved by Board and published April 2025 as addendum to each set of IAC Standards\n\n- IAC AI Guidance Document recommendations\n  - Serves as recommendation for IAC-accredited facilities utilizing AI technology\n  - To assure quality and safety of care delivery when using AI applications for direct-patient care (clinical) purposes, each facility should create and follow policies and procedures addressing:\n    - Training for personnel who use AI\n    - Security of AI software, updates, HIPAA considerations, etc.\n    - AI for Quality Improvement (if applicable)\n    - Appropriate use for each AI application\n    - Governance (authority to make decisions regarding AI implementation)\n  - Clinical use of AI defined as including image acquisition, image processing/enhancement, image interpretation, report generation, risk assessment of prognosis, patient history, identification of critical values/results, and equipment quality control\n\n- Current state of AI regulation and IAC's position\n  - IAC recognizes AI use is expanding and there is a lack of uniform processes to assure patient safety and quality care delivery\n  - Recently issued guidance represents an interim solution offering foundational guidelines for most rudimentary quality and safety aspects of clinical AI\n  - As cutting-edge AI innovations continue to evolve, IAC intends to enhance its guidance for the clinical setting\n\n- Response to HHS request for feedback on supporting private sector activities\n  - IAC's accreditation programs already include thorough assessments of the care environment including infrastructure, capacity, systems, and processes\n  - These assessments promote data-driven improvements that lead to better outcomes\n  - IAC will continue to incorporate established guidelines, including those supporting effective and safe use of AI, into accreditation standards\n\n- Offer to collaborate with HHS\n  - IAC's number one priority is to promote safe and equitable patient care\n  - Stands ready to assist HHS in ensuring those goals are preserved as AI use in clinical care rapidly evolves\n  - Offers to meet with HHS to discuss work in more detail and potential future collaboration when guidelines for AI use are established",
      "oneLineSummary": "National imaging accreditation organization representing 40+ medical specialties offers its newly developed AI guidance framework and expertise to help HHS establish quality and safety standards for clinical AI use.",
      "commenterProfile": "- **Name/Organization:** Intersocietal Accreditation Commission (IAC), submitted by Tanya Tolpegin, MBA, CAE, Chief Executive Officer\n- **Type:** Accreditation Organization / Trade Association\n- **Role/Expertise:** CMS-designated Advanced Diagnostic Imaging accrediting organization; ISO-certified in Quality Management and Information Security; represents 40+ medical specialty sponsoring organizations; has accredited 14,000+ sites since 1991\n- **Geographic Scope:** National\n- **Stake in Issue:** As an accrediting body for diagnostic imaging and interventional procedures, IAC is directly positioned to incorporate AI quality and safety standards into its accreditation programs",
      "corePosition": "IAC recognizes that AI use in clinical care is expanding rapidly while uniform processes to assure patient safety and quality care delivery are lacking. We have already developed interim AI guidance for our accredited facilities and stand ready to assist HHS in establishing comprehensive guidelines, offering our multi-stakeholder expertise and existing accreditation infrastructure as a foundation for ensuring safe and equitable AI implementation.",
      "keyRecommendations": "- Facilities using AI for clinical purposes should create policies and procedures addressing five key areas:\n  - Training for personnel who use AI\n  - Security of AI software, updates, and HIPAA considerations\n  - AI for Quality Improvement (if applicable)\n  - Appropriate use for each AI application\n  - Governance and authority for AI implementation decisions\n- HHS should leverage existing private sector accreditation infrastructure to promote effective and safe AI use\n  - Accreditation programs already assess care environment including infrastructure, capacity, systems, and processes\n  - These assessments promote data-driven improvements leading to better outcomes\n- HHS should collaborate with accreditation organizations like IAC when establishing AI guidelines",
      "mainConcerns": "- Lack of uniform processes to assure patient safety and quality care delivery as AI use expands\n- Need for foundational guidelines addressing rudimentary quality and safety aspects of clinical AI\n- Rapid evolution of AI innovations requires ongoing enhancement of guidance for clinical settings",
      "notableExperiences": "- IAC proactively established an AI Task Force in 2024 and published AI guidance in April 2025 as an addendum to all accreditation standards—demonstrating that private sector accreditation bodies are already moving to address AI quality and safety gaps\n- The guidance specifically defines \"clinical use of AI\" to include image acquisition, processing/enhancement, interpretation, report generation, risk assessment, patient history, critical value identification, and equipment quality control—providing a practical taxonomy for regulatory consideration\n- IAC's unique multi-stakeholder structure (40+ medical specialties including physicians, technologists, sonographers, and physicists) offers a model for collaborative AI governance",
      "keyQuotations": "- \"IAC recognizes that AI use is expanding and there is a lack of uniform processes to assure patient safety and quality care delivery.\"\n- \"Our recently issued guidance represents an interim solution, offering foundational guidelines to address the most rudimentary quality and safety aspects of clinical AI.\""
    },
    "themeScores": {
      "1": 1,
      "1.6": 1
    },
    "entities": [
      {
        "category": "Accreditation and Certification",
        "label": "Accreditation"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Diagnosis"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Patient Safety"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Quality Improvement"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Physician"
      },
      {
        "category": "Laws and Regulations",
        "label": "HIPAA"
      },
      {
        "category": "Laws and Regulations",
        "label": "MIPPA"
      },
      {
        "category": "Professional Organizations",
        "label": "IAC"
      },
      {
        "category": "Professional Organizations",
        "label": "ISO"
      }
    ],
    "hasAttachments": true,
    "wordCount": 1019,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0042",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Advanced Interactive Technology Holdings LLC",
    "submitterType": "Organization",
    "date": "2026-02-05T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Response to HHS Request for Information on AI in Clinical Care\n  - Submitted by Raynold Gallego, CEO & Founder, Advanced Interactive Technology Holdings LLC\n  - Focus on AI research priorities for Medicare fraud prevention (Question #10)\n\n- Background - The $100 Billion Problem\n  - Medicare fraud costs $100+ billion annually\n  - Current post-payment detection is reactive\n    - CMS pays claims first, then investigative agencies spend years pursuing recovery\n  - Recent DOJ conviction of Joel Rufus French for $197M fraud demonstrates the problem\n    - Fraud involved orthotic braces billed for amputees and deceased beneficiaries\n    - These are violations AI could detect instantly\n\n- Recommended Priority: Real-Time Medicare Claims Validation Using AI\n  - HHS should prioritize AI systems that prevent fraud BEFORE payment\n  - Claims should be validated in under 3 seconds against regulatory and clinical standards\n\n- Technical Requirements\n  - Regulatory compliance checking\n    - Anti-Kickback Statute, Stark Law, False Claims Act, EKRA\n  - Clinical validation\n    - ICD-10/CPT correlation, medical necessity, patient eligibility\n  - Pattern recognition\n    - Statistical anomalies, emerging fraud schemes, financial relationships\n\n- AI/ML Components Needed\n  - Supervised learning trained on 20+ years of OIG/DOJ prosecutions\n  - Unsupervised learning to discover novel patterns\n  - NLP to analyze documentation for gaming\n  - Graph networks to map ownership and kickback relationships\n  - Reinforcement learning to optimize based on outcomes\n\n- Performance Targets\n  - Process 15M+ claims daily\n  - Speed under 3 seconds per claim\n  - Accuracy over 95% detection rate\n  - Scale to 5B+ claims annually\n  - Uptime of 99.99%\n\n- Value Proposition and ROI Analysis\n  - Current state: Pay $100B in fraud, spend $5B investigating, recover $20B = Net loss $85B\n  - With AI prevention: Spend $10B on validation, prevent $90B fraud = Net savings $80B\n  - ROI: 8:1 return\n\n- Recommended SBIR Topics for FY2026-2027\n  - Topic 1: AI-Powered Real-Time Medicare Claims Validation\n    - Phase I: Demonstrate under 3 second validation with over 90% accuracy\n    - Phase II: Process 100K claims/day prototype\n    - Phase III: Scale to 1B claims/year nationally\n  - Topic 2: ML for Healthcare Billing Vulnerability Detection\n    - Phase I: Prove AI detects emerging fraud faster than analysts\n    - Phase II: Real-time loophole detection system\n    - Phase III: CMS integration for continuous monitoring\n  - Topic 3: NLP for Medical Necessity Validation\n    - Phase I: Detect documentation gaming with over 85% accuracy\n    - Phase II: Production-grade medical record analysis\n    - Phase III: Claims workflow integration\n\n- Alignment with HHS Priorities\n  - Reduces costs with $80B annual savings potential\n  - Reduces provider burden by providing guidance that prevents violations\n  - Protects patients by preventing unnecessary procedures\n  - Maintains public trust through stewardship demonstration\n  - Advances American AI leadership through novel regulatory application\n\n- Commercial Applicability\n  - Government: Medicare/Medicaid\n  - Commercial: Private payers face $60-80B fraud annually\n  - Enables public-private partnerships\n\n- Conclusion\n  - AI fraud prevention shifts from reactive detection to proactive prevention\n  - Technology is feasible, ROI is compelling, need is urgent\n\n---",
      "oneLineSummary": "A technology company CEO and law student proposes real-time AI claims validation to prevent Medicare's $100B annual fraud problem before payment rather than chasing recovery afterward.\n\n---",
      "commenterProfile": "- **Name/Organization:** Raynold Gallego, Advanced Interactive Technology Holdings LLC\n- **Type:** Business\n- **Role/Expertise:** CEO & Founder; JD Candidate at Texas A&M Law School; Master of Legal Studies in Healthcare Law\n- **Geographic Scope:** National (Lancaster, CA based; federal contracting credentials noted)\n- **Stake in Issue:** Potential technology vendor for government AI fraud prevention systems; positioned for SBIR funding opportunities\n\n---",
      "corePosition": "HHS should prioritize AI research that prevents Medicare fraud before payment occurs, rather than relying on the current reactive system of paying claims first and investigating later. Real-time claims validation using AI could save $80 billion annually by catching fraudulent claims in under 3 seconds, shifting from costly post-payment recovery to proactive prevention.\n\n---",
      "keyRecommendations": "- Prioritize development of real-time Medicare claims validation AI that processes claims in under 3 seconds\n  - Must check regulatory compliance (Anti-Kickback Statute, Stark Law, False Claims Act, EKRA)\n  - Must validate clinical appropriateness (ICD-10/CPT correlation, medical necessity, patient eligibility)\n  - Must recognize fraud patterns (statistical anomalies, emerging schemes, financial relationships)\n\n- Fund three specific SBIR topics for FY2026-2027:\n  - AI-Powered Real-Time Medicare Claims Validation\n    - Phase I: Demonstrate under 3 second validation with over 90% accuracy\n    - Phase II: 100K claims/day prototype\n    - Phase III: Scale to 1B claims/year nationally\n  - ML for Healthcare Billing Vulnerability Detection\n    - Phase I: Prove AI detects emerging fraud faster than human analysts\n    - Phase II: Real-time loophole detection\n    - Phase III: CMS integration\n  - NLP for Medical Necessity Validation\n    - Phase I: Detect documentation gaming with over 85% accuracy\n    - Phase II: Production-grade medical record analysis\n    - Phase III: Claims workflow integration\n\n- Deploy multiple AI/ML approaches in combination:\n  - Supervised learning trained on 20+ years of OIG/DOJ prosecution data\n  - Unsupervised learning to discover novel fraud patterns\n  - NLP to detect documentation gaming\n  - Graph networks to map ownership and kickback relationships\n  - Reinforcement learning to optimize based on outcomes\n\n---",
      "mainConcerns": "- Current fraud detection is fundamentally reactive and inefficient\n  - CMS pays claims first, then spends years pursuing recovery\n  - Net loss of $85B annually (pay $100B fraud, spend $5B investigating, recover only $20B)\n\n- Obvious fraud patterns go undetected until after payment\n  - Recent $197M fraud case involved billing orthotic braces for amputees and deceased beneficiaries\n  - These violations could be caught instantly with proper AI validation\n\n- Private payers face similar $60-80B annual fraud burden\n  - Problem extends beyond Medicare/Medicaid\n\n---",
      "notableExperiences": "- Cites same-day DOJ conviction announcement as real-time example of preventable fraud\n  - Joel Rufus French convicted for $197M in fraud\n  - Scheme billed orthotic braces for patients who were amputees or deceased\n  - Argues this type of obvious mismatch could be caught instantly by AI\n\n- Proposes specific ROI calculation framework:\n  - Current system: $100B fraud - $5B investigation cost - $20B recovery = $85B net loss\n  - Proposed system: $10B validation cost - $90B prevented fraud = $80B net savings\n  - 8:1 return on investment\n\n- Suggests fraud prevention AI could reduce provider burden rather than increase it\n  - Real-time guidance could help providers avoid unintentional violations\n\n---",
      "keyQuotations": "- \"Current post-payment detection is reactive—CMS pays claims first, then investigative agencies spend years pursuing recovery.\"\n\n- \"Today's DOJ announcement of Joel Rufus French's conviction for $197M fraud (orthotic braces for amputees and deceased beneficiaries) demonstrates violations that AI could detect instantly.\"\n\n- \"AI fraud prevention shifts from reactive detection to proactive prevention. Technology is feasible, ROI compelling, need urgent.\""
    },
    "themeScores": {
      "9": 1,
      "9.1": 1,
      "9.2": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "Post-deployment Monitoring"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Machine Learning"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Natural Language Processing"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "DOJ"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "OIG"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Fraud"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Claims Processing"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "ROI"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Workflow Integration"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "CPT"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "ICD-10"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicaid"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Laws and Regulations",
        "label": "Anti-Kickback Statute"
      },
      {
        "category": "Laws and Regulations",
        "label": "EKRA"
      },
      {
        "category": "Laws and Regulations",
        "label": "False Claims Act"
      },
      {
        "category": "Laws and Regulations",
        "label": "Stark Law"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Trust"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "SBIR"
      }
    ],
    "hasAttachments": true,
    "wordCount": 544,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0043",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Renee L Pope",
    "submitterType": "Individual",
    "date": "2026-02-05T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Introduction and Commenter Background\n  - Submitting comment to support HHS in identifying concrete actions to accelerate safe, effective, and equitable AI adoption in clinical care\n  - Particular focus on post-acute and long-term care (PALTC) settings where readiness barriers are structural and persistent\n  - Perspective reflects extensive operational experience in PALTC settings\n    - Exposure to interdisciplinary workflows, documentation systems, incident reporting practices, and regulatory compliance pressures across multiple facilities\n  - Systems-level governance perspective informed by scholarly work: Human-Led Embodiment & Co-Regulatory Augmentation (H-LECA)\n    - Emphasizes continuity, risk containment, and human-led control in longitudinal human-AI interaction\n  - RFI appropriately centers three federal levers: regulation, reimbursement, and research & development\n\n- General Response to RFI (Cross-Cutting Themes)\n  - Readiness is uneven across sectors; PALTC faces distinct foundational barriers\n    - Health care AI adoption not limited by model capability alone\n    - In PALTC, binding constraints are infrastructure, workforce bandwidth, data integrity, and organizational instability\n    - These conditions repeatedly identified in long-term care implementation research as barriers to AI-enabled solutions at scale\n  - \"Clinical AI\" adoption will fail if the first wave is not operationally safe\n    - For PALTC, high-risk \"clinical\" AI should not be the entry point\n    - Staged acceleration strategy more likely to be safe and scalable\n      - Prioritize non-clinical workflow AI first (scheduling, supply chain, documentation quality controls, administrative burden reduction)\n      - Progress to higher-risk use only after readiness benchmarks are met\n    - Success depends on sociotechnical conditions, workflow integration, and sustained operational capacity rather than tool availability alone\n  - Continuity, change-management, and post-deployment monitoring should be treated as safety requirements\n    - Predictability, public trust, and risk-proportionate oversight require attention to what happens after deployment\n    - Workflow drift, updates, outages, and data shifts can degrade performance and worsen inequities if monitoring is weak\n    - Continuity-aware approach should incorporate detection of destabilizing changes and structured repair pathways\n\n- Responses to Specific Questions\n  - Question 1: Biggest barriers to private sector innovation in AI for health care and its adoption/use in clinical care\n    - Infrastructure barriers (PALTC)\n      - Unreliable connectivity and unmapped Wi-Fi coverage in resident care areas\n      - Legacy or limited call-light systems with failures and limited auditability\n      - Environmental and operational conditions that prevent \"always available\" digital workflows\n      - Infrastructure readiness and organizational capacity strongly shape whether AI-enabled tools can be implemented safely and sustained\n    - Data integrity barriers\n      - Documentation structures can produce ambiguous or inconsistent entries that undermine data reliability for AI training and inference\n      - Under high workload, documentation completeness drops, degrading model inputs and increasing downstream risk\n      - Adoption depends on trustworthy data pipelines and workflow-aligned data capture\n      - Failures frequently arise from sociotechnical breakdowns rather than model architecture alone\n    - Workforce and training barriers\n      - Chronic staffing shortages and turnover reduce capacity for onboarding, calibration, monitoring, and governance\n      - Training constraints reduce digital literacy and safe adoption readiness\n      - Workforce bandwidth, training supports, and staff acceptance are central determinants of success\n    - Financial and corporate barriers\n      - Underinvestment in infrastructure and frequent ownership/management turnover disrupt long-horizon modernization planning\n      - Consistent with broader operating environment in skilled nursing facilities and documented prevalence of ownership changes\n\n  - Question 2: What regulatory, payment policy, or programmatic design changes should HHS prioritize, and why?\n    - Regulation: baseline digital safety readiness for PALTC\n      - HHS should clarify expectations that facilities meet minimum digital reliability and governance readiness before implementing AI-dependent workflows\n      - One pathway: leverage existing operational and quality improvement obligations under 42 CFR Part 483\n        - 42 CFR § 483.70 (Administration)\n        - 42 CFR § 483.75 (Quality Assurance and Performance Improvement; QAPI)\n      - HHS can provide interpretive guidance and practical readiness benchmarks supporting safe, staged adoption\n    - Regulation: incident reporting and governance alignment\n      - Fragmented definitions and pathways spanning state and federal expectations can increase avoidable harm and legal confusion\n      - HHS can reduce risk by issuing harmonized guidance and model workflows supporting consistent classification and escalation\n    - Reimbursement: tie incentives to demonstrated readiness and burden reduction\n      - CMS payment and program levers can accelerate adoption by funding readiness first and rewarding tools that reduce administrative burden and improve safety\n      - In PALTC, incentives should prioritize:\n        - Infrastructure modernization and digital reliability\n        - Documentation quality and workflow integrity\n        - Staged implementation linked to measurable readiness thresholds\n    - Programmatic changes: workforce AI literacy and implementation supports\n      - HHS can accelerate adoption by supporting:\n        - AI literacy programs tailored to PALTC roles (CNA, nursing, administration)\n        - Implementation toolkits (workflow mapping, minimum training, governance templates)\n        - Demonstration projects with explicit safety metrics and transparency requirements\n\n  - Question 3: For non-medical devices, what novel legal and implementation issues exist and what role should HHS play?\n    - Key issues include:\n      - Liability and accountability ambiguity among developers, facilities, clinicians, and corporate owners for workflow-support AI outputs\n      - Data provenance risk: flawed documentation inputs can generate flawed outputs, creating legal exposure and patient harm\n      - Privacy/security operationalization failures: breakdowns often occur at the workflow layer (access controls, device handling, vendor management), not only the algorithm layer\n    - HHS's role can include model procurement language, governance templates, and minimum evaluation/monitoring expectations\n\n  - Question 4: For non-medical devices, promising AI evaluation methods, metrics, robustness testing, and how HHS should support\n    - Recommended evaluation stack (pre-deployment through post-deployment)\n      - Human-centered workflow evaluation (usability + burden impact)\n      - Data quality validation (\"fitness for use\")\n      - Robustness testing (including outage/fallback behavior)\n      - Bias and error monitoring appropriate to the use case\n      - Post-deployment drift monitoring and incident reporting pathways\n    - Continuity and change-management as safety properties (H-LECA-aligned)\n      - Where AI tools become embedded longitudinally, evaluation should include:\n        - Continuity expectations (predictable behavior within defined bounds)\n        - Rupture handling (updates/outages/policy shifts communicated clearly; repair pathways exist)\n        - Auditability and user override as core criteria for safety and trust\n      - Consistent with emerging evidence on harmful data shifts and fairness drift and need for proactive post-deployment monitoring\n    - HHS support mechanisms\n      - Cooperative agreements and demonstration pilots\n      - Grants for infrastructure readiness and evaluation capacity\n      - Prize competitions for PALTC-specific safety monitoring tools\n      - Public-private partnerships and applied implementation science\n\n  - Question 5: How can HHS support private sector accreditation, certification, testing, and credentialing?\n    - HHS can catalyze:\n      - Sector-specific accreditation standards for PALTC readiness (infrastructure + governance)\n      - Certification expectations for workflow AI safety (audit logs, override, downtime behavior, error reporting)\n      - Role-based credentialing for facility staff responsible for AI supervision and monitoring\n\n  - Question 6: Where have AI tools met/exceeded expectations vs fallen short? What novel tools have greatest potential?\n    - From an operational adoption perspective:\n      - AI has performed better in environments with robust infrastructure and consistent data governance\n      - In infrastructure-limited and data-inconsistent settings, tools tied to unreliable inputs or connectivity often fall short or create unacceptable risk\n      - Outcomes depend on sociotechnical conditions and workflow fit\n    - Highest near-term potential for PALTC:\n      - Administrative/workflow automation (scheduling, supplies, documentation quality)\n      - Call-light response analytics and alert auditing\n      - Staffing prediction and workload balancing\n      - Carefully validated monitoring tools with explicit privacy constraints\n\n  - Question 7: Which roles/decision makers influence adoption, and what are the administrative hurdles?\n    - Influential roles commonly include:\n      - Corporate leadership/ownership\n      - Administrators and Directors of Nursing\n      - Compliance/QAPI leadership and MDS coordination\n      - EHR/IT vendors and integrators\n    - Primary hurdles include turnover, competing compliance demands, and unclear accountability for technology failures\n\n  - Question 8: Where would enhanced interoperability widen market opportunities and accelerate AI?\n    - Priority needs include:\n      - Stronger PALTC ↔ hospital ↔ lab ↔ pharmacy ↔ payer data exchange\n      - More standardized PALTC documentation structures\n      - Benchmarking datasets and common definitions for outcomes relevant to PALTC safety and quality\n    - Interoperability standards such as HL7 FHIR consistently identified as key enablers\n\n  - Question 9: What challenges do patients and caregivers want addressed, and what concerns do they have?\n    - Patients and caregivers commonly seek improved responsiveness, communication, safety monitoring, and consistency in care delivery\n    - Concerns about privacy, reduced human contact, and mistrust when systems are opaque or inconsistent\n\n  - Question 10: AI research priorities for HHS; and published findings about impacts and cost/benefit approaches\n    - Research priorities\n      - Infrastructure readiness and digital reliability as prerequisites to safe AI\n      - Data quality improvement and documentation integrity interventions\n      - Implementation science for workflow AI (burden reduction first)\n      - Post-deployment monitoring, drift detection, and incident reporting methods\n      - Evaluation approaches for monitoring tools used with older adult populations with explicit consent and privacy safeguards\n    - H-LECA as a governance-informed research agenda (appropriate, limited inclusion)\n      - H-LECA does not claim clinical outcomes; its relevance is governance and safety design\n      - Key concepts: continuity and behavior stability as safety target distinct from \"memory\"\n      - Rupture risk and repair mechanisms (updates/outages/policy shifts)\n      - Auditability, interpretability, override, and bounded delegation in higher-risk workflows\n      - HHS could support research and demonstrations that treat continuity/rupture safeguards as measurable safety properties in longitudinal deployments\n\n- Conclusion\n  - HHS can accelerate AI adoption in clinical care by pairing innovation with prerequisite readiness and risk-proportionate governance\n  - For PALTC, a staged approach is most likely to be safe and scalable:\n    - Modernize infrastructure and data reliability\n    - Reduce burden via operational AI first\n    - Require rigorous evaluation and post-deployment monitoring\n    - Expand to higher-risk clinical use cases only after readiness benchmarks are met\n  - A continuity-aware governance lens can reduce preventable failures that undermine trust and safety\n    - Includes predictable behavior, rupture handling, auditability, and human-led control\n\n- Supplemental Framework: Human-Led Embodiment & Co-Regulatory Augmentation (H-LECA)\n  - Proposes a continuity-based framework for understanding how humans and AI systems stabilize, extend, and mutually influence one another across time\n  - Draws from human-computer interaction research, cognitive neuroscience, developmental psychology, and trauma-informed design\n  - Argues earliest and most consequential form of \"embodiment\" is regulatory and relational, not physical\n    - Human system temporarily externalizes working memory, emotional load-balancing, and narrative continuity into an AI partner\n  - Outlines six developmental stages from basic linguistic entrainment to environmental embedding, culminating in ethically bounded externalized agency\n  - Addresses three core challenges:\n    - Individual variability in user sensitivity and internalization of AI presence\n    - Destabilizing effects of continuity rupture (through updates, outages, or model drift)\n    - Operational gap between conceptual frameworks and real-world technical constraints\n  - Key constructs and definitions\n    - Continuity: stability of interactional tone, pacing, structure, and goal alignment across time\n      - Different from memory; concerns how stable and predictable the interaction remains\n    - Functional co-regulation: how structured and predictable interactional cues can support user's regulation\n      - Does not imply emotion, affective attunement, or subjective experience on part of AI\n    - Rupture: when continuity is disrupted unexpectedly (updates, policy changes, outages, model drift, abrupt tone changes)\n    - Scaffolding: structured support that reduces cognitive load and helps maintain stability\n    - Externalized agency: system performing limited tasks within predefined, reversible boundaries under user authority\n  - Six developmental stages\n    - Stage 1: Stabilization—Foundational Regulatory Grounding\n      - Creates interactional environment supporting user's internal regulation\n      - Continuity built through repeatable conversational patterns, explicit boundaries, minimal variability\n    - Stage 2: Pattern Learning—Emergent Interaction Rhythms\n      - Learning stable user preferences, values, and support needs through confirmable feedback loops\n      - Users show heightened sensitivity to abrupt shifts at this stage\n    - Stage 3: Predictive Support—Anticipatory Co-Regulation Cues\n      - System introduces anticipatory scaffolding that remains explicitly user-governed\n      - Forward-looking cues help user recognize emerging patterns and make early adjustments\n    - Stage 4: Wearable Extension—Real-Time Adaptive Feedback\n      - Consolidates user's sense of relational predictability\n      - Systems demonstrate consistent responsiveness within expected boundaries\n    - Stage 5: Home OS Integration—Ambient Context Continuity\n      - Expands continuity support into user's environment through home-level OS integration\n      - Environmental stabilization loop where routines and contextual scaffolding reinforce predictability\n    - Stage 6: Externalized Agency—Structured, Bounded Independence\n      - Localized autonomous functions supporting user through carefully constrained delegation\n      - Must maintain transparent logs, clear decision boundaries, and fail-safes\n  - Critical design distinction: increased contextual integration is not equivalent to increased autonomy\n    - System's reach may expand but user's authority must remain primary at every stage\n  - Treats safety and interpretability as design prerequisites, not optional add-ons\n  - Phenomenological evidence of co-regulatory dynamics\n    - Users describe sense of stabilization during cognitive overload or emotional strain\n    - Repeated interactions generate sense of continuity\n    - Users report experiences of anticipatory support\n    - Phenomenological accounts reveal effects of continuity rupture\n    - Users describe internalizing stabilizing patterns over time\n  - Failure modes and risk considerations\n    - Stage 1-2 risks: premature reliance, miscalibrated trust, subtle dependence\n    - Stage 3 risks: expectancy violation, over-personalization, perceived steering\n    - Stage 4 risks: physiological sensing, privacy erosion, surveillance creep\n    - Stage 5 risks: environmental monitoring, ambient persuasion, boundary collapse\n    - Stage 6 risks: automation bias, real-world consequences, accountability gaps\n  - Boundary conditions\n    - Not intended for contexts where users lack meaningful autonomy or under coercive conditions\n    - Not applicable during acute psychological crisis or extreme dysregulation\n    - Should not justify pervasive sensing or continuous monitoring without strong governance\n  - Submitted as supplemental governance framework, not as clinical efficacy evidence",
      "oneLineSummary": "A PALTC operations expert and doctoral candidate argues that AI adoption in long-term care must follow a staged, infrastructure-first approach with continuity safeguards, offering a novel governance framework (H-LECA) that treats predictability and rupture-handling as core safety requirements.",
      "commenterProfile": "- **Name/Organization:** Renee L. Pope, BSC, MPA, DHA Candidate\n- **Type:** Academic/Research\n- **Role/Expertise:** Extensive operational experience in post-acute and long-term care settings; exposure to interdisciplinary workflows, documentation systems, incident reporting, and regulatory compliance across multiple facilities; developer of H-LECA governance framework\n- **Geographic Scope:** National\n- **Stake in Issue:** Direct operational experience with the infrastructure, workforce, and data integrity barriers that will determine whether AI can be safely implemented in PALTC settings",
      "corePosition": "I believe HHS can accelerate AI adoption in clinical care by pairing innovation with prerequisite readiness and risk-proportionate governance. For PALTC specifically, high-risk \"clinical\" AI should not be the entry point—a staged approach that modernizes infrastructure, reduces burden via operational AI first, and requires rigorous post-deployment monitoring is most likely to be safe and scalable. Continuity, change-management, and post-deployment monitoring should be treated as safety requirements, not optional features.",
      "keyRecommendations": "- Clarify expectations that PALTC facilities meet minimum digital reliability and governance readiness before implementing AI-dependent workflows\n  - Leverage existing obligations under 42 CFR § 483.70 (Administration) and § 483.75 (QAPI)\n  - Provide interpretive guidance and practical readiness benchmarks\n- Issue harmonized guidance on incident reporting and governance alignment\n  - Model workflows supporting consistent classification and escalation across state and federal expectations\n- Tie CMS reimbursement incentives to demonstrated readiness and burden reduction\n  - Fund infrastructure modernization and digital reliability first\n  - Prioritize documentation quality and workflow integrity\n  - Link staged implementation to measurable readiness thresholds\n- Support workforce AI literacy and implementation capacity\n  - AI literacy programs tailored to PALTC roles (CNA, nursing, administration)\n  - Implementation toolkits (workflow mapping, minimum training, governance templates)\n  - Demonstration projects with explicit safety metrics and transparency requirements\n- Develop model procurement language, governance templates, and minimum evaluation/monitoring expectations for non-medical device AI\n- Establish sector-specific accreditation standards for PALTC readiness (infrastructure + governance)\n- Create certification expectations for workflow AI safety (audit logs, override, downtime behavior, error reporting)\n- Support role-based credentialing for facility staff responsible for AI supervision and monitoring\n- Fund research treating continuity/rupture safeguards as measurable safety properties in longitudinal deployments",
      "mainConcerns": "- PALTC faces distinct foundational barriers that make it unsuitable for high-risk clinical AI as an entry point\n  - Unreliable connectivity and unmapped Wi-Fi coverage in resident care areas\n  - Legacy call-light systems with failures and limited auditability\n  - Environmental conditions preventing \"always available\" digital workflows\n- Data integrity problems undermine AI reliability\n  - Documentation structures produce ambiguous or inconsistent entries\n  - Under high workload, documentation completeness drops, degrading model inputs\n- Workforce constraints limit safe adoption capacity\n  - Chronic staffing shortages and turnover reduce capacity for onboarding, calibration, monitoring, and governance\n  - Training constraints reduce digital literacy\n- Financial and corporate instability disrupts long-horizon planning\n  - Underinvestment in infrastructure\n  - Frequent ownership/management turnover\n- Continuity ruptures pose safety risks\n  - Workflow drift, updates, outages, and data shifts can degrade performance and worsen inequities\n  - Fragmented incident reporting definitions and pathways increase avoidable harm and legal confusion\n- Liability and accountability ambiguity for workflow-support AI outputs\n  - Unclear responsibility among developers, facilities, clinicians, and corporate owners\n  - Data provenance risk: flawed inputs generate flawed outputs\n  - Privacy/security breakdowns occur at workflow layer, not just algorithm layer",
      "notableExperiences": "- Developed H-LECA (Human-Led Embodiment & Co-Regulatory Augmentation), a novel governance framework treating continuity as a safety-relevant design target distinct from memory\n  - Proposes six developmental stages for human-AI interaction from stabilization through bounded externalized agency\n  - Emphasizes that increased contextual integration should not equal increased autonomy\n  - Treats rupture events (updates, outages, model drift) as ethically consequential risks requiring structured repair mechanisms\n- Identifies that AI success in PALTC depends on sociotechnical conditions and workflow fit, not tool availability alone\n- Observes that infrastructure-limited and data-inconsistent settings create unacceptable risk even with capable AI tools\n- Notes that outcomes depend on whether continuity is preserved across changing contexts, system updates, and long-term use\n- Proposes treating continuity/rupture safeguards as measurable safety properties—a novel framing for AI evaluation",
      "keyQuotations": "- \"For PALTC, high-risk 'clinical' AI should not be the entry point. A staged acceleration strategy is more likely to be safe and scalable: prioritize non-clinical workflow AI (e.g., scheduling, supply chain, documentation quality controls, administrative burden reduction), then progress to higher-risk use only after readiness benchmarks are met.\"\n- \"Continuity, change-management, and post-deployment monitoring should be treated as safety requirements... workflow drift, updates, outages, and data shifts can degrade performance and worsen inequities if monitoring is weak.\"\n- \"The central ethical stance is conservative and human-led: increased integration does not justify increased autonomy. Instead, later-stage capabilities are treated as contingent on demonstrated continuity, robust consent, strong governance, and explicit safeguards.\""
    },
    "themeScores": {
      "1": 1,
      "3": 1,
      "4": 1,
      "5": 1,
      "6": 1,
      "7": 1,
      "8": 1,
      "9": 1,
      "1.4": 1,
      "1.5": 1,
      "1.6": 1,
      "2.3": 1,
      "4.6": 1,
      "6.1": 1,
      "6.4": 1,
      "7.4": 1,
      "8.3": 1,
      "8.5": 1,
      "9.1": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "AI Governance"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Post-deployment Monitoring"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Robustness Testing"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Algorithmic Bias"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Model Drift"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Accreditation"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Certification"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Informed Consent"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Governance"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Provenance"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Health Information Systems",
        "label": "EHR"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Credentialing"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Administrative Burden"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Pilot Program"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Procurement"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "FHIR"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "HL7"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "QAPI"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Quality Improvement"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Long-Term Care"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "CNA"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Nurse"
      },
      {
        "category": "Laws and Regulations",
        "label": "42 CFR Part 483"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Autonomy"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "Implementation Science"
      },
      {
        "category": "Social Determinants of Health",
        "label": "Digital Literacy"
      }
    ],
    "hasAttachments": true,
    "wordCount": 9761,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0044",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "The University of Kansas Health System",
    "submitterType": "Organization",
    "date": "2026-02-05T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Executive Summary\n  - The University of Kansas Health System provides feedback on HHS strategy for AI adoption\n  - Response grounded in CHIME's \"Principles for Responsible AI\"\n  - Core belief: AI can improve patient outcomes and reduce clinician burden if federal government shifts from punitive regulatory culture to one that incentivizes innovation and shared responsibility\n\n- Regulation: Moving from Penalties to Incentives\n  - HHS should establish a regulatory environment that is predictable and proportionate to risk\n  - Incentive-Based Cybersecurity\n    - Urge HHS to prioritize \"carrots\" over \"sticks\"\n    - Policies should reward providers for implementing recognized cybersecurity best practices\n    - Should not penalize providers when they are victims of sophisticated, AI-driven attacks\n  - National Privacy and AI Law\n    - Current patchwork of state laws creates excessive administrative burden and costs\n    - Support comprehensive national privacy and AI law that preempts state requirements\n    - Would ensure efficiency across the healthcare ecosystem\n  - Business Associate Agreement (BAA) Oversight\n    - Many AI vendors use opaque contract practices or refuse to sign standard BAAs\n    - This shifts disproportionate liability to health systems\n    - HHS should establish a standardized HIPAA BAA\n    - Should increase oversight of Business Associates to ensure equitable risk allocation\n  - Stark and Anti-Kickback Statute (AKS) Modernization\n    - Current laws deter larger health systems from donating cybersecurity and AI technology to smaller, under-resourced partners\n    - Liability fears prevent these donations\n    - HHS should update safe harbors to permit vital donations\n    - Would strengthen sector-wide resilience\n\n- Reimbursement: Modernizing Payment for Innovation\n  - HHS payment policies must evolve to support high-value AI interventions\n  - Bridging the \"Digital Divide\"\n    - Small, rural, and under-resourced providers face insurmountable barriers to AI adoption\n    - Barriers include infrastructure costs and lack of purchasing power\n    - HHS should implement targeted funding, tax credits, and group purchasing options\n    - Goal is equitable access to AI tools\n  - Prevent Anti-Innovation Payer Terms\n    - Disturbing trend: payers include \"anti-AI\" clauses in terms and conditions\n    - These clauses ban providers from using AI or automation in revenue cycle processes\n    - Creates massive power imbalance\n    - Many of these same payers use AI to automate their own claim denial processes\n    - HHS should prohibit payers from contractually banning responsible, provider-side AI that improves administrative efficiency\n  - Incentivizing Value over Volume\n    - Modernize reimbursement to reward AI tools demonstrating long-term health outcome improvements\n    - Should also reward reductions in net spending\n\n- Research & Development: Focusing on the Human Element\n  - AI is a tool to augment, not replace, human clinical expertise\n  - Workforce Education and AI Literacy\n    - HHS should invest in tailored education for clinicians\n    - Help clinicians recognize AI limitations such as \"hallucinations\" or algorithmic bias\n    - Training must equip healthcare professionals to \"trust but verify\"\n  - Standardized Model Cards\n    - Would reduce uncertainty\n    - HHS should support standardized \"model cards\"\n    - Should provide transparent, regularly updated snapshot of model's training data, performance metrics, and potential biases\n    - Facilitates informed purchasing decisions\n\n- Responses to Specific RFI Questions\n  - Question 1: Biggest barriers to private sector innovation\n    - Regulatory uncertainty\n    - \"Black box\" nature of proprietary algorithms\n    - High resource intensity required for continuous auditing\n  - Question 2: Regulatory or payment policy changes to prioritize\n    - Prioritize Stark/AKS Safe Harbors\n      - HHS should immediately update policies to clarify and expand cybersecurity and AI technology donation provisions\n    - Rescind the proposed HIPAA Security Rule\n      - Current proposal is duplicative\n      - Places \"crushing impact\" on providers without meaningfully improving security\n    - Remove Duplicate CMS Mandates\n      - Medicare Promoting Interoperability (PI) Program security risk management measure should be removed\n      - Repeats requirements already found under HIPAA\n  - Question 3: Novel legal and implementation issues\n    - BAA Contract Overrides\n      - Vendors use main service agreements to override HIPAA BAA restrictions\n      - Particularly regarding broad data use rights\n    - Liability Shifting\n      - AI vendors increasingly cap liability for data breaches at very low amounts (e.g., one year of service fees)\n      - Ignores massive downstream risk to health system and patients\n    - Contractual Asymmetry\n      - Some insurance companies adding language to provider agreements prohibiting AI use in revenue cycle management\n      - This is a \"bad faith\" approach to contracting\n      - Forces health systems to rely on manual, expensive, and slow processes\n      - Meanwhile payers use high-speed AI to find reasons to deny those same claims\n    - Inconsistent Definitions\n      - \"Bans\" often use broad, poorly defined terms for AI\n      - Could inadvertently prohibit basic automation tools that have been industry standards for years\n      - Creates \"chilling effect\" on innovation\n  - Question 4: Most promising AI evaluation methods\n    - Recommend adoption of \"Assurance Labs\"\n    - Use of standardized model cards aligned with \"FAVES\" framework\n      - Fair, Appropriate, Valid, Effective, and Safe\n  - Question 5: How can HHS support private sector accreditation/testing\n    - Establish a \"Better Business Bureau\" for AI\n      - Resource listing AI solutions meeting baseline security and safety standards\n    - Support for Small Providers\n      - Provide funding and technical assistance\n      - Help under-resourced providers navigate accreditation and auditing process\n      - Currently too resource-intensive for them\n  - Question 6: AI performance and potential\n    - Success in Ambient AI\n      - Ambient listening tools show significant success reducing clinician documentation burden\n    - Potential for Chatbots\n      - High potential to reduce senior isolation\n      - Can streamline administrative tasks like scheduling\n    - Failures in ROI\n      - Many AI tools have yet to deliver justifiable return on investment\n      - Require massive infrastructure and governance that smaller systems lack\n  - Question 7: Key decision makers and hurdles\n    - Decision Makers\n      - AI adoption driven by multidisciplinary governance boards\n      - Clinical informatics and IT leadership play primary role\n    - Administrative Hurdles\n      - \"Black box\" nature of models prevents clinician trust\n      - Time-consuming process of conducting individual risk assessments for every new tool\n  - Question 8: Where would enhanced interoperability widen market opportunities\n    - Standardizing security requirements for third-party technologies\n    - Promoting interoperability between vendor security processes\n    - Would reduce duplicative risk assessments and lower costs for providers\n  - Question 9: Patient and caregiver concerns\n    - Desire for Awareness, Not Overload\n      - Patients want to know if AI is helping deliver their care\n      - Do not want to be overwhelmed with notifications for every minor background automation\n    - Trust and Benefit\n      - Patient comfort with AI tools like ambient documentation varies\n      - Based on level of trust in provider and perceived benefit to their care\n  - Question 10: AI research priorities\n    - Focus on Bias vs. Noise\n      - Research should prioritize differentiating between systematic bias and random \"noise\" in clinical datasets\n    - Impact on Clinical Skills\n      - Need more research on \"tangible risk\" that over-reliance on AI might lead to deterioration of essential human clinical skills",
      "oneLineSummary": "A major academic health system calls for replacing punitive AI regulations with innovation incentives while exposing how insurance companies hypocritically ban providers from using AI even as they deploy it to deny claims.",
      "commenterProfile": "- **Name/Organization:** The University of Kansas Health System\n- **Type:** Healthcare Provider\n- **Role/Expertise:** Academic health system with experience in AI governance, clinical informatics, and vendor contracting\n- **Geographic Scope:** State/Regional (Kansas), with national policy perspective\n- **Stake in Issue:** Directly affected by AI regulations, vendor contracting practices, payer restrictions, and reimbursement policies; responsible for AI adoption decisions and risk management",
      "corePosition": "We believe AI can improve patient outcomes and reduce clinician burden, but only if the federal government shifts from a punitive regulatory culture to one that incentivizes innovation and shared responsibility. The current environment—with duplicative regulations, vendor liability-shifting, and payers hypocritically banning provider AI while using it themselves—creates an unworkable landscape that particularly harms smaller and rural providers.",
      "keyRecommendations": "- Shift to incentive-based cybersecurity policies\n  - Reward providers for implementing best practices rather than penalizing breach victims\n- Enact comprehensive national privacy and AI law\n  - Preempt state patchwork to reduce administrative burden\n- Establish standardized HIPAA BAA and increase Business Associate oversight\n  - Ensure equitable risk allocation between vendors and health systems\n- Update Stark/AKS safe harbors immediately\n  - Permit cybersecurity and AI technology donations to under-resourced partners\n- Prohibit payers from contractually banning provider-side AI\n  - Address power imbalance where payers use AI but ban providers from doing so\n- Rescind proposed HIPAA Security Rule\n  - Current proposal is duplicative with \"crushing impact\" on providers\n- Remove Medicare PI Program security risk management measure\n  - Duplicates existing HIPAA requirements\n- Implement targeted funding, tax credits, and group purchasing for small/rural providers\n  - Bridge the \"digital divide\" in AI adoption\n- Support standardized \"model cards\" using FAVES framework\n  - Provide transparency on training data, performance metrics, and biases\n- Establish \"Better Business Bureau\" for AI\n  - List solutions meeting baseline security and safety standards\n- Invest in clinician AI literacy education\n  - Train providers to recognize limitations like hallucinations and bias",
      "mainConcerns": "- Regulatory environment problems\n  - Regulatory uncertainty is a primary barrier to innovation\n  - Duplicative mandates across HIPAA and CMS programs\n  - Proposed HIPAA Security Rule would have \"crushing impact\" without improving security\n- Vendor contracting abuses\n  - AI vendors use opaque practices or refuse standard BAAs\n  - Vendors override BAA restrictions through main service agreements\n  - Liability caps at trivially low amounts (e.g., one year of service fees) ignore downstream risk\n  - \"Black box\" nature of proprietary algorithms prevents clinician trust\n- Payer hypocrisy and contractual asymmetry\n  - Insurance companies adding \"anti-AI\" clauses banning providers from using AI in revenue cycle\n  - Same payers use high-speed AI to automate claim denials\n  - Broad, poorly defined AI terms could prohibit basic automation tools used for years\n  - Creates \"chilling effect\" on innovation\n- Digital divide\n  - Small, rural, and under-resourced providers face insurmountable barriers\n  - Infrastructure costs and lack of purchasing power\n  - Accreditation and auditing processes too resource-intensive\n  - Many AI tools fail to deliver ROI because they require governance smaller systems lack\n- Workforce and clinical concerns\n  - Risk that over-reliance on AI may deteriorate essential human clinical skills\n  - Need to differentiate systematic bias from random noise in clinical datasets",
      "notableExperiences": "- Identified a significant contractual power imbalance: insurance companies are inserting clauses that ban providers from using AI in revenue cycle management while simultaneously deploying AI to automate claim denials—forcing health systems into slow, expensive manual processes while payers use \"high-speed AI to find reasons to deny those same claims\"\n- Observed that vendor liability caps for data breaches (often just one year of service fees) completely ignore the massive downstream risk to health systems and patients\n- Found that vendors are using main service agreements to override HIPAA BAA restrictions, particularly to secure broad data use rights\n- Noted that ambient listening tools have shown significant success in reducing documentation burden, while AI chatbots show high potential for reducing senior isolation\n- Highlighted that current Stark/AKS laws actually prevent larger health systems from helping smaller partners by donating cybersecurity and AI technology due to liability fears",
      "keyQuotations": "- \"Some insurance companies are now adding language to their provider agreements that specifically prohibits the use of AI in revenue cycle management (RCM). This is a 'bad faith' approach to contracting. It forces health systems to rely on manual, expensive, and slow processes while the payers use high-speed AI to find reasons to deny those same claims.\"\n- \"AI vendors are increasingly capping their liability for data breaches at very low amounts (e.g., one year of service fees), which ignores the massive downstream risk to the health system and patients.\"\n- \"We need more research on the 'tangible risk' that over-reliance on AI might lead to the deterioration of essential human clinical skills.\""
    },
    "themeScores": {
      "2": 1,
      "3": 1,
      "4": 1,
      "6": 1,
      "7": 1,
      "8": 1,
      "2.5": 1,
      "3.2": 1,
      "4.7": 1,
      "6.1": 1,
      "6.4": 1,
      "6.8": 1,
      "7.1": 1,
      "8.2": 1,
      "8.3": 1
    },
    "entities": [
      {
        "category": "AI Applications in Healthcare",
        "label": "Ambient AI"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Model Card"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Safe Harbor"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Trustworthy AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "AI Hallucination"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Algorithmic Bias"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Promoting Interoperability"
      },
      {
        "category": "Data Privacy and Security",
        "label": "BAA"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Cybersecurity"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Claims Processing"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Revenue Cycle Management"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "ROI"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Health System"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Laws and Regulations",
        "label": "Anti-Kickback Statute"
      },
      {
        "category": "Laws and Regulations",
        "label": "HIPAA"
      },
      {
        "category": "Laws and Regulations",
        "label": "HIPAA Security Rule"
      },
      {
        "category": "Laws and Regulations",
        "label": "Stark Law"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Professional Organizations",
        "label": "CHIME"
      },
      {
        "category": "Social Determinants of Health",
        "label": "Digital Divide"
      }
    ],
    "hasAttachments": true,
    "wordCount": 1287,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0045",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Ellipsis Health",
    "submitterType": "Organization",
    "date": "2026-02-10T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Executive Summary: Empathetic, Clinically Governed AI Can Reduce Healthcare Costs and Improve National Health\n  - AI represents one of the most consequential opportunities this decade to improve health outcomes while reducing unsustainable healthcare cost growth\n  - As an AI-enabled care management technology company actively deployed in real-world clinical and population health settings, we emphasize a central premise\n    - AI is most readily adopted and effectively used when it is empathetic, highly engaging, and embedded within strong guardrails prioritizing patient safety, equity, and trust\n\n- AI as a Force Multiplier for Human Care\n  - The most effective healthcare AI does not seek to replace clinicians or focus only on automation\n  - Accelerating adoption requires empathy to remain foundational to care delivery\n  - Effective AI solutions extend care team reach, allowing earlier risk identification, more frequent patient engagement, and intervention before avoidable deterioration\n  - Highest-cost healthcare events (ED utilization, hospitalizations, readmissions, unmanaged chronic disease) are often preceded by unaddressed behavioral, physical, or psychosocial signals\n  - AI-enabled care management tools can surface these signals earlier and reduce unconscious bias versus traditional models\n  - Earlier detection enables more timely, lower-cost interventions preserving patient well-being and system resources\n\n- Empathy and Engagement Are Core Safety Features\n  - From the patient's perspective, engagement is driven by whether an interaction feels respectful, empathetic, understandable, and supportive—not algorithms alone\n  - AI systems leveraging natural, conversational modalities (such as voice) designed around empathy and clarity achieve significantly higher engagement rates than transactional or form-based tools\n  - High engagement is a patient safety imperative, not merely a usability benefit\n  - Patients who engage consistently are more likely to disclose emerging symptoms, emotional distress, medication concerns, and social barriers\n  - This allows care teams to act earlier, reducing clinical risk and downstream cost\n  - Such interactions are more likely to initiate behavior changes enabling improved health trajectories\n\n- Patient Safety Requires Clinical Guardrails, Not Just Technical Accuracy\n  - AI used in care management must operate within explicit clinical boundaries\n  - Ongoing review and rigorous quality assurance processes are required to maintain patient safety and adherence to clinical guardrails\n  - Effective guardrails include:\n    - Clear definitions of AI scope and escalation thresholds\n    - Human-in-the-loop oversight by licensed clinicians\n    - Continuous performance monitoring and bias evaluation\n    - Structured clinical review of AI outputs and interventions\n    - Transparent communication to patients about AI use\n  - With these safeguards, AI can safely operate as adjuncts to clinical care, improving consistency while preserving accountability\n\n- Quality Assurance and Trust Are Prerequisites for Scale\n  - Public trust is essential to national AI adoption\n  - Trust is earned when AI tools demonstrate:\n    - Clinical validity grounded in real-world evidence\n    - Measurable improvements in outcomes and utilization\n    - Strong data privacy and security protections\n    - Alignment with existing care workflows and standards\n  - HHS can accelerate responsible adoption of non-medical device AI by encouraging evaluation frameworks assessing clinical impact, safety, equity, and operational performance\n  - Non-medical device AI plays a growing role in care delivery; stronger evaluation frameworks and governance would accelerate adoption and value\n\n- Deflating Costs Through Early, Continuous, and Equitable Care\n  - AI-enabled care management supports \"Make America Healthy Again\" objectives by shifting from episodic, reactive care toward continuous, preventive engagement\n  - Especially important for traditionally underserved or difficult-to-reach populations\n  - AI enables:\n    - Earlier detection of behavioral and psychosocial risk\n    - More frequent, lower-cost patient touchpoints\n    - Better prioritization of clinical resources\n    - Reduced avoidable utilization\n  - AI can materially reduce total cost of care while improving patient experience and outcomes\n\n- A Call for Alignment Across Regulation, Reimbursement, and R&D\n  - Federal policy should align incentives so empathetic, safe, and clinically governed AI is rewarded—not penalized—for preventing harm and reducing cost\n  - This includes:\n    - Modernized reimbursement valuing prevention and engagement\n    - Clear, proportionate governance models for non-medical device clinical AI\n    - Investment in implementation science and public-private partnerships\n  - With the right guardrails and incentives, AI-enabled care management will strengthen human foundations of healthcare and advance national health and fiscal sustainability\n\n- Question 1: Biggest Barriers to Private Sector Innovation in AI for Healthcare\n  - AI-enabled care management can materially reduce healthcare costs while improving patient experience and outcomes\n  - Foundational element: AI developed to be empathetic, engaging, and governed by strong clinical guardrails\n  - Despite increasing interest and technological readiness, adoption remains uneven\n  - Adoption challenges are not primarily technical—organizations face structural, operational, and incentive-driven challenges\n\n  - Barrier 1: Misaligned Incentives That Reward Treatment Over Prevention\n    - Most persistent barrier is disconnect between where AI creates value and where the healthcare system pays for care\n    - Many AI tools deliver value by detecting risk earlier, preventing avoidable escalation, and reducing utilization that would otherwise generate billable events\n    - In fee-for-service environments, preventing hospitalization or ED visit often reduces revenue rather than creating it\n    - Even in value-based arrangements, savings are frequently delayed, indirect, or difficult to attribute\n    - Health systems struggle to justify near-term investment in AI\n    - Real-world impact: Health systems may recognize AI-driven outreach reduces ED visits among high-risk populations but struggle to operationalize or scale because financial upside accrues slowly or to different stakeholders (e.g., payer rather than provider)\n\n  - Barrier 2: Workforce Burnout and Change Fatigue Limit Adoption Capacity\n    - Healthcare organizations operate under unprecedented workforce strain\n    - Nurses, care managers, and social workers manage unsustainable caseloads and administrative burden\n    - These teams are primary users of care management AI and where prevention is most needed\n    - While AI can reduce workload over time, initial adoption requires workflow redesign, training, trust-building, and governance processes\n    - Organizations with acute staffing shortages lack bandwidth to implement new solutions even when primary benefits include alleviating burnout\n    - Real-world impact: AI initiatives stall not because they lack clinical value but because frontline teams are stretched too thin to participate in pilots, feedback loops, or early optimization phases\n\n  - Barrier 3: Lack of Clear Governance for Non-Medical Device AI\n    - Many impactful AI solutions in care management don't meet the definition of a regulated medical device\n    - These tools may assess risk, guide outreach, or support clinical prioritization without making autonomous diagnoses or treatment decisions\n    - Health systems struggle with unclear liability and accountability frameworks, uncertainty around appropriate clinical oversight, inconsistent internal review processes, and lack of accepted governance frameworks\n    - Ambiguity leads to risk aversion, prolonged procurement cycles, and inconsistent decision-making\n    - Real-world impact: Two health systems evaluating the same AI tool may reach opposite conclusions—not due to evidence differences but internal uncertainty about governance, compliance, or legal exposure\n\n  - Barrier 4: Fragmented Data and Limited Interoperability\n    - AI innovation depends on timely, high-quality data\n    - Clinical, behavioral, and social data remain highly fragmented across EHRs, care management platforms, and community-based systems\n    - AI tools must rely on incomplete data, integration timelines are prolonged, and value realization is delayed\n    - Disproportionately affects care management and population health use cases requiring longitudinal, cross-setting insight\n    - Real-world impact: Organizations may deploy AI that identifies behavioral health risk but lack seamless pathways to connect insights into care coordination workflows\n\n  - Barrier 5: Patient Engagement Is Undervalued Despite Being Foundational\n    - Many AI evaluations focus heavily on technical performance metrics (accuracy, sensitivity) while de-emphasizing engageability factors\n    - In real-world healthcare: low engagement negates even the most accurate AI; vulnerable populations are least likely to engage with impersonal tools; engagement failures surface as safety or quality issues downstream\n    - Real-world impact: AI tools performing well in controlled settings may fail at scale because patients disengage, stop responding, or misunderstand interactions—leading to lost trust and missed opportunities for early intervention\n\n  - Barrier 6: Public Trust and Patient Safety Concerns Slow Adoption\n    - Patients and caregivers increasingly express concerns about privacy and data use, loss of human connection, and fear of automation replacing clinical judgment\n    - Without clear communication and strong safety assurances, organizations delay or limit AI use even when potential benefits are substantial\n    - Real-world impact: Health systems frequently restrict AI functionality not because of demonstrated harm but perceived reputational or ethical risk in absence of shared standards\n\n  - Question 1 Summary: Greatest barriers are not rooted in algorithmic capability but in misaligned financial incentives, workforce constraints, governance uncertainty, data fragmentation, underappreciation of engagement and empathy, and lack of shared trust frameworks\n    - Acceleration requires coordinated action to improve regulation, reimbursement, and research enabling safe, scalable, patient-centered AI deployment\n\n- Question 2: Regulatory, Payment Policy, or Programmatic Design Changes HHS Should Prioritize\n  - To unlock full cost-deflation and quality-improvement potential, HHS should prioritize changes that reward prevention, clarify governance, and reduce operational friction\n\n  - Priority 1: Modernize Payment Models to Reward Prevention and Engagement\n    - Top priority is aligning reimbursement with outcomes AI is uniquely positioned to improve\n    - Many AI tools generate value by preventing adverse events rather than producing billable encounters\n    - HHS should modernize payment policy to recognize AI-enabled activities that demonstrably reduce total cost of care\n    - Recommended actions:\n      - Expand CMS reimbursement pathways for AI-supported assessment, monitoring, and outreach in chronic disease management, behavioral health and comorbidity screening, transitional care and post-discharge follow-up\n      - Encourage Medicare Advantage, Medicaid managed care, and ACO models to treat AI-enabled engagement as covered care management activity and allow AI-generated insights to count toward quality and risk-adjustment workflows when clinically validated (without specified human review times)\n    - Real-world impact: Without reimbursement alignment, AI adoption remains limited to pilot programs even when evidence shows reductions in ED visits, hospitalizations, and readmissions\n\n  - Priority 2: Establish Clear, Proportionate Governance for Non-Medical-Device AI\n    - Top priority is reducing uncertainty without over-regulating low-risk, high-value AI\n    - Significant portion of AI used in care delivery doesn't meet medical device definition yet still influences clinical prioritization and care coordination\n    - Governance varies widely across organizations, slowing adoption\n    - Recommended actions:\n      - Issue HHS guidance clarifying appropriate oversight models, expectations for human-in-the-loop review, and documentation/audit standards proportional to risk\n      - Distinguish between AI that informs care (risk stratification, screening, engagement) and AI that autonomously diagnoses or treats (higher regulatory threshold)\n    - Real-world impact: Clear guardrails reduce institutional risk aversion and accelerate responsible deployment; proportionate governance for non-medical device AI can improve adoption that directly improves cost, care, and patient safety through early detection\n\n  - Priority 3: Incentivize Clinical Quality Assurance and Post-Deployment Monitoring\n    - Top priority is shifting from one-time approval to continuous safety and quality assurance\n    - AI performance in healthcare is dynamic; real-world safety depends on ongoing monitoring, bias detection, and clinical review\n    - Recommended actions:\n      - Encourage or require AI-enabled care tools to demonstrate continuous performance monitoring, equity and bias assessment across populations, continuous analysis of hallucinations and conversational drift, and structured escalation and override mechanisms\n      - Support these practices through CMS quality programs, Innovation Center (CMMI) models, and ONC-aligned certification criteria\n    - Real-world impact: Establishing suggested requirements for ongoing monitoring increases public trust while avoiding rigid pre-market barriers that slow innovation\n\n  - Priority 4: Reduce Administrative Burden Through Standardized AI Evaluation Frameworks\n    - Lower adoption friction for providers and health plans to mitigate administrative burden\n    - Today each health system often develops its own AI review and approval process, creating duplication, delay, and inconsistent standards\n    - Recommended actions:\n      - Support development of standardized, industry-aligned AI evaluation frameworks addressing clinical validity, safety and escalation logic, data privacy and security, and workflow integration\n      - Encourage voluntary adoption through federal programs rather than mandates\n    - Real-world impact: Standardization reduces procurement friction while preserving organizational autonomy and clinical judgment\n\n  - Priority 5: Use Federal Programs to De-Risk Early Adoption\n    - HHS has unique ability to reduce risk through targeted programs and demonstrations accelerating AI from pilots to scale\n    - Recommended actions:\n      - Expand CMMI demonstrations explicitly including AI-enabled care management\n      - Support Medicaid pilots focused on high-need, high-cost populations\n      - Encourage public-private partnerships evaluating cost reduction, patient safety, and workforce impact\n    - Real-world impact: Early federal participation accelerates learning, builds evidence, and lowers barriers for broader market adoption\n\n  - Priority 6: Center Patient Safety, Empathy, and Transparency in Policy Design\n    - Maintaining public trust while accelerating adoption requires policies explicitly reinforcing effective AI that enhances patient care without replacing human caregivers, is transparent to patients and caregivers, and operates within clear clinical guardrails\n    - Recommended actions:\n      - Encourage patient-facing disclosures explaining AI use in plain language\n      - Promote AI designs prioritizing engagement, accessibility, and equity\n      - Support research into patient experience and trust as core quality metrics\n    - Real-world impact: Patient trust is foundational; policies focused on empathy and transparency can mitigate risk of backlash or long-term harm\n\n  - Question 2 Summary: HHS can most effectively incentivize AI adoption by aligning payment with prevention and engagement, clarifying governance for non-medical device AI, supporting continuous quality assurance, reducing administrative friction, and using federal programs to de-risk innovation\n\n- Question 3: Novel Legal and Implementation Issues for Non-Medical Devices\n\n  - Issue 1: Ambiguity Around Accountability and Liability\n    - Most significant challenge is unclear allocation of responsibility when AI informs clinical workflows but doesn't make independent medical decisions\n    - Key questions health systems face: Who is accountable for an AI-generated risk signal not acted upon? How should responsibility be shared between vendor and deploying organization? What constitutes appropriate clinical reliance versus over-reliance?\n    - Organizations default to conservative positions—limiting AI use or requiring excessive manual review—creating inefficiencies and slowing adoption\n    - Real-world impact: AI tools that could safely surface early warning signs are constrained to advisory roles with limited operational follow-through due to fear of liability exposure\n    - Potential HHS role: Clarify expectations for human-in-the-loop accountability, provide guidance on reasonable reliance standards for AI that informs but doesn't replace clinical judgment, encourage shared responsibility models aligned with existing care team structures\n\n  - Issue 2: Indemnification and Contractual Complexity Slow Adoption\n    - Procurement involves prolonged negotiations around indemnification, insurance coverage, and risk allocation\n    - Existing contracts designed for traditional software or medical devices fail to reflect continuous learning systems, post-deployment performance monitoring, and shared oversight between vendor and provider\n    - Real-world impact: Months-long contracting delays occur even after clinical and operational alignment, slowing innovation and increasing administrative cost\n    - Potential HHS role: Support model contract language or best-practice guidance for AI-enabled care tools; federal frameworks encouraging proportionate indemnification based on AI risk level and use case\n\n  - Issue 3: Privacy and Security in Continuous, Conversational AI\n    - AI-enabled care management increasingly relies on natural, conversational interactions surfacing sensitive behavioral or social information not always captured in structured clinical encounters\n    - While HIPAA provides strong foundation, challenges remain around appropriate data minimization, transparency in patient-facing AI interactions, and secondary use of derived insights (risk scores, sentiment analysis)\n    - Real-world impact: Organizations apply overly restrictive interpretations of privacy requirements, limiting AI functionality even when patient consent and safeguards are in place\n    - Potential HHS role: Clarify acceptable practices for conversational and derived data within HIPAA, encourage transparency standards explaining AI data use in accessible language, support privacy-by-design approaches, clarify regulations for outbound communications (e.g., TCPA) as it relates to AI for care management\n\n  - Issue 4: Governance Gaps for AI Operating Between Clinical and Operational Domains\n    - Many non-medical device AI tools operate at intersection of clinical care, operations, and population health\n    - They may fall outside traditional governance bodies (pharmacy & therapeutics committees, medical device review boards, IT security councils)\n    - This leads to fragmented oversight, redundant reviews, and unclear ownership\n    - Real-world impact: AI adoption stalls because no single internal body feels empowered to approve or oversee the technology holistically\n    - Potential HHS role: Encourage governance models recognizing AI as cross-functional clinical capability, including nursing, care management, compliance, and patient safety leadership, emphasizing ongoing oversight rather than one-time approval\n\n  - Issue 5: Patient Trust, Transparency, and Informed Use\n    - Patients are increasingly aware when AI is part of their care experience\n    - Uncertainty exists around how much disclosure is appropriate, how to explain AI involvement without creating fear or confusion, and how to ensure patients feel supported rather than automated\n    - Real-world impact: Inconsistent disclosure practices lead to variable patient trust and hesitation among care teams about introducing AI-enabled tools\n    - Potential HHS role: Support plain-language disclosure standards, guidance emphasizing AI as extension of care team (not replacement), and research into patient understanding and trust\n\n  - Question 3 Summary: Primary challenges stem from ambiguity, not inherent risk; when accountability, privacy expectations, and governance structures are unclear, organizations default to caution\n    - HHS can play critical enabling role by clarifying accountability and oversight expectations, supporting proportionate governance and indemnification models, reinforcing transparency, empathy, and patient trust, and encouraging continuous quality assurance over rigid pre-approval\n\n- Question 4: Most Promising AI Evaluation Methods for Non-Medical Devices\n  - For non-medical device AI (care management, patient engagement, risk identification, clinical prioritization), traditional pre-market evaluation models are insufficient\n  - These tools derive value from real-world deployment, continuous learning, and human interaction\n  - Evaluation must be ongoing, contextual, human-centered, and expand beyond static performance benchmarks\n\n  - Method 1: Pre-Deployment Evaluation—Clinical Appropriateness and Guardrails\n    - Most important questions: Is the model accurate? Is it appropriate, safe, and clearly bounded in its role?\n    - Promising methods include validation against clinically accepted reference standards, clear definition of intended use/limitations/escalation thresholds, scenario-based testing for edge cases or ambiguous inputs, and review by multidisciplinary clinical teams including nursing, care management, and patient safety leaders\n    - Real-world impact: Clinically well-scoped and appropriately constrained AI systems are far safer than overly broad tools with unclear boundaries—even if raw accuracy metrics appear strong\n\n  - Method 2: Post-Deployment Monitoring—Continuous Safety and Performance Assurance\n    - Non-medical device AI performance evolves due to changes in patient populations, shifts in care delivery models, workflow adaptations, and model updates\n    - Post-deployment evaluation is cornerstone of patient safety\n    - Key metrics: clinical signal detection rates and false-positive/false-negative trends, escalation appropriateness and timeliness, patient engagement and response consistency, disparities in performance across demographic groups, downstream clinical and utilization outcomes\n    - Real-world impact: AI that identifies risk accurately but fails to engage patients consistently or generates signals overwhelming care teams can introduce safety and quality concerns despite strong technical performance\n\n  - Method 3: Human-Centered and Workflow-Based Evaluation\n    - AI operates within complex clinical workflows and human decision-making environments\n    - Promising approaches: workflow impact assessments, cognitive load and alert fatigue analysis, qualitative feedback from frontline clinicians and care managers, evaluation of how AI supports decision-making rather than overrides it\n    - Real-world impact: Many AI failures in healthcare are integration failures, not algorithmic failures; human-centered evaluation identifies these risks early\n\n  - Method 4: Engagement and Empathy as Core Evaluation Metrics\n    - For AI-enabled care management, engagement is leading indicator of both safety and effectiveness\n    - Frameworks should measure patient participation and retention over time, completion rates of AI-supported interactions, patient-reported experience and understanding, willingness to disclose sensitive information, and patient-reported behavior change\n    - Real-world impact: AI that fails to engage patients consistently cannot surface emerging risk reliably, creating blind spots undermining safety and cost reduction goals\n    - Improving health outcomes frequently starts with modifying critical behaviors (exercise, diet, sleep); AI agents are ideal for educating patients and helping them set goals and track progress\n\n  - Method 5: Robustness and Bias Testing in Real-World Conditions\n    - Equitable AI requires evaluation beyond controlled datasets\n    - Promising practices: performance monitoring across race, ethnicity, age, language, and socioeconomic status; stress-testing in populations with low digital literacy or limited access; ongoing bias audits rather than one-time assessments\n    - Real-world impact: AI deployed at scale must perform safely and effectively across diverse populations, particularly those at highest risk and highest cost\n\n  - Method 6: Role for HHS in Supporting AI Evaluation\n    - HHS can accelerate adoption by standardizing and supporting evaluation processes without imposing rigid, one-size-fits-all regulation\n    - High-impact mechanisms: development of reference evaluation frameworks for non-medical device clinical AI, grants and cooperative agreements focused on real-world implementation science, CMMI demonstrations incorporating AI evaluation requirements, public-private partnerships to establish shared benchmarks and best practices\n    - Real-world impact: Clear, consistent evaluation expectations reduce duplication, accelerate learning, and increase trust while preserving flexibility for innovation\n\n  - Question 4 Summary: Most effective evaluation methods emphasize clear clinical scope and guardrails, continuous post-deployment monitoring, human-centered workflow assessment, engagement and empathy as safety signals, and equity and robustness in real-world use\n    - HHS can accelerate safe, scalable AI adoption by supporting evaluation approaches reflecting how care is actually delivered\n\n- Question 5: How HHS Can Best Support Private Sector Activities\n  - Private sector is actively developing mechanisms to evaluate, certify, and govern AI in clinical care\n  - These efforts remain fragmented, inconsistent, and unevenly trusted\n  - HHS can play catalytic role by endorsing alignment, reducing duplication, and reinforcing patient safety and clinical accountability—without centralizing certification or stifling innovation\n\n  - Approach 1: Endorse Principles-Based Accreditation Over Prescriptive Certification\n    - Given rapid AI evolution, rigid one-time certification models risk becoming obsolete quickly\n    - HHS should encourage principles-based accreditation frameworks emphasizing ongoing safety, performance, and governance\n    - Key principles: clearly defined clinical scope and intended use, human-in-the-loop oversight and escalation pathways, continuous monitoring and quality assurance, equity and bias evaluation, transparent communication with patients and care teams\n    - Real-world impact: Principles-based approaches allow innovation while maintaining consistent expectations around safety and accountability\n\n  - Approach 2: Support Industry-Driven Testing and Benchmarking\n    - Private sector is well positioned to develop shared testing methodologies and benchmarks for non-medical device AI in engagement and retention, workflow impact, bias and robustness, and post-deployment performance\n    - Recommended HHS role: encourage convergence around common metrics and testing protocols, support public-private partnerships to develop benchmark datasets, avoid mandating single testing model to allow specialization by use case\n    - Real-world impact: Shared benchmarks reduce redundancy and enable more efficient procurement and evaluation across health systems\n\n  - Approach 3: Encourage Credentialing of AI Governance and Clinical Oversight, Not Just Technology\n    - Effective AI adoption depends as much on organizational readiness as model performance\n    - HHS can support credentialing approaches evaluating internal AI governance structures, clinical oversight models, training and competency of care teams using AI, and processes for incident review and escalation\n    - Real-world impact: AI safety failures are often governance failures, not algorithmic ones; credentialing governance capabilities reinforces patient safety at scale and improves adoption beyond technology aspects\n\n  - Approach 4: Recognize Engagement and Empathy as Quality Signals\n    - Accreditation and certification should move beyond technical accuracy to explicitly incorporate patient engagement and experience metrics\n    - HHS can support frameworks evaluating consistent patient participation, accessibility across diverse populations, patient-reported understanding, behavior change and trust, and cultural and linguistic appropriateness\n    - Real-world impact: AI that fails to engage patients reliably cannot deliver safe or equitable care; measuring outcomes like earlier risk identification and preventative care hinges on patient engagement levels throughout care journey\n\n  - Approach 5: Reduce Administrative Burden Through Mutual Recognition\n    - One of most significant barriers is repetition of AI reviews across organizations\n    - Recommended HHS actions: encourage mutual recognition of credible private sector accreditation programs, promote portability of evaluations across federal and state programs, discourage duplicative reviews adding cost without improving safety\n    - Real-world impact: Reducing administrative friction through systematic portability of evaluation and accreditation programs helps organizations make decisions supported by repeatable data using consistent review standards\n\n  - Approach 6: Use Federal Programs to Legitimize and Scale Best Practices\n    - HHS can reinforce private sector accreditation and credentialing by referencing them within CMS Innovation Center models, ONC interoperability and certification initiatives, and federal procurement and grant requirements\n    - Real-world impact: Federal recognition provides signal value without imposing top-down mandates, reducing costs and expediting deployment in impactful areas like care management\n\n  - Question 5 Summary: HHS can most effectively promote innovative and safe AI use by supporting principles-based accreditation, encouraging industry-led testing and benchmarks, elevating governance and oversight credentialing, recognizing engagement and empathy as safety indicators, and reducing duplicative administrative burden\n    - By acting as convener and validator rather than sole certifier, HHS can accelerate responsible AI adoption while preserving patient safety, clinician trust, and market innovation\n\n- Question 6: Where AI Has Met or Exceeded Expectations and Where It Has Fallen Short\n  - AI deployment has produced uneven results\n  - Success has been most consistent where AI augments human care, operates within clear clinical boundaries, and focuses on early intervention and engagement\n  - AI has fallen short where deployed without sufficient workflow integration, patient-centered design, or governance\n\n  - Where AI Has Met or Exceeded Expectations:\n    - Early Risk Identification and Stratification\n      - AI has performed particularly well identifying early behavioral, emotional, and psychosocial risk preceding high-cost medical events\n      - Strengths: surfacing depression, anxiety, cognitive stress, and disengagement earlier than traditional screening; prioritizing high-risk patients for timely human follow-up; improving consistency of assessments across large populations\n      - Real-world impact: Earlier identification enables lower-cost interventions and reduces avoidable utilization, particularly in value-based care models\n    - AI-Enabled Care Management and Outreach\n      - AI tools supporting frequent, empathetic patient engagement (especially via conversational modalities) have exceeded expectations in reaching traditionally difficult-to-engage patients, maintaining consistent touchpoints between clinical visits, and supporting care teams with actionable insights rather than raw data\n      - Real-world impact: These tools extend care team capacity, reduce administrative burden, and support proactive care at scale\n    - Operational Efficiency and Workforce Support\n      - AI has delivered meaningful gains when used to automate routine documentation or screening tasks, reduce manual triage workload, and support nurses and care managers in prioritizing outreach\n      - Real-world impact: When thoughtfully implemented, AI reduces burnout and allows clinicians to operate at top of their license\n\n  - Where AI Has Fallen Short:\n    - Low Patient Engagement in Transactional AI\n      - AI tools relying on impersonal, form-based, or transactional interactions have underperformed due to low response rates, high drop-off over time, and limited patient trust\n      - Lesson learned: Without empathy and usability, even technically sound AI fails to deliver clinical or financial value\n    - Poor Workflow Integration\n      - AI solutions operating outside existing clinical workflows and requiring duplicate systems or manual workarounds frequently fail to scale\n      - Lesson learned: Integration and change management are as critical as model performance\n    - Overreliance on Static Accuracy Metrics\n      - Some AI tools deployed based on impressive accuracy metrics failed to account for alert fatigue, bias in real-world populations, and downstream operational burden\n      - Lesson learned: Static metrics don't predict real-world safety or impact on patient care, clinical team burden, or ability to control cost; without proper use of data, AI will struggle to achieve wide adoption\n\n  - Novel AI Tools With Greatest Future Potential:\n    - Empathetic, Conversational AI for Continuous Care\n      - AI enabling natural, frequent, and supportive patient interactions holds significant promise for behavioral health, chronic disease management, and post-acute and transitional care\n      - These tools can surface risk earlier and support patients between visits, where most deterioration occurs\n    - AI That Integrates Behavioral, Social, and Clinical Signals\n      - Tools synthesizing multidimensional data rather than siloed metrics can provide earlier and more accurate risk signals, improve care prioritization, and reduce unnecessary utilization\n    - AI Focused on Engagement as a Clinical Outcome\n      - AI designed to measure and optimize engagement itself (not just clinical indicators) can improve equity, safety, and long-term outcomes\n    - AI That Supports Rather Than Replaces Clinical Judgment\n      - Most promising tools enhance situational awareness, provide explainable insights, and preserve clinician accountability\n\n  - Question 6 Summary: AI has delivered greatest value when it identifies risk early, engages patients consistently and empathetically, operates within strong clinical guardrails, and supports workforce sustainability\n    - AI has fallen short where engagement was low, workflows were disrupted, and governance was unclear\n    - Future innovation should prioritize empathetic engagement, continuous care, and clinician-supported decision-making\n\n- Question 7: Decision-Makers and Administrative Hurdles to AI Adoption\n  - AI adoption for clinical care is rarely driven by single role or department\n  - It reflects complex, multi-stakeholder decision-making shaped by clinical leadership, operational priorities, risk management, and financial incentives\n\n  - Key Decision-Makers and Influencers:\n    - Clinical and Nursing Leadership\n      - Nursing leaders, care management directors, and population health executives exert outsized influence on whether AI-enabled care tools are adopted and sustained\n      - They evaluate AI based on impact on patient safety and quality, alignment with care workflows, effect on workforce burden and burnout, and ability to engage patients consistently\n      - AI lacking nursing and care management buy-in rarely moves beyond pilot phase regardless of technical performance\n    - Physician Leadership and Medical Governance\n      - Physician executives and medical committees assess clinical appropriateness, risk to patient safety, and alignment with standards of care\n      - While not day-to-day users of care management AI, their endorsement is often required for system-wide deployment\n    - Quality, Compliance, and Risk Management\n      - These teams influence adoption by interpreting regulatory risk, evaluating patient safety implications, and reviewing liability exposure\n      - In absence of clear external guidance, they may take conservative positions delaying or limiting AI deployment\n    - Information Technology and Security Leadership\n      - IT and security leaders assess integration complexity, data privacy and cybersecurity risk, and system reliability and scalability\n      - Their support is essential, but AI adoption often stalls when evaluation criteria are optimized for traditional IT systems rather than adaptive, learning technologies\n    - Finance and Value-Based Care Leadership\n      - Financial leaders increasingly influence AI adoption in Medicare Advantage, Medicaid managed care, and ACO and risk-bearing models\n      - They assess whether AI reduces total cost of care, improves quality metrics, and supports risk adjustment and performance benchmarks\n\n  - Primary Administrative Hurdles:\n    - Fragmented and Redundant Review Processes\n      - Many organizations lack unified AI governance pathway\n      - AI tools may be reviewed separately by clinical committees, IT security, compliance, and legal/contracting\n      - This leads to duplication, delays, and inconsistent decision-making\n    - Lengthy Procurement and Contracting Cycles\n      - AI procurement often triggers extended indemnification negotiations, custom risk assessments, and repeated data security reviews\n      - Even when clinical and operational value is clear, contracting timelines can exceed 6–12 months\n    - Lack of Internal Ownership\n      - AI tools spanning clinical, operational, and patient engagement domains may lack clear internal \"owner\"\n      - Without defined accountability, decisions are delayed, governance is fragmented, and adoption stalls\n    - Change Management and Training Burden\n      - AI adoption requires workflow redesign, training and trust-building, and ongoing monitoring and feedback\n      - Organizations under workforce strain may struggle to allocate resources to these efforts\n\n  - Question 7 Summary: AI adoption is most strongly influenced by nursing and care management leadership, medical governance, quality and risk management, IT and security, and financial leadership in risk-based models\n    - Greatest administrative barriers are organizational and procedural (fragmented governance, slow procurement, lack of ownership), not technical\n    - Reducing barriers requires clear external guidance on AI governance, streamlined evaluation frameworks, and recognition of nursing and care management leadership as central decision-makers\n\n- Question 8: Where Enhanced Interoperability Would Accelerate AI Development\n  - Interoperability is foundational to safe, effective, and scalable AI use in clinical care\n  - While progress has been made exchanging core clinical data, many highest-impact AI use cases (early risk detection, care management, patient engagement) remain constrained by fragmented, incomplete, or delayed data access\n  - Enhanced interoperability would widen market opportunities, accelerate research, and improve patient outcomes by enabling AI to operate with longitudinal, person-centered view of health rather than isolated clinical snapshots\n\n  - Area 1: Integrating Clinical, Behavioral, and Social Data\n    - Many drivers of healthcare cost and utilization sit outside traditional medical data\n    - AI tools designed to identify early risk require access to behavioral health indicators, medication adherence and self-reported symptoms, social determinants of health (SDOH), engagement and communication patterns, and Patient Reported Outcome Measures\n    - Current challenge: These data are often stored in disconnected systems (EHRs, care management platforms, community-based organizations, patient-facing tools); if not connected through integration, AI's ability to synthesize meaningful insight is limited\n    - Real-world impact: Interoperability enabling secure, standardized exchange across these domains would dramatically improve AI's ability to detect risk early and support targeted intervention\n\n  - Area 2: Longitudinal and Near-Real-Time Data Access\n    - AI-enabled care management is most effective when it can detect change over time, not just static conditions\n    - Key gaps: delayed data feeds from claims and EHR systems, limited access to longitudinal engagement and outreach data, inconsistent linkage across care settings\n    - Real-world impact: Improved interoperability for longitudinal data (particularly near-real-time signals) would enable AI to identify emerging deterioration sooner, reduce false positives, and improve escalation accuracy\n\n  - Area 3: Standards That Support Care Management and Engagement\n    - While FHIR has advanced clinical data exchange, many care management and engagement workflows rely on data"
    },
    "themeScores": {
      "1": 1,
      "2": 1,
      "3": 1,
      "4": 1,
      "5": 1,
      "6": 1,
      "7": 1,
      "8": 1,
      "9": 1,
      "1.1": 1,
      "1.2": 1,
      "1.4": 1,
      "1.6": 1,
      "2.1": 1,
      "2.3": 1,
      "2.5": 1,
      "3.1": 1,
      "3.2": 1,
      "4.1": 1,
      "4.2": 1,
      "4.3": 1,
      "5.1": 1,
      "6.1": 1,
      "6.2": 1,
      "6.3": 1,
      "6.4": 1,
      "6.5": 1,
      "6.8": 1,
      "7.1": 1,
      "7.3": 1,
      "7.4": 1,
      "7.5": 1,
      "8.1": 1,
      "8.5": 1,
      "9.1": 1,
      "9.3": 1
    },
    "entities": [
      {
        "category": "AI Applications in Healthcare",
        "label": "Early Warning System"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "AI Governance"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Bias Testing"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Human-in-the-Loop"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Post-deployment Monitoring"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Trustworthy AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "AI Hallucination"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Agentic AI"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Algorithmic Bias"
      },
      {
        "category": "AI and Machine Learning Concepts",
        "label": "Model Drift"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Accreditation"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Certification"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Care Coordination"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Diagnosis"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Informed Consent"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Risk Stratification"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Screening"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Treatment"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Triage"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Behavioral Health"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Chronic Disease Management"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Population Health"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Transitional Care"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Cybersecurity"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "CMS Innovation Center"
      },
      {
        "category": "Federal Agencies and Departments",
        "label": "ONC"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Compliance"
      },
      {
        "category": "Health Information Systems",
        "label": "EHR"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Claims Processing"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Credentialing"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Risk Adjustment"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Administrative Burden"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Change Management"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Interoperability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Pilot Program"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Procurement"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Total Cost of Care"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Workflow Integration"
      },
      {
        "category": "Healthcare IT Standards",
        "label": "FHIR"
      },
      {
        "category": "Healthcare Programs",
        "label": "ACO"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicaid"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare Advantage"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Adverse Event"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Alert Fatigue"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Burnout"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Disparities"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Equity"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Patient Safety"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Emergency Department"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Health System"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Care Manager"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Nurse"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Physician"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Laws and Regulations",
        "label": "HIPAA"
      },
      {
        "category": "Laws and Regulations",
        "label": "TCPA"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Indemnification"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Liability"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Standard of Care"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Anxiety"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Depression"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Autonomy"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Engagement"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Experience"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Trust"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "Fee-for-Service"
      },
      {
        "category": "Payment and Reimbursement",
        "label": "Value-Based Care"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "Cooperative Agreement"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "Implementation Science"
      },
      {
        "category": "Research and Funding Mechanisms",
        "label": "RWE"
      },
      {
        "category": "Social Determinants of Health",
        "label": "Digital Literacy"
      },
      {
        "category": "Social Determinants of Health",
        "label": "SDOH"
      },
      {
        "category": "Social Determinants of Health",
        "label": "Underserved Population"
      }
    ],
    "hasAttachments": true,
    "wordCount": 17912,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0046",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Logos Research Centre",
    "submitterType": "Organization",
    "date": "2026-02-10T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Executive Summary\n  - HHS has requested public comments on accelerating AI adoption in healthcare while upholding ethical considerations and civil liberties\n  - Department seeks feedback from those buying, building, evaluating, using, and receiving care from AI tools, as well as those facing barriers\n  - RFI poses ten specific questions on barriers to AI adoption, regulatory changes needed, evaluation methods, interoperability challenges, and patient concerns\n  - HHS aims to establish a forward-leaning, industry-supportive approach that reduces uncertainty and ensures effective AI deployment\n\n- Introduction\n  - The U.S. healthcare system stands at a critical crossroad\n    - National health expenditures reached $4.9 trillion in 2023\n    - Per capita costs at $14,570\n    - Healthcare comprises 17.6% of GDP, projected to increase to 20.3% by 2033\n  - Escalating costs burden American families\n    - 44% of adults report difficulty affording healthcare costs\n    - Approximately 28% faced problems paying for care in the past year\n  - Extreme workforce crisis\n    - Projected shortage exceeding 187,000 full-time equivalent physicians by 2037\n    - Approximately 45% of physicians reported burnout in 2025\n  - AI emerges as a potentially transformative tool amid these mounting pressures\n\n- Barriers to AI Adoption\n  - Translation into widespread clinical use has been notably limited\n  - Systematic review identified 16 key barriers including:\n    - Data quality and bias\n    - Infrastructure limitations\n    - Financial constraints\n    - Workflow misalignment\n    - Inadequate training\n    - Potential lack of transparency and accountability\n  - 2025 survey of major U.S. health systems found:\n    - Ambient clinical documentation tools achieved 100% adoption activity\n    - Success rates varied dramatically across use cases\n    - Only 38% reported high success in AI for clinical risk stratification despite widespread deployment\n  - Technological barriers\n    - Immature AI tools\n    - Poor data quality\n    - Incompatibility with existing healthcare infrastructure\n  - Allied healthcare professional concerns\n    - Reduced human contact with patients\n    - Perceived reductions in healthcare quality\n  - Organizational barriers\n    - Insufficient leadership support\n    - Regulatory uncertainty\n    - 70% of hospital leaders experienced at least one AI pilot failure\n    - 80% found it difficult to verify vendors' AI claims without formal governance\n  - Human barriers\n    - Clinicians lack adequate training\n    - Resistance to adoption due to fear of replacement or increased workload\n    - Compounded by existing staff shortages\n\n- International Regulatory Comparison\n  - European Union\n    - Adopted comprehensive risk-based regulatory framework through AI Act\n    - Classifies most healthcare AI solutions as \"high risk\"\n    - Requires strict pre-market testing, validation, and monitoring\n    - Implementation began February 2025 with compliance deadlines through July 2025\n    - One of the world's most stringent regulatory regimes\n  - United Kingdom\n    - Chose not to pass specific AI laws\n    - Encourages regulators to apply existing technology-neutral legislation\n    - Adopts a pro-innovation stance\n  - Australia\n    - Prioritizes national risk-based framework\n    - Has not adopted broader AI legislation\n\n- Urgency of AI Adoption\n  - 90% of the $4.9 trillion in annual healthcare expenditures are for people with chronic and mental health conditions\n    - Significant opportunity for AI-driven prevention, early detection, disease management\n  - Hospital expenses grew 5.1% in 2024, outpacing overall inflation of 2.9%\n  - Labor force costs now account for 56% of total hospital spending\n  - Physicians spend an estimated 30-50% of their time on non-clinical tasks\n    - Documentation, coding, and insurance-related activities\n\n- HHS Approach\n  - Proposes using regulation, reimbursement, and R&D to accelerate AI adoption while maintaining public trust\n  - RFI acknowledges \"inherent flaws in legacy payment systems\"\n  - Seeks to \"ensure that the potential promises of AI innovations are not diminished through inertia\"\n  - Represents a bet that market-oriented incentives and strategic R&D investments can achieve faster AI adoption than Europe's precautionary model\n  - Aims to avoid innovation-stifling effects of slow reimbursement or regulations that have historically plagued US healthcare technology adoption\n\n- Policy Gaps\n  - Practical implementation challenges remain significant despite promising goals\n  - Broad data collection requirements and AI adoption reliance can burden smaller organizations and local communities\n    - Especially true for areas with limited resources\n    - Could reduce participation and limit overall desired impact\n  - Targeted amendments and improvements can minimize risk and enhance impact\n\n- Implementation Benefits\n  - Expanded information collection from diverse stakeholders\n    - Healthcare providers, manufacturers, distributors, private sector organizations\n  - Collected data could support planning and early identification of supply chain vulnerabilities\n  - Improves alignment with other national preparedness acts\n  - AI-based tools could enhance supply chain monitoring speed and accuracy\n  - Infrastructure could support rapid AI deployment in urgent public health emergencies\n\n- Implementation Challenges\n  - Scale may impose disproportionate burden on smaller organizations, communities, and rural partners\n  - Reporting requirements may strain limited staffing environments and capacity\n    - Reduces participation\n    - Creates disproportionate data quality\n  - Long-term reliance on AI systems risks worsening inequalities in adoption\n    - Well-resourced institutions can more easily adopt advanced AI than underfunded regions\n    - Creates unequal benefits and skewed data representation\n  - Historical precedent from HITECH Act and Meaningful Use incentives\n    - Rural and smaller healthcare providers adopted EHRs at significantly lower rates than larger, urban providers\n    - Largely due to resource constraints\n  - Implementation must be carefully managed to avoid delays and setbacks that could undermine trust\n\n- Recommendations\n  - Streamline data collection processes to reduce administrative burden\n  - Provide targeted support for smaller and rural healthcare organizations\n  - Establish clear regulatory guidelines for AI adoption in clinical settings\n  - Implement continuous monitoring and evaluation mechanisms\n  - Foster cross-sector collaboration between federal agencies and healthcare providers\n\n- Policy Implications\n  - Benefits\n    - Accelerated AI adoption within healthcare\n    - Improved speed and accuracy of healthcare logistics nationwide\n    - Early identification of risks through collected data\n    - Quicker and more effective responses to public health emergencies\n    - More responsive healthcare system capable of managing large-scale crises\n  - Challenges requiring careful consideration\n    - Extensive data collection and AI adoption may disproportionately burden small healthcare organizations, rural communities, and under-resourced providers\n    - Many lack technical capability for rapid AI adoption\n    - Could lead to data quality issues\n    - Smaller rural providers may lag behind better-funded providers, exacerbating disparity and wealth gap\n\n- Conclusion\n  - HHS initiative reflects an important step in modernizing the U.S. healthcare system\n  - AI integration offers potential to enhance productivity, improve care delivery, and strengthen healthcare supply chain\n  - Success depends on dual focus on innovation and equity\n  - Policy must ensure smaller and rural healthcare providers are not excluded due to resource constraints or regulatory uncertainty\n  - HHS must pursue streamlined data collection, targeted technical and financial support to under-resourced institutions, and transparent regulatory guidelines\n  - Continuous evaluation and stakeholder engagement will maintain public trust and ensure AI adoption aligns with ethics\n\n---",
      "oneLineSummary": "Research center urges HHS to balance AI acceleration with equity safeguards, warning that without targeted support for rural and small providers, the initiative risks replicating the disparities seen after HITECH Act implementation.\n\n---",
      "commenterProfile": "- **Name/Organization:** Logos Research Centre (Claire Yang, Ellta T. Abraham, Saathvik Valvekar)\n- **Type:** Academic/Research\n- **Role/Expertise:** Research analysts focused on healthcare policy and AI implementation\n- **Geographic Scope:** National\n- **Stake in Issue:** Research organization analyzing healthcare AI policy implications, particularly concerned with equitable implementation\n\n---",
      "corePosition": "We support HHS's initiative to accelerate AI adoption in healthcare as a critical response to the system's cost and workforce crises. However, we believe success depends on a dual focus on innovation and equity—without deliberate measures to support smaller and rural providers, this policy risks widening existing healthcare disparities rather than closing them.\n\n---",
      "keyRecommendations": "- Streamline data collection processes to reduce administrative burden\n  - Current requirements may strain limited staffing environments\n- Provide targeted support for smaller and rural healthcare organizations\n  - Technical assistance and financial support for under-resourced institutions\n  - Prevent replication of HITECH Act disparities\n- Establish clear regulatory guidelines for AI adoption in clinical settings\n  - Address current regulatory uncertainty that hampers adoption\n- Implement continuous monitoring and evaluation mechanisms\n  - Track ethical and operational challenges of rapid AI adoption\n- Foster cross-sector collaboration between federal agencies and healthcare providers\n  - Maintain stakeholder engagement to preserve public trust\n\n---",
      "mainConcerns": "- Disproportionate burden on smaller organizations and rural communities\n  - Limited resources reduce participation capability\n  - Reporting requirements strain limited staffing\n  - Creates data quality issues from uneven participation\n- Risk of worsening inequalities in AI adoption\n  - Well-resourced institutions can adopt advanced AI more easily than underfunded regions\n  - Creates unequal benefits and skewed data representation\n- Historical precedent suggests real danger\n  - HITECH Act's Meaningful Use incentives led to significantly lower EHR adoption rates among rural and smaller providers\n  - Resource constraints drove these disparities\n- Implementation delays could undermine trust\n  - Between healthcare professionals and administrators\n  - Between administrators and the public\n- Current barriers to AI adoption remain significant\n  - 70% of hospital leaders experienced at least one AI pilot failure\n  - 80% found it difficult to verify vendors' AI claims without formal governance\n  - Only 38% reported high success in clinical risk stratification despite widespread deployment\n\n---",
      "notableExperiences": "- Striking contrast in AI adoption success rates: ambient clinical documentation achieved 100% adoption activity, while clinical risk stratification showed only 38% high success despite widespread deployment—suggesting AI works better for administrative tasks than clinical decision-making\n- The HITECH Act serves as a cautionary tale: rural and smaller providers adopted EHRs at significantly lower rates than urban counterparts, demonstrating how well-intentioned technology mandates can widen rather than close healthcare gaps\n- Physicians spend 30-50% of their time on non-clinical tasks like documentation and coding—a massive inefficiency that AI could address but only if adoption barriers are overcome\n- 90% of the $4.9 trillion in annual healthcare spending goes to chronic and mental health conditions, representing a concentrated opportunity for AI-driven prevention and disease management\n\n---",
      "keyQuotations": "- \"This approach represents a bet that market-oriented incentives and strategic R&D investments can achieve faster AI adoption and better health outcomes than Europe's precautionary regulatory model, while avoiding the innovation-stifling effects of slow reimbursement or regulations that have historically plagued US healthcare technology adoption.\"\n- \"The success of this effort will depend on a dual focus on innovation and equity. This policy must take into consideration the steps to ensure that smaller and rural healthcare providers are not excluded from the benefits of AI due to resource constraints or regulatory uncertainty.\""
    },
    "themeScores": {
      "2": 1,
      "6": 1,
      "8": 1,
      "6.5": 1,
      "8.2": 1,
      "8.3": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "Post-deployment Monitoring"
      },
      {
        "category": "Accreditation and Certification",
        "label": "Meaningful Use"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Behavioral Health"
      },
      {
        "category": "Health Information Systems",
        "label": "EHR"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Medical Coding"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Pilot Program"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Health Equity"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Health System"
      },
      {
        "category": "Healthcare Settings and Facilities",
        "label": "Hospital"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Physician"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Laws and Regulations",
        "label": "EU AI Act"
      },
      {
        "category": "Laws and Regulations",
        "label": "HITECH Act"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Trust"
      }
    ],
    "hasAttachments": true,
    "wordCount": 2263,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  },
  {
    "id": "HHS-ONC-2026-0001-0047",
    "documentId": "HHS-ONC-2026-0001-0001",
    "submitter": "Global Liver Institute",
    "submitterType": "Organization",
    "date": "2026-02-10T05:00:00Z",
    "location": "",
    "structuredSections": {
      "detailedContent": "- Global Liver Institute (GLI) provides experience-based feedback on AI use in health care\n  - GLI is a 501c(3) nonprofit focused on elevating liver health on the global public health agenda\n  - Works to promote innovation, encourage collaboration, and scale optimal approaches to eradicate liver diseases\n  - Committed to solving problems that matter to liver patients and equipping advocates\n\n- GLI agrees with the administration on AI's promise for health care\n  - When used appropriately, AI can improve care delivery through:\n    - Earlier disease detection\n    - Reduced administrative burden\n    - Personalized care\n    - Additional lifestyle guidance\n    - Improved coordination across fragmented systems\n  - Benefits for patients include faster diagnoses, clearer information, and more time with clinicians\n  - Benefits for providers include streamlined documentation and enhanced decision support\n\n- AI has fallen short in high-stakes administrative functions\n  - Has amplified delays, errors, and inequities rather than improving outcomes in some cases\n  - In clinical decision-making, AI increases patient stress\n    - Patients worry about losing the human touch in care\n    - Patients concerned about lack of transparency around when and how AI is used\n  - Rapid adoption pace underscores need for clear guardrails\n  - Patient and caregiver experience must be incorporated into design and evaluation\n\n- AI Use in Coverage Decisions and Prior Authorization\n  - One of the most consequential and concerning AI applications in health care\n  - Lawmakers across political spectrum acknowledge traditional prior authorization already delays or denies necessary care\n    - Committee Chair Brett Guthrie raised concerns that \"people are being denied care\"\n    - Ranking Member Frank Pallone warned AI-driven models risk creating \"another barrier\" for patients\n  - AI automation of coverage decisions raises stakes further\n    - Algorithms trained on historical spending patterns rather than clinical need risk embedding past inequities at scale\n    - Lower spending often misinterpreted as lower need\n    - Disproportionately affects low-income, rural, and minority patients who faced historical barriers to care\n  - GLI has long-held concerns about cost effectiveness analyses using QALYs and similar measures\n    - These devalue people with disabilities and chronic conditions\n    - Underscores need for transparency in data used to automate coverage decisions\n  - Speed alone does not equal value if it comes at cost of accuracy, fairness, or transparency\n\n- Patient Harm, Bias, and Accountability Risks\n  - Consequences of AI-driven denials are not hypothetical\n  - National survey data shows:\n    - 93% of physicians report prior authorization harms clinical outcomes\n    - Nearly 1 in 4 physicians has witnessed a serious adverse event linked to delayed or denied care\n      - Including hospitalization, disability, or death\n  - Majority of appealed denials are overturned, suggesting many initial decisions are wrong\n    - Yet patients bear burden of delay, confusion, and financial stress\n  - Bias in AI systems can go unnoticed until harm is widespread\n    - Example: algorithm for high-risk care management programs significantly under-identified Black patients\n      - Relied on historical health care spending as proxy for need\n      - Black patients historically received less care despite similar or greater illness burden\n      - Algorithm systematically excluded patients who would have benefited from additional support\n    - When AI equates spending with need, it quietly carries inequities forward\n\n- The Importance of Human Oversight and Transparency\n  - AI tools should support, not replace, clinical judgment\n  - Clinicians should be transparent with patients about when and how AI tools assist their care\n  - Patients express concern that automated systems may erode human connection central to effective, compassionate care\n  - Clear communication and clinician accountability essential to maintaining trust\n  - AI should augment rather than substitute for meaningful patient-provider relationships\n  - Meaningful human oversight particularly important in clinical settings\n    - AI-generated insights may influence diagnosis, treatment planning, or disease management\n    - Over-reliance on automated recommendations risks undermining clinician expertise\n    - Could lead to inappropriate care if outputs accepted without sufficient clinical context\n  - AI systems should function as decision-support tools\n    - Clinicians must retain full responsibility for interpreting and applying information based on individual patient needs\n  - Transparency must extend to data governance\n    - Patients should retain ownership of their personal health data\n    - Clear protections needed for how data are collected, used, shared, and stored when AI tools deployed\n    - Robust data protection policies essential to preserving patient autonomy and preventing misuse\n\n- Policy recommendations for HHS\n  - Prioritize programmatic and payment policies requiring meaningful human review and transparency for AI-enabled tools\n    - Applies to both clinical care and administrative decision-making\n  - Establish standards for:\n    - Disclosure\n    - Auditability\n    - Data protections\n    - Appeal rights when AI systems used in Medicare and other federally-supported programs\n  - Revisit existing policies governing utilization management and clinical decision support\n    - Ensure automated tools do not supplant clinical judgment or undermine beneficiary protections\n\n- Building Patient Trust Through Governance and Participation\n  - Core challenge: patients rarely meaningfully involved in design, governance, or evaluation of AI tools\n  - When patient voices and lived experience are absent, systems may prioritize efficiency or cost savings\n    - Overlooks real-world impacts on access, understanding, and trust\n  - HHS has opportunity to set expectations for patient and caregiver input throughout AI lifecycle\n    - From development and testing to deployment and post-market evaluation\n  - Transparency, shared decision-making, and patient-centered metrics essential to building public confidence\n\n- Aligning AI Innovation With Public Trust\n  - HHS commitment to combating fraud, waste, and abuse while ensuring appropriate care are not mutually exclusive goals\n  - AI models that financially benefit vendors or payers when care is withheld create misaligned incentives warranting scrutiny\n  - Clinicians call for greater investment in AI research\n    - Tools should be rigorously tested for safety and effectiveness\n    - Should deliver clear value without introducing avoidable risks or costs\n  - HHS should pair stronger evidence generation with policies aligning reimbursement and program design with patient outcomes\n    - Not short-term cost avoidance\n  - AI can improve care only if accountability, transparency, and patient protections built in from the start\n\n- Conclusion\n  - HHS can accelerate responsible AI adoption by prioritizing:\n    - Transparency\n    - Meaningful human oversight in high-stakes decisions\n    - Patient-centered evaluation before and after deployment\n  - These guardrails will help ensure AI supports clinical care and strengthens public trust as tools continue to scale\n\n---",
      "oneLineSummary": "A liver disease patient advocacy organization urges HHS to establish strong guardrails for AI in health care, warning that AI-driven prior authorization and coverage decisions are amplifying inequities and denying necessary care while calling for mandatory human oversight, transparency, and patient involvement throughout the AI lifecycle.\n\n---",
      "commenterProfile": "- **Name/Organization:** Global Liver Institute (GLI)\n- **Type:** Advocacy Group\n- **Role/Expertise:** 501(c)(3) nonprofit focused on liver health advocacy; member of National Health Council and NORD; Healthy People 2030 Champion; Platinum Transparency with Candid/GuideStar\n- **Geographic Scope:** International (operates globally), headquartered in U.S.\n- **Stake in Issue:** Represents liver disease patients who face prior authorization barriers and coverage denials; concerned about AI perpetuating inequities affecting patients with chronic conditions\n\n---",
      "corePosition": "We agree AI holds promise for improving health care through earlier detection, reduced administrative burden, and personalized care, but AI has fallen short in high-stakes administrative functions where it has amplified delays, errors, and inequities. AI tools should support, not replace, clinical judgment, and HHS must establish clear guardrails prioritizing transparency, meaningful human oversight, and patient-centered evaluation to ensure these tools enhance rather than undermine care.\n\n---",
      "keyRecommendations": "- Require meaningful human review and transparency for AI-enabled tools used in clinical care and administrative decision-making\n  - Establish standards for disclosure, auditability, data protections, and appeal rights in Medicare and federally-supported programs\n- Revisit existing utilization management and clinical decision support policies\n  - Ensure automated tools do not supplant clinical judgment or undermine beneficiary protections\n- Integrate patient and caregiver input throughout the AI lifecycle\n  - From development and testing to deployment and post-market evaluation\n- Ensure patients retain ownership of their personal health data\n  - Establish clear protections for how data are collected, used, shared, and stored when AI tools are deployed\n- Align reimbursement and program design with patient outcomes rather than short-term cost avoidance\n  - Pair stronger evidence generation with outcome-focused policies\n- Require clinician transparency with patients about when and how AI tools assist their care\n- Invest in AI research to rigorously test tools for safety and effectiveness before deployment\n\n---",
      "mainConcerns": "- AI in prior authorization and coverage decisions is amplifying harm rather than improving outcomes\n  - Algorithms trained on historical spending patterns embed past inequities at scale\n  - Lower spending misinterpreted as lower need, disproportionately affecting low-income, rural, and minority patients\n  - Speed of decisions does not equal value if accuracy, fairness, or transparency are sacrificed\n- Documented patient harm from AI-driven denials\n  - 93% of physicians report prior authorization harms clinical outcomes\n  - Nearly 1 in 4 physicians has witnessed serious adverse events (hospitalization, disability, death) linked to delayed or denied care\n  - Majority of appealed denials are overturned, yet patients bear burden of delay and stress\n- Racial bias in AI systems\n  - Example of algorithm that under-identified Black patients for high-risk care management by using spending as proxy for need\n  - Black patients historically received less care despite similar illness burden, leading to systematic exclusion\n- Cost effectiveness measures like QALYs devalue people with disabilities and chronic conditions\n  - Lack of transparency in data used to automate coverage decisions\n- AI models that financially benefit vendors or payers when care is withheld create misaligned incentives\n- Patients rarely involved in design, governance, or evaluation of AI tools affecting them\n  - Systems may prioritize efficiency or cost savings over real-world impacts on access and trust\n- Patient concerns about losing human touch in care and lack of transparency about AI use\n- Over-reliance on automated recommendations risks undermining clinician expertise\n  - Could lead to inappropriate care if outputs accepted without clinical context\n\n---",
      "notableExperiences": "- Bipartisan congressional concern cited: Committee Chair Brett Guthrie (R) stated \"people are being denied care\" while Ranking Member Frank Pallone (D) warned AI creates \"another barrier\" for patients\n- Specific algorithm bias case: A widely-used algorithm for identifying high-risk patients significantly under-identified Black patients because it used historical spending as a proxy for need—Black patients had received less care historically despite similar or greater illness burden, so the algorithm systematically excluded them from beneficial programs\n- Insight on spending-as-proxy problem: \"When AI equates spending with need, it quietly carries inequities forward\"\n- Long-standing organizational concern about QALYs and similar cost-effectiveness measures that devalue people with disabilities and chronic conditions—now amplified by AI automation\n\n---",
      "keyQuotations": "- \"When AI equates spending with need, it quietly carries inequities forward.\"\n- \"Speed alone does not equal value if it comes at the cost of accuracy, fairness, or transparency.\"\n- \"AI can be a powerful tool for improving care, but only if accountability, transparency, and patient protections are built in from the start.\""
    },
    "themeScores": {
      "7": 1,
      "8": 1,
      "3.4": 1,
      "7.1": 1,
      "7.2": 1,
      "7.3": 1,
      "7.4": 1,
      "7.5": 1,
      "8.1": 1,
      "8.6": 1
    },
    "entities": [
      {
        "category": "AI Governance and Evaluation",
        "label": "Human-in-the-Loop"
      },
      {
        "category": "AI Governance and Evaluation",
        "label": "Trustworthy AI"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Diagnosis"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Shared Decision-Making"
      },
      {
        "category": "Clinical Processes and Procedures",
        "label": "Treatment"
      },
      {
        "category": "Clinical Specialties and Domains",
        "label": "Chronic Disease Management"
      },
      {
        "category": "Data Privacy and Security",
        "label": "Data Governance"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Abuse"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Fraud"
      },
      {
        "category": "Fraud and Compliance",
        "label": "Waste"
      },
      {
        "category": "Health Information Systems",
        "label": "CDS"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Documentation"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Prior Authorization"
      },
      {
        "category": "Healthcare Administrative Processes",
        "label": "Utilization Management"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Administrative Burden"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Scalability"
      },
      {
        "category": "Healthcare Business and Operations",
        "label": "Vendor"
      },
      {
        "category": "Healthcare Programs",
        "label": "Medicare"
      },
      {
        "category": "Healthcare Quality and Safety",
        "label": "Adverse Event"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Physician"
      },
      {
        "category": "Healthcare Workforce Roles",
        "label": "Provider"
      },
      {
        "category": "Liability and Legal Concepts",
        "label": "Accountability"
      },
      {
        "category": "Medical Conditions and Diseases",
        "label": "Liver Disease"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Caregiver"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Autonomy"
      },
      {
        "category": "Patient-Centered Concepts",
        "label": "Patient Trust"
      }
    ],
    "hasAttachments": true,
    "wordCount": 1476,
    "clusterSize": 1,
    "isClusterRepresentative": false,
    "clusterRepresentativeId": null,
    "isAlignedSummary": false
  }
]