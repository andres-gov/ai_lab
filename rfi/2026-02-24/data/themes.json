[
  {
    "code": "1",
    "description": "Governance and Accountability Frameworks for Clinical AI",
    "level": 1,
    "parent_code": null,
    "detailed_guidelines": "This theme encompasses all structures, processes, and organizational mechanisms needed to oversee AI deployment in healthcare settings, from internal committees to federal guidance. It includes discussions of governance uncertainty as a barrier to adoption, the need for clear accountability structures, organizational readiness requirements, and the establishment of oversight bodies at both organizational and federal levels. This theme covers who is responsible for AI oversight and how oversight should be structured. It does NOT include specific regulatory agency jurisdictional questions (covered under Regulatory Clarity), liability and legal frameworks (separate theme), or technical standards for AI systems. This matters because commenters consistently identify governance uncertainty as a primary barrier preventing organizations from moving forward with AI adoption.",
    "comment_count": 17,
    "direct_count": 17,
    "touch_count": 0,
    "detailedDescription": "This theme encompasses all structures, processes, and organizational mechanisms needed to oversee AI deployment in healthcare settings, from internal committees to federal guidance. It includes discussions of governance uncertainty as a barrier to adoption, the need for clear accountability structures, organizational readiness requirements, and the establishment of oversight bodies at both organizational and federal levels. This theme covers who is responsible for AI oversight and how oversight should be structured. It does NOT include specific regulatory agency jurisdictional questions (covered under Regulatory Clarity), liability and legal frameworks (separate theme), or technical standards for AI systems. This matters because commenters consistently identify governance uncertainty as a primary barrier preventing organizations from moving forward with AI adoption.",
    "children": [
      "1.1",
      "1.2",
      "1.3",
      "1.4",
      "1.5",
      "1.6",
      "1.7"
    ]
  },
  {
    "code": "1.1",
    "description": "Executive Ownership and Cross-Functional Authority",
    "level": 2,
    "parent_code": "1",
    "detailed_guidelines": "This sub-theme addresses the need for designated leadership with clear decision-making authority over AI initiatives that span multiple departments. It includes concerns about diffuse accountability across multiple stakeholders (CIO, CMIO, Compliance, CFO), recommendations for named AI Executive Owners with cross-functional authority, and the challenge that AI spans nursing, HR, IT, and operations without clear organizational ownership. Commenters report that multiple stakeholders hold effective veto power, causing promising initiatives to die in committee. This excludes committee structures and governance processes. This matters because lack of clear ownership is cited as causing AI initiatives to stall indefinitely.",
    "comment_count": 6,
    "direct_count": 6,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses the need for designated leadership with clear decision-making authority over AI initiatives that span multiple departments. It includes concerns about diffuse accountability across multiple stakeholders (CIO, CMIO, Compliance, CFO), recommendations for named AI Executive Owners with cross-functional authority, and the challenge that AI spans nursing, HR, IT, and operations without clear organizational ownership. Commenters report that multiple stakeholders hold effective veto power, causing promising initiatives to die in committee. This excludes committee structures and governance processes. This matters because lack of clear ownership is cited as causing AI initiatives to stall indefinitely.",
    "children": []
  },
  {
    "code": "1.2",
    "description": "AI Committee Review Processes and Approval Cycles. This sub-theme covers the internal review and approval processes organizations use to evaluate AI tools before deployment. || It includes concerns about 12+ month AI Committee review cycles, inconsistent evaluation requirements across health systems creating duplicative effort, and the need for streamlined pathways for lower-risk operational AI",
    "level": 2,
    "parent_code": "1",
    "detailed_guidelines": "Commenters report that organizations default to enterprise IT evaluation processes designed for EHRs, which are poorly suited to AI tools. This excludes executive authority questions and external certification. This matters because lengthy review cycles are cited as a major barrier to timely AI adoption.",
    "comment_count": 5,
    "direct_count": 5,
    "touch_count": 0,
    "detailedDescription": "Commenters report that organizations default to enterprise IT evaluation processes designed for EHRs, which are poorly suited to AI tools. This excludes executive authority questions and external certification. This matters because lengthy review cycles are cited as a major barrier to timely AI adoption.",
    "children": []
  },
  {
    "code": "1.3",
    "description": "Medical Staff Bylaws and Clinical Governance Integration",
    "level": 2,
    "parent_code": "1",
    "detailed_guidelines": "This sub-theme addresses how existing clinical governance structures should incorporate AI oversight for tools affecting patient care decisions. It includes concerns that medical staff bylaws rarely address AI, recommendations to update Conditions of Participation to require AI oversight designation, and integration with existing quality and safety paradigms. Clinical AI affects patient care decisions that traditionally fall under medical staff governance, yet most bylaws predate AI deployment. This excludes administrative governance and IT oversight. This matters because clinical governance structures must evolve to address AI's role in care decisions.",
    "comment_count": 0,
    "direct_count": 0,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses how existing clinical governance structures should incorporate AI oversight for tools affecting patient care decisions. It includes concerns that medical staff bylaws rarely address AI, recommendations to update Conditions of Participation to require AI oversight designation, and integration with existing quality and safety paradigms. Clinical AI affects patient care decisions that traditionally fall under medical staff governance, yet most bylaws predate AI deployment. This excludes administrative governance and IT oversight. This matters because clinical governance structures must evolve to address AI's role in care decisions.",
    "children": []
  },
  {
    "code": "1.4",
    "description": "Organizational Readiness Assessment Before AI Deployment",
    "level": 2,
    "parent_code": "1",
    "detailed_guidelines": "This sub-theme addresses organizational preparedness for AI adoption, including readiness assessments, maturity models, and prerequisite conditions. It includes recommendations for AI readiness assessments before deployment, maturity models for organizations and vendors, \"readiness before reimbursement\" policies, and concerns about premature adoption without adequate preparation. Commenters warn that failed implementations due to lack of readiness erode confidence in future AI initiatives. This excludes ongoing governance once AI is deployed. This matters because organizations need clear benchmarks for what constitutes adequate preparation.",
    "comment_count": 5,
    "direct_count": 5,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses organizational preparedness for AI adoption, including readiness assessments, maturity models, and prerequisite conditions. It includes recommendations for AI readiness assessments before deployment, maturity models for organizations and vendors, \"readiness before reimbursement\" policies, and concerns about premature adoption without adequate preparation. Commenters warn that failed implementations due to lack of readiness erode confidence in future AI initiatives. This excludes ongoing governance once AI is deployed. This matters because organizations need clear benchmarks for what constitutes adequate preparation.",
    "children": []
  },
  {
    "code": "1.5",
    "description": "Infrastructure Readiness and Technical Prerequisites. This sub-theme covers the technical and operational infrastructure prerequisites for AI deployment, particularly in resource-constrained settings. || It includes concerns about unreliable connectivity, legacy systems, unmapped Wi-Fi coverage in care areas, and the need for minimum digital reliability before implementing AI-dependent workflows. This matters particularly for PALTC and rural settings where infrastructure gaps are most acute",
    "level": 2,
    "parent_code": "1",
    "detailed_guidelines": "This excludes workforce readiness and governance structure readiness. This matters because AI-dependent workflows fail when underlying infrastructure is unreliable.",
    "comment_count": 5,
    "direct_count": 5,
    "touch_count": 0,
    "detailedDescription": "This excludes workforce readiness and governance structure readiness. This matters because AI-dependent workflows fail when underlying infrastructure is unreliable.",
    "children": []
  },
  {
    "code": "1.6",
    "description": "Minimum Governance Baselines and Floor Standards",
    "level": 2,
    "parent_code": "1",
    "detailed_guidelines": "This sub-theme addresses the establishment of floor-level governance requirements that all organizations should meet regardless of size or resources. It includes recommendations for minimal, risk-proportionate governance baselines, four demonstrable elements (documented intended use, fit-for-purpose evaluation, defined accountability, post-deployment monitoring), and basic governance norms. Commenters seek clarity on what constitutes \"reasonable\" oversight that will be defensible to regulators, accreditors, and courts. This excludes best-in-class practices and aspirational standards. This matters because organizations need clear minimum standards rather than aspirational guidance.",
    "comment_count": 11,
    "direct_count": 11,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses the establishment of floor-level governance requirements that all organizations should meet regardless of size or resources. It includes recommendations for minimal, risk-proportionate governance baselines, four demonstrable elements (documented intended use, fit-for-purpose evaluation, defined accountability, post-deployment monitoring), and basic governance norms. Commenters seek clarity on what constitutes \"reasonable\" oversight that will be defensible to regulators, accreditors, and courts. This excludes best-in-class practices and aspirational standards. This matters because organizations need clear minimum standards rather than aspirational guidance.",
    "children": []
  },
  {
    "code": "1.7",
    "description": "Federal Bioethics Engagement and Ethics Advisory Structures. This sub-theme encompasses recommendations for involving bioethics expertise in AI governance at the federal level through formal advisory mechanisms. || It includes calls for engaging NIH Department of Bioethics staff, using existing advisory committees, collaborating with academic and nonprofit ethics organizations, supporting restoration of a Presidential Bioethics Commission, and creating ethics advisory boards modeled on NAIRR",
    "level": 2,
    "parent_code": "1",
    "detailed_guidelines": "This excludes organizational-level ethics processes and specific ethical issues. This matters because commenters argue that multiple ethical issues require systematic review processes that don't currently exist at the federal level.",
    "comment_count": 2,
    "direct_count": 2,
    "touch_count": 0,
    "detailedDescription": "This excludes organizational-level ethics processes and specific ethical issues. This matters because commenters argue that multiple ethical issues require systematic review processes that don't currently exist at the federal level.",
    "children": []
  },
  {
    "code": "2",
    "description": "Regulatory Clarity and Agency Coordination. This theme encompasses all concerns about unclear regulatory frameworks, agency jurisdictional boundaries, and the need for coordinated federal guidance for clinical AI. It includes concerns about regulatory uncertainty for non-device AI, unclear FDA approval requirements, fragmented guidance across HHS divisions, and recommendations for cross-agency coordination",
    "level": 1,
    "parent_code": null,
    "detailed_guidelines": "This theme encompasses all concerns about unclear regulatory frameworks, agency jurisdictional boundaries, and the need for coordinated federal guidance for clinical AI. It includes concerns about regulatory uncertainty for non-device AI, unclear FDA approval requirements, fragmented guidance across HHS divisions, and recommendations for cross-agency coordination. This theme covers questions about which agency's rules apply, what requires approval versus guidance, and how to navigate the current regulatory landscape. It does NOT include specific governance structures, liability frameworks, or technical standards. This matters because regulatory uncertainty is cited as a primary barrier causing legal and compliance teams to delay or block AI deployment.",
    "comment_count": 24,
    "direct_count": 24,
    "touch_count": 0,
    "detailedDescription": "This theme encompasses all concerns about unclear regulatory frameworks, agency jurisdictional boundaries, and the need for coordinated federal guidance for clinical AI. It includes concerns about regulatory uncertainty for non-device AI, unclear FDA approval requirements, fragmented guidance across HHS divisions, and recommendations for cross-agency coordination. This theme covers questions about which agency's rules apply, what requires approval versus guidance, and how to navigate the current regulatory landscape. It does NOT include specific governance structures, liability frameworks, or technical standards. This matters because regulatory uncertainty is cited as a primary barrier causing legal and compliance teams to delay or block AI deployment.",
    "children": [
      "2.1",
      "2.2",
      "2.3",
      "2.4",
      "2.5",
      "2.6"
    ]
  },
  {
    "code": "2.1",
    "description": "Non-Device AI Regulatory Gap",
    "level": 2,
    "parent_code": "2",
    "detailed_guidelines": "This sub-theme specifically addresses the regulatory vacuum for AI that influences clinical care but is not regulated as a medical device. It includes concerns about documentation support AI, care coordination tools, generative AI, and clinical decision support that falls outside FDA's traditional scope. Commenters report using AI tools daily without clarity on their regulatory status. This excludes FDA-regulated medical devices. This matters because this category of AI is expanding rapidly while accountability frameworks remain unclear.",
    "comment_count": 14,
    "direct_count": 14,
    "touch_count": 0,
    "detailedDescription": "This sub-theme specifically addresses the regulatory vacuum for AI that influences clinical care but is not regulated as a medical device. It includes concerns about documentation support AI, care coordination tools, generative AI, and clinical decision support that falls outside FDA's traditional scope. Commenters report using AI tools daily without clarity on their regulatory status. This excludes FDA-regulated medical devices. This matters because this category of AI is expanding rapidly while accountability frameworks remain unclear.",
    "children": []
  },
  {
    "code": "2.2",
    "description": "Clinical Decision Support Classification Boundaries. This sub-theme addresses the boundary between regulated medical devices and unregulated clinical decision support tools. || It includes concerns about risk scores and AI-generated summaries that shape clinical decisions without device regulation, the need for clearer guidance on what requires FDA approval, and recommendations for basic certification approaches for non-medical AI devices. Commenters report confusion about whether specific tools require regulatory approval",
    "level": 2,
    "parent_code": "2",
    "detailed_guidelines": "This excludes clearly regulated devices and purely administrative AI. This matters because classification uncertainty creates compliance risk.",
    "comment_count": 4,
    "direct_count": 4,
    "touch_count": 0,
    "detailedDescription": "This excludes clearly regulated devices and purely administrative AI. This matters because classification uncertainty creates compliance risk.",
    "children": []
  },
  {
    "code": "2.3",
    "description": "Operational Versus Clinical AI Distinction",
    "level": 2,
    "parent_code": "2",
    "detailed_guidelines": "This sub-theme covers the need to distinguish operational AI (scheduling, documentation, workflow) from clinical AI requiring different oversight levels. It includes recommendations for streamlined pathways for lower-risk administrative AI, guidance distinguishing operational from clinical AI, and concerns that organizations apply enterprise IT evaluation processes designed for EHRs to all AI tools regardless of risk. This excludes clinical decision-making AI. This matters because commenters argue operational AI faces disproportionate review burdens relative to its risk level.",
    "comment_count": 5,
    "direct_count": 5,
    "touch_count": 0,
    "detailedDescription": "This sub-theme covers the need to distinguish operational AI (scheduling, documentation, workflow) from clinical AI requiring different oversight levels. It includes recommendations for streamlined pathways for lower-risk administrative AI, guidance distinguishing operational from clinical AI, and concerns that organizations apply enterprise IT evaluation processes designed for EHRs to all AI tools regardless of risk. This excludes clinical decision-making AI. This matters because commenters argue operational AI faces disproportionate review burdens relative to its risk level.",
    "children": []
  },
  {
    "code": "2.4",
    "description": "Cross-Agency Coordination and Harmonization",
    "level": 2,
    "parent_code": "2",
    "detailed_guidelines": "This sub-theme addresses the need for alignment across HHS divisions and other federal agencies on AI guidance and requirements. It includes concerns about conflicting guidance across HHS divisions, recommendations for coordinated action across FDA, CMS, ONC, OCR, and NIH, and calls for consistency in AI use standards. Commenters report that fragmented guidance creates confusion about which agency's rules apply to specific use cases. This excludes single-agency guidance. This matters because organizations cannot comply with contradictory requirements.",
    "comment_count": 5,
    "direct_count": 5,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses the need for alignment across HHS divisions and other federal agencies on AI guidance and requirements. It includes concerns about conflicting guidance across HHS divisions, recommendations for coordinated action across FDA, CMS, ONC, OCR, and NIH, and calls for consistency in AI use standards. Commenters report that fragmented guidance creates confusion about which agency's rules apply to specific use cases. This excludes single-agency guidance. This matters because organizations cannot comply with contradictory requirements.",
    "children": []
  },
  {
    "code": "2.5",
    "description": "Regulatory Safe Harbors and Learning Environments",
    "level": 2,
    "parent_code": "2",
    "detailed_guidelines": "This sub-theme covers recommendations for protected spaces where AI can be deployed with reduced regulatory risk to enable learning. It includes support for time-limited learning environments, safe harbors for well-scoped AI use cases, safe harbors for organizations implementing defined governance practices, and time-limited federal safe-harbor mechanisms during declared staffing emergencies. This excludes permanent regulatory frameworks. This matters because without safe harbors, organizations cannot gain the real-world experience needed to develop evidence.",
    "comment_count": 8,
    "direct_count": 8,
    "touch_count": 0,
    "detailedDescription": "This sub-theme covers recommendations for protected spaces where AI can be deployed with reduced regulatory risk to enable learning. It includes support for time-limited learning environments, safe harbors for well-scoped AI use cases, safe harbors for organizations implementing defined governance practices, and time-limited federal safe-harbor mechanisms during declared staffing emergencies. This excludes permanent regulatory frameworks. This matters because without safe harbors, organizations cannot gain the real-world experience needed to develop evidence.",
    "children": []
  },
  {
    "code": "2.6",
    "description": "Autonomous AI Systems Regulatory Standards. This sub-theme addresses the need for proactive regulatory frameworks for AI systems that operate without human clinician involvement. || It includes concerns that compliance standards for autonomous medical decision-making tools don't yet exist, recommendations to develop standards before such tools become widespread, and questions about oversight of adaptive or evolving AI systems. Commenters warn that autonomous AI is coming and current frameworks are unprepared",
    "level": 2,
    "parent_code": "2",
    "detailed_guidelines": "This excludes human-in-the-loop AI. This matters because autonomous systems require different oversight than assistive tools.",
    "comment_count": 5,
    "direct_count": 5,
    "touch_count": 0,
    "detailedDescription": "This excludes human-in-the-loop AI. This matters because autonomous systems require different oversight than assistive tools.",
    "children": []
  },
  {
    "code": "3",
    "description": "Liability, Accountability, and Legal Frameworks. This theme encompasses all concerns about legal responsibility, malpractice implications, and accountability allocation when AI influences clinical decisions. || It includes concerns about unclear liability when AI makes errors, malpractice insurance treatment of AI-assisted care, vendor liability-shifting through contracts, and the need for clear accountability frameworks. This theme covers questions of who is responsible when things go wrong and how legal frameworks should adapt",
    "level": 1,
    "parent_code": null,
    "detailed_guidelines": "It does NOT include governance structures, regulatory classification, or technical auditability requirements. This matters because unresolved liability questions are cited as major barriers causing legal departments to block AI deployment.",
    "comment_count": 18,
    "direct_count": 18,
    "touch_count": 0,
    "detailedDescription": "It does NOT include governance structures, regulatory classification, or technical auditability requirements. This matters because unresolved liability questions are cited as major barriers causing legal departments to block AI deployment.",
    "children": [
      "3.1",
      "3.2",
      "3.3",
      "3.4"
    ]
  },
  {
    "code": "3.1",
    "description": "Malpractice and Professional Liability for AI-Assisted Care",
    "level": 2,
    "parent_code": "3",
    "detailed_guidelines": "This sub-theme addresses how traditional malpractice frameworks apply when AI influences clinical decisions made by individual clinicians. It includes concerns that traditional frameworks assume human decision-makers, questions about how malpractice insurers will treat AI-assisted care, and the reality that clinicians retain full legal responsibility regardless of AI assistance. Frontline clinicians shoulder disproportionate ambiguity and accountability for AI-influenced decisions. This excludes vendor liability and organizational liability. This matters because clinicians need clarity on their exposure when using AI tools.",
    "comment_count": 8,
    "direct_count": 8,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses how traditional malpractice frameworks apply when AI influences clinical decisions made by individual clinicians. It includes concerns that traditional frameworks assume human decision-makers, questions about how malpractice insurers will treat AI-assisted care, and the reality that clinicians retain full legal responsibility regardless of AI assistance. Frontline clinicians shoulder disproportionate ambiguity and accountability for AI-influenced decisions. This excludes vendor liability and organizational liability. This matters because clinicians need clarity on their exposure when using AI tools.",
    "children": []
  },
  {
    "code": "3.2",
    "description": "Vendor Liability and Contractual Risk Allocation",
    "level": 2,
    "parent_code": "3",
    "detailed_guidelines": "This sub-theme covers how liability is allocated between AI vendors and healthcare organizations through contracts and service agreements. It includes concerns about vendors using opaque practices or refusing standard BAAs, liability caps at trivially low amounts, vendors overriding BAA restrictions through main service agreements, and recommendations for model contract provisions. Commenters report that standard vendor contracts don't address AI-specific risks and shift liability inappropriately to providers. This excludes clinician malpractice. This matters because contractual terms determine who bears risk when AI fails.",
    "comment_count": 6,
    "direct_count": 6,
    "touch_count": 0,
    "detailedDescription": "This sub-theme covers how liability is allocated between AI vendors and healthcare organizations through contracts and service agreements. It includes concerns about vendors using opaque practices or refusing standard BAAs, liability caps at trivially low amounts, vendors overriding BAA restrictions through main service agreements, and recommendations for model contract provisions. Commenters report that standard vendor contracts don't address AI-specific risks and shift liability inappropriately to providers. This excludes clinician malpractice. This matters because contractual terms determine who bears risk when AI fails.",
    "children": []
  },
  {
    "code": "3.3",
    "description": "Responsibility Laundering and Accountability Gaps. This sub-theme addresses concerns that current frameworks allow responsibility to be diffused or avoided through disclaimers and performative oversight. || It includes concerns about \"responsibility laundering\" through vendor disclaimers and performative \"human review,\" the risk that system designs prevent meaningful intervention while claiming oversight exists, and recommendations to block accountability-avoiding mechanisms. Commenters warn that transparency-focused governance without enforcement mechanisms will fail",
    "level": 2,
    "parent_code": "3",
    "detailed_guidelines": "This excludes clear liability allocation. This matters because accountability gaps undermine the entire governance framework.",
    "comment_count": 2,
    "direct_count": 2,
    "touch_count": 0,
    "detailedDescription": "This excludes clear liability allocation. This matters because accountability gaps undermine the entire governance framework.",
    "children": []
  },
  {
    "code": "3.4",
    "description": "Contestability and Patient Appeal Rights. This sub-theme covers patient and clinician rights to challenge AI-driven decisions affecting care access or treatment. || It includes recommendations for contestability pathways with binding timelines, requirements that patients be able to understand, contest, and appeal AI-mediated decisions, and concerns that patients may have no appeal pathway when AI-driven decisions delay care",
    "level": 2,
    "parent_code": "3",
    "detailed_guidelines": "This excludes liability allocation. This matters because AI systems that cannot be contested are unsafe regardless of technical accuracy.",
    "comment_count": 3,
    "direct_count": 3,
    "touch_count": 0,
    "detailedDescription": "This excludes liability allocation. This matters because AI systems that cannot be contested are unsafe regardless of technical accuracy.",
    "children": []
  },
  {
    "code": "4",
    "description": "Reimbursement and Payment Model Alignment. This theme encompasses all concerns about how payment policies affect AI adoption, including fee-for-service limitations, value-based alternatives, and specific reimbursement mechanisms. It includes concerns about the \"efficiency penalty\" where AI that improves quality reduces payments, recommendations for new payment codes, value-based payment models, and reimbursement parity for AI-enabled services",
    "level": 1,
    "parent_code": null,
    "detailed_guidelines": "This theme encompasses all concerns about how payment policies affect AI adoption, including fee-for-service limitations, value-based alternatives, and specific reimbursement mechanisms. It includes concerns about the \"efficiency penalty\" where AI that improves quality reduces payments, recommendations for new payment codes, value-based payment models, and reimbursement parity for AI-enabled services. This theme covers the financial incentives and barriers created by current payment structures. It does NOT include procurement costs, liability costs, or general economic analysis. This matters because payment model misalignment is identified as a fundamental barrier where organizations investing in AI cannot capture its financial benefits.",
    "comment_count": 16,
    "direct_count": 16,
    "touch_count": 0,
    "detailedDescription": "This theme encompasses all concerns about how payment policies affect AI adoption, including fee-for-service limitations, value-based alternatives, and specific reimbursement mechanisms. It includes concerns about the \"efficiency penalty\" where AI that improves quality reduces payments, recommendations for new payment codes, value-based payment models, and reimbursement parity for AI-enabled services. This theme covers the financial incentives and barriers created by current payment structures. It does NOT include procurement costs, liability costs, or general economic analysis. This matters because payment model misalignment is identified as a fundamental barrier where organizations investing in AI cannot capture its financial benefits.",
    "children": [
      "4.1",
      "4.2",
      "4.3",
      "4.4",
      "4.5",
      "4.6",
      "4.7"
    ]
  },
  {
    "code": "4.1",
    "description": "Fee-for-Service Efficiency Penalty",
    "level": 2,
    "parent_code": "4",
    "detailed_guidelines": "This sub-theme addresses how current fee-for-service payment structures create disincentives for AI adoption by penalizing efficiency gains. It includes concerns that AI improving efficiency may be interpreted as lowering physician work and reducing RVUs, that CPT codes equate \"value\" with \"time spent,\" and that innovation improving quality may be financially penalized. Commenters describe a fundamental paradox where AI that works well financially harms the organizations deploying it. This excludes value-based alternatives. This matters because current payment structures actively discourage beneficial AI adoption.",
    "comment_count": 6,
    "direct_count": 6,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses how current fee-for-service payment structures create disincentives for AI adoption by penalizing efficiency gains. It includes concerns that AI improving efficiency may be interpreted as lowering physician work and reducing RVUs, that CPT codes equate \"value\" with \"time spent,\" and that innovation improving quality may be financially penalized. Commenters describe a fundamental paradox where AI that works well financially harms the organizations deploying it. This excludes value-based alternatives. This matters because current payment structures actively discourage beneficial AI adoption.",
    "children": []
  },
  {
    "code": "4.2",
    "description": "Value-Based Payment Model Opportunities",
    "level": 2,
    "parent_code": "4",
    "detailed_guidelines": "This sub-theme covers recommendations for payment models that better align with AI's value proposition by rewarding outcomes rather than volume. It includes recommendations for allowing AI costs within value-based models, bundled payments that let providers capture efficiency benefits, outcome-linked payment pilots, and value-based AI add-ons under MSSP, ACO REACH, and bundled payments. This excludes fee-for-service modifications. This matters because value-based models could resolve the efficiency penalty problem and align incentives with AI benefits.",
    "comment_count": 9,
    "direct_count": 9,
    "touch_count": 0,
    "detailedDescription": "This sub-theme covers recommendations for payment models that better align with AI's value proposition by rewarding outcomes rather than volume. It includes recommendations for allowing AI costs within value-based models, bundled payments that let providers capture efficiency benefits, outcome-linked payment pilots, and value-based AI add-ons under MSSP, ACO REACH, and bundled payments. This excludes fee-for-service modifications. This matters because value-based models could resolve the efficiency penalty problem and align incentives with AI benefits.",
    "children": []
  },
  {
    "code": "4.3",
    "description": "New Payment Codes and Billing Mechanisms for AI Services. This sub-theme addresses specific recommendations for new billing codes, modifiers, and reimbursement pathways for AI-enabled services. || It includes recommendations for new CPT codes or modifiers for AI-assisted services, \"AI-Guided Clinical Interpretation\" add-on codes, permanent add-on payments tied to validated AI use, and reimbursement parity between AI-driven and traditional procedures. This excludes payment model restructuring",
    "level": 2,
    "parent_code": "4",
    "detailed_guidelines": "This matters because concrete reimbursement pathways are needed to recognize AI's contribution to care",
    "comment_count": 7,
    "direct_count": 7,
    "touch_count": 0,
    "detailedDescription": "This matters because concrete reimbursement pathways are needed to recognize AI's contribution to care",
    "children": []
  },
  {
    "code": "4.4",
    "description": "Tiered Complexity Models for AI Reimbursement",
    "level": 2,
    "parent_code": "4",
    "detailed_guidelines": "This sub-theme covers proposals for risk-stratified reimbursement based on AI complexity and clinical value. It includes the recommendation for three tiers (Administrative AI, Assistive AI, Predictive/Multimodal AI) with different reimbursement structures reflecting different resource investments and clinical value. Different AI types require different reimbursement recognition based on their sophistication and impact. This excludes flat reimbursement approaches. This matters because one-size-fits-all reimbursement fails to account for varying AI complexity.",
    "comment_count": 1,
    "direct_count": 1,
    "touch_count": 0,
    "detailedDescription": "This sub-theme covers proposals for risk-stratified reimbursement based on AI complexity and clinical value. It includes the recommendation for three tiers (Administrative AI, Assistive AI, Predictive/Multimodal AI) with different reimbursement structures reflecting different resource investments and clinical value. Different AI types require different reimbursement recognition based on their sophistication and impact. This excludes flat reimbursement approaches. This matters because one-size-fits-all reimbursement fails to account for varying AI complexity.",
    "children": []
  },
  {
    "code": "4.5",
    "description": "Concurrent Billing Restrictions and Service Limitations. This sub-theme addresses regulatory barriers preventing billing for multiple AI-enabled services simultaneously. || It includes concerns about inability to concurrently bill RPM and RTM services, recommendations to amend 42 C.F.R. ยง 410.78 to permit concurrent billing when distinct hardware/data streams are used, and concerns that billing restrictions create data silos. Current restrictions force providers to choose between services, blinding AI models to patient context",
    "level": 2,
    "parent_code": "4",
    "detailed_guidelines": "This excludes new code creation. This matters because billing restrictions fragment care and limit AI effectiveness.",
    "comment_count": 1,
    "direct_count": 1,
    "touch_count": 0,
    "detailedDescription": "This excludes new code creation. This matters because billing restrictions fragment care and limit AI effectiveness.",
    "children": []
  },
  {
    "code": "4.6",
    "description": "Reimbursement Tied to Governance and Outcomes. This sub-theme covers recommendations for linking payment to demonstrated governance, readiness, or outcomes rather than technology adoption alone. || It includes \"readiness before reimbursement\" policies, requiring governance attestation artifacts as reimbursement conditions, pilot reimbursement models tied to verified delivery rather than capability attestations, and enhanced reimbursement when governance artifacts enable post-payment verification",
    "level": 2,
    "parent_code": "4",
    "detailed_guidelines": "This excludes unconditional reimbursement. This matters because paying for technology adoption without governance requirements risks repeating EHR mandate failures.",
    "comment_count": 6,
    "direct_count": 6,
    "touch_count": 0,
    "detailedDescription": "This excludes unconditional reimbursement. This matters because paying for technology adoption without governance requirements risks repeating EHR mandate failures.",
    "children": []
  },
  {
    "code": "4.7",
    "description": "Payer Contractual Restrictions on Provider AI Use",
    "level": 2,
    "parent_code": "4",
    "detailed_guidelines": "This sub-theme addresses concerns about insurance companies contractually prohibiting providers from using AI while using AI themselves. It includes concerns about \"anti-AI\" clauses banning providers from using AI in revenue cycle while payers use AI to automate claim denials, the power imbalance this creates, and recommendations to prohibit such contractual restrictions. Commenters describe this as hypocrisy that creates a chilling effect on innovation. This excludes reimbursement rates. This matters because contractual barriers may prevent beneficial AI adoption.",
    "comment_count": 2,
    "direct_count": 2,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses concerns about insurance companies contractually prohibiting providers from using AI while using AI themselves. It includes concerns about \"anti-AI\" clauses banning providers from using AI in revenue cycle while payers use AI to automate claim denials, the power imbalance this creates, and recommendations to prohibit such contractual restrictions. Commenters describe this as hypocrisy that creates a chilling effect on innovation. This excludes reimbursement rates. This matters because contractual barriers may prevent beneficial AI adoption.",
    "children": []
  },
  {
    "code": "5",
    "description": "Data Infrastructure and Interoperability. This theme encompasses all concerns about the data foundations required for clinical AI, including data quality, completeness, exchange standards, and semantic interoperability. It includes concerns about fragmented records, incomplete longitudinal data, data lacking provenance and context, and recommendations for enhanced interoperability standards",
    "level": 1,
    "parent_code": null,
    "detailed_guidelines": "This theme encompasses all concerns about the data foundations required for clinical AI, including data quality, completeness, exchange standards, and semantic interoperability. It includes concerns about fragmented records, incomplete longitudinal data, data lacking provenance and context, and recommendations for enhanced interoperability standards. This theme covers the data layer that AI systems depend upon. It does NOT include AI-specific technical standards, governance data exchange, or privacy concerns. This matters because AI inherits the limitations of fragmented, incomplete, or semantically inconsistent data.",
    "comment_count": 25,
    "direct_count": 25,
    "touch_count": 0,
    "detailedDescription": "This theme encompasses all concerns about the data foundations required for clinical AI, including data quality, completeness, exchange standards, and semantic interoperability. It includes concerns about fragmented records, incomplete longitudinal data, data lacking provenance and context, and recommendations for enhanced interoperability standards. This theme covers the data layer that AI systems depend upon. It does NOT include AI-specific technical standards, governance data exchange, or privacy concerns. This matters because AI inherits the limitations of fragmented, incomplete, or semantically inconsistent data.",
    "children": [
      "5.1",
      "5.2",
      "5.3",
      "5.4",
      "5.5",
      "5.6",
      "5.7",
      "5.8",
      "5.9"
    ]
  },
  {
    "code": "5.1",
    "description": "Longitudinal Data Completeness and Record Fragmentation",
    "level": 2,
    "parent_code": "5",
    "detailed_guidelines": "This sub-theme addresses the problem of incomplete patient records due to care fragmentation across multiple systems and providers. It includes concerns that clinical AI inherits incompleteness of fragmented records, that missingness correlates with socioeconomic access patterns worsening bias, and recommendations for patient-designated longitudinal endpoints where all health data converges. Missing history drives false positives/negatives, and fragmentation causes direct clinical harm even without AI. This excludes data exchange standards. This matters because AI cannot make good recommendations with incomplete information.",
    "comment_count": 7,
    "direct_count": 7,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses the problem of incomplete patient records due to care fragmentation across multiple systems and providers. It includes concerns that clinical AI inherits incompleteness of fragmented records, that missingness correlates with socioeconomic access patterns worsening bias, and recommendations for patient-designated longitudinal endpoints where all health data converges. Missing history drives false positives/negatives, and fragmentation causes direct clinical harm even without AI. This excludes data exchange standards. This matters because AI cannot make good recommendations with incomplete information.",
    "children": []
  },
  {
    "code": "5.2",
    "description": "Patient-Designated Data Convergence Endpoints. This sub-theme covers the specific proposal for patient-controlled endpoints that aggregate longitudinal health data across providers. || It includes recommendations for certified patient-designated endpoints accepting complete USCDI-aligned payloads, standardized query and return to treating systems with consent, and persistent routing pointers making patient designations discoverable",
    "level": 2,
    "parent_code": "5",
    "detailed_guidelines": "This would fix care coordination immediately while creating data conditions AI needs. This excludes general interoperability. This matters because patient-controlled aggregation could solve fragmentation.",
    "comment_count": 2,
    "direct_count": 2,
    "touch_count": 0,
    "detailedDescription": "This would fix care coordination immediately while creating data conditions AI needs. This excludes general interoperability. This matters because patient-controlled aggregation could solve fragmentation.",
    "children": []
  },
  {
    "code": "5.3",
    "description": "Event-Based Data Delivery and Verification",
    "level": 2,
    "parent_code": "5",
    "detailed_guidelines": "This sub-theme addresses the need for reliable, timely data updates rather than occasional exports or manual transfers. It includes recommendations for automatic delivery of standardized payloads as events occur, delivery verification with receipt and integrity checks, and pilot reimbursement models tied to verified delivery performance. Capability attestations are insufficient without verified delivery. This excludes static data exchange. This matters because AI needs current data to make accurate recommendations.",
    "comment_count": 3,
    "direct_count": 3,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses the need for reliable, timely data updates rather than occasional exports or manual transfers. It includes recommendations for automatic delivery of standardized payloads as events occur, delivery verification with receipt and integrity checks, and pilot reimbursement models tied to verified delivery performance. Capability attestations are insufficient without verified delivery. This excludes static data exchange. This matters because AI needs current data to make accurate recommendations.",
    "children": []
  },
  {
    "code": "5.4",
    "description": "Semantic Interoperability and Data Meaning Preservation",
    "level": 2,
    "parent_code": "5",
    "detailed_guidelines": "This sub-theme addresses the gap between data movement and data interpretation, where exchanged data may not be semantically equivalent across systems. It includes concerns that interoperability enables data movement but not data meaning, that clinical data lacks provenance, context, and labeling needed for AI, and recommendations for semantic validation layers. AI systems will attempt to infer meaning when semantic gaps are not explicitly defined, leading to errors. This excludes transport-layer interoperability. This matters because data meaning must be preserved for AI to interpret correctly.",
    "comment_count": 5,
    "direct_count": 5,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses the gap between data movement and data interpretation, where exchanged data may not be semantically equivalent across systems. It includes concerns that interoperability enables data movement but not data meaning, that clinical data lacks provenance, context, and labeling needed for AI, and recommendations for semantic validation layers. AI systems will attempt to infer meaning when semantic gaps are not explicitly defined, leading to errors. This excludes transport-layer interoperability. This matters because data meaning must be preserved for AI to interpret correctly.",
    "children": []
  },
  {
    "code": "5.5",
    "description": "Deterministic Semantic Substrates for Policy and Clinical Meaning",
    "level": 2,
    "parent_code": "5",
    "detailed_guidelines": "This sub-theme covers proposals for machine-retrievable authoritative policy and clinical meaning that AI can verify against. It includes recommendations for deterministic semantic memory making authoritative policy meaning explicit, versioned, and machine-retrievable, enabling AI outputs to be verified against published definitions without inspecting model internals. This converts AI governance from reactive control to preventive content-governance. This excludes probabilistic approaches. This matters because deterministic verification enables accountability.",
    "comment_count": 4,
    "direct_count": 4,
    "touch_count": 0,
    "detailedDescription": "This sub-theme covers proposals for machine-retrievable authoritative policy and clinical meaning that AI can verify against. It includes recommendations for deterministic semantic memory making authoritative policy meaning explicit, versioned, and machine-retrievable, enabling AI outputs to be verified against published definitions without inspecting model internals. This converts AI governance from reactive control to preventive content-governance. This excludes probabilistic approaches. This matters because deterministic verification enables accountability.",
    "children": []
  },
  {
    "code": "5.6",
    "description": "Unit Normalization and Measurement Comparability",
    "level": 2,
    "parent_code": "5",
    "detailed_guidelines": "This sub-theme addresses specific semantic challenges around measurement comparability across different instruments and contexts. It includes concerns that differences in units, measurement context, specimen, and method create problems, that some clinical values cannot be meaningfully compared without explicit transformation rules, and recommendations for distinct LOINC codes for non-comparable results. Small semantic differences compound over time in longitudinal monitoring. This excludes general semantic issues. This matters because measurement inconsistency causes AI errors.",
    "comment_count": 1,
    "direct_count": 1,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses specific semantic challenges around measurement comparability across different instruments and contexts. It includes concerns that differences in units, measurement context, specimen, and method create problems, that some clinical values cannot be meaningfully compared without explicit transformation rules, and recommendations for distinct LOINC codes for non-comparable results. Small semantic differences compound over time in longitudinal monitoring. This excludes general semantic issues. This matters because measurement inconsistency causes AI errors.",
    "children": []
  },
  {
    "code": "5.7",
    "description": "Data Provenance and Transformation Metadata",
    "level": 2,
    "parent_code": "5",
    "detailed_guidelines": "This sub-theme covers the need for data to include information about its origin, transformations, and context for proper interpretation. It includes concerns that system-specific transformations may not be visible downstream, recommendations for explicit provenance and audit logs capturing transformations and assumptions, and requirements for metadata including device ID and firmware version. Without provenance, AI may interpret hardware noise as biological signals or miss context-dependent meaning. This excludes data content. This matters because context determines data interpretation.",
    "comment_count": 5,
    "direct_count": 5,
    "touch_count": 0,
    "detailedDescription": "This sub-theme covers the need for data to include information about its origin, transformations, and context for proper interpretation. It includes concerns that system-specific transformations may not be visible downstream, recommendations for explicit provenance and audit logs capturing transformations and assumptions, and requirements for metadata including device ID and firmware version. Without provenance, AI may interpret hardware noise as biological signals or miss context-dependent meaning. This excludes data content. This matters because context determines data interpretation.",
    "children": []
  },
  {
    "code": "5.8",
    "description": "Instrument Drift and Hardware Metadata Requirements. This sub-theme addresses the specific problem of AI model degradation when data capture hardware changes or drifts over time. || It includes concerns about \"silent instrument drift\" where firmware upgrades cause undetectable diagnostic errors, recommendations for instrument-agnostic validation, and requirements that AI systems ingest and validate hardware metadata",
    "level": 2,
    "parent_code": "5",
    "detailed_guidelines": "Commenters with industrial experience warn this is a fundamental engineering failure mode not addressed in current frameworks. This excludes software drift. This matters because hardware changes can silently invalidate AI models.",
    "comment_count": 1,
    "direct_count": 1,
    "touch_count": 0,
    "detailedDescription": "Commenters with industrial experience warn this is a fundamental engineering failure mode not addressed in current frameworks. This excludes software drift. This matters because hardware changes can silently invalidate AI models.",
    "children": []
  },
  {
    "code": "5.9",
    "description": "Clinical Society Knowledge Accessibility",
    "level": 2,
    "parent_code": "5",
    "detailed_guidelines": "This sub-theme addresses the need for medical and clinical societies to make their knowledge resources accessible for AI applications through standardized interfaces. It includes concerns that societies haven't collectively adopted FHIR APIs at scale, recommendations for societies to make clinical guidelines, care plans, and treatment algorithms accessible through standardized interfaces. This knowledge could power AI applications if made accessible. This excludes EHR interoperability. This matters because clinical knowledge is essential for AI to provide evidence-based recommendations.",
    "comment_count": 2,
    "direct_count": 2,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses the need for medical and clinical societies to make their knowledge resources accessible for AI applications through standardized interfaces. It includes concerns that societies haven't collectively adopted FHIR APIs at scale, recommendations for societies to make clinical guidelines, care plans, and treatment algorithms accessible through standardized interfaces. This knowledge could power AI applications if made accessible. This excludes EHR interoperability. This matters because clinical knowledge is essential for AI to provide evidence-based recommendations.",
    "children": []
  },
  {
    "code": "6",
    "description": "Workforce Impact and Clinical Workflow Integration",
    "level": 1,
    "parent_code": null,
    "detailed_guidelines": "This theme encompasses all concerns about how AI affects healthcare workers, including workflow integration, burden reduction, training needs, and workforce stability. It includes concerns about AI adding burden rather than reducing it, alert fatigue, cognitive load, training requirements, and workforce capacity constraints. This theme covers the human factors determining whether AI succeeds in practice. It does NOT include patient-facing concerns, technical integration, or governance structures. This matters because workflow misfit is cited as the dominant barrier to adoption, and AI failing to reduce burden will fail regardless of accuracy.",
    "comment_count": 20,
    "direct_count": 20,
    "touch_count": 0,
    "detailedDescription": "This theme encompasses all concerns about how AI affects healthcare workers, including workflow integration, burden reduction, training needs, and workforce stability. It includes concerns about AI adding burden rather than reducing it, alert fatigue, cognitive load, training requirements, and workforce capacity constraints. This theme covers the human factors determining whether AI succeeds in practice. It does NOT include patient-facing concerns, technical integration, or governance structures. This matters because workflow misfit is cited as the dominant barrier to adoption, and AI failing to reduce burden will fail regardless of accuracy.",
    "children": [
      "6.1",
      "6.2",
      "6.3",
      "6.4",
      "6.5",
      "6.6",
      "6.7",
      "6.8"
    ]
  },
  {
    "code": "6.1",
    "description": "Workflow Integration and Burden Reduction Requirements. This sub-theme addresses whether AI tools fit into clinical workflows and reduce rather than add burden to already-stretched clinicians. || It includes concerns about AI adding documentation burden, disrupting cognitive flow, creating ambiguous responsibility, and requiring extra clicks. It covers recommendations for prioritizing AI that saves clinician time and reduces documentation burden",
    "level": 2,
    "parent_code": "6",
    "detailed_guidelines": "Exhausted care teams cannot absorb poorly designed tools. This excludes training and workforce stability. This matters because workflow fit determines whether AI delivers value or adds burden.",
    "comment_count": 15,
    "direct_count": 15,
    "touch_count": 0,
    "detailedDescription": "Exhausted care teams cannot absorb poorly designed tools. This excludes training and workforce stability. This matters because workflow fit determines whether AI delivers value or adds burden.",
    "children": []
  },
  {
    "code": "6.2",
    "description": "Alert Fatigue and Non-Actionable Notifications. This sub-theme specifically addresses the problem of excessive or non-actionable AI alerts that lead to clinician desensitization. || It includes concerns about hundreds of false alarms in SNF settings, alert fatigue as a safety risk leading to override of valid recommendations, and the need for clinical verification to filter signal from noise",
    "level": 2,
    "parent_code": "6",
    "detailed_guidelines": "Pure software solutions generating excessive alerts create safety concerns rather than solving them. This excludes other workflow issues. This matters because alert fatigue undermines AI safety benefits.",
    "comment_count": 6,
    "direct_count": 6,
    "touch_count": 0,
    "detailedDescription": "Pure software solutions generating excessive alerts create safety concerns rather than solving them. This excludes other workflow issues. This matters because alert fatigue undermines AI safety benefits.",
    "children": []
  },
  {
    "code": "6.3",
    "description": "Cognitive Load and Decision Support Design",
    "level": 2,
    "parent_code": "6",
    "detailed_guidelines": "This sub-theme covers how AI affects clinician cognitive burden and decision-making processes in time-pressured environments. It includes recommendations for cognitive load mapping as a primary safety endpoint, concerns about AI outputs requiring clinicians to leave established workflows, and the need for AI to reduce time-to-decision rather than increase burden. AI not well integrated imposes additional cognitive and operational burden. This excludes alert-specific issues. This matters because cognitive overload leads to errors.",
    "comment_count": 5,
    "direct_count": 5,
    "touch_count": 0,
    "detailedDescription": "This sub-theme covers how AI affects clinician cognitive burden and decision-making processes in time-pressured environments. It includes recommendations for cognitive load mapping as a primary safety endpoint, concerns about AI outputs requiring clinicians to leave established workflows, and the need for AI to reduce time-to-decision rather than increase burden. AI not well integrated imposes additional cognitive and operational burden. This excludes alert-specific issues. This matters because cognitive overload leads to errors.",
    "children": []
  },
  {
    "code": "6.4",
    "description": "Clinician Training and AI Literacy Requirements. This sub-theme addresses the need for healthcare worker education on AI tools, their capabilities, and their limitations. || It includes recommendations for continuing education specific to AI-enabled technologies, AI literacy programs tailored to different roles (CNA, nursing, administration), training to recognize limitations like hallucinations and bias, and competency assessment before AI use",
    "level": 2,
    "parent_code": "6",
    "detailed_guidelines": "Clinicians need to understand AI capabilities and limitations to use tools appropriately. This excludes workflow design. This matters because untrained users may misuse or over-trust AI.",
    "comment_count": 8,
    "direct_count": 8,
    "touch_count": 0,
    "detailedDescription": "Clinicians need to understand AI capabilities and limitations to use tools appropriately. This excludes workflow design. This matters because untrained users may misuse or over-trust AI.",
    "children": []
  },
  {
    "code": "6.5",
    "description": "Workforce Stability and Capacity Preservation",
    "level": 2,
    "parent_code": "6",
    "detailed_guidelines": "This sub-theme covers AI's potential role in addressing workforce shortages and preventing capacity degradation in healthcare systems. It includes recommendations for AI as \"Deflationary Infrastructure\" preserving staffed clinical capacity, concerns about projected physician deficits and massive financial impact of workforce instability, and AI predicting attrition for proactive intervention. Healthcare is already operating beyond safe margins with projected shortages worsening. This excludes individual workflow issues. This matters because AI could help address critical workforce shortages.",
    "comment_count": 6,
    "direct_count": 6,
    "touch_count": 0,
    "detailedDescription": "This sub-theme covers AI's potential role in addressing workforce shortages and preventing capacity degradation in healthcare systems. It includes recommendations for AI as \"Deflationary Infrastructure\" preserving staffed clinical capacity, concerns about projected physician deficits and massive financial impact of workforce instability, and AI predicting attrition for proactive intervention. Healthcare is already operating beyond safe margins with projected shortages worsening. This excludes individual workflow issues. This matters because AI could help address critical workforce shortages.",
    "children": []
  },
  {
    "code": "6.6",
    "description": "Workforce Readiness Telemetry and Strain Monitoring",
    "level": 2,
    "parent_code": "6",
    "detailed_guidelines": "This sub-theme covers the specific proposal for standardized metrics tracking operational strain and staffing fragility at the system level. It includes recommendations for anonymized, aggregate indicators of operational strain, cognitive load, and staffing fragility designed for system-level planning rather than individual evaluation. FTEs are lagging indicators failing to capture burnout, moral injury, or disengagement. This excludes clinical data standards. This matters because workforce strain must be measured to be addressed.",
    "comment_count": 1,
    "direct_count": 1,
    "touch_count": 0,
    "detailedDescription": "This sub-theme covers the specific proposal for standardized metrics tracking operational strain and staffing fragility at the system level. It includes recommendations for anonymized, aggregate indicators of operational strain, cognitive load, and staffing fragility designed for system-level planning rather than individual evaluation. FTEs are lagging indicators failing to capture burnout, moral injury, or disengagement. This excludes clinical data standards. This matters because workforce strain must be measured to be addressed.",
    "children": []
  },
  {
    "code": "6.7",
    "description": "Workforce Data Interoperability and System Integration",
    "level": 2,
    "parent_code": "6",
    "detailed_guidelines": "This sub-theme addresses the lack of data standards and integration for workforce management systems across healthcare organizations. It includes concerns about fragmented technology stacks across scheduling, time/attendance, credentialing, and payroll, absence of standard APIs delaying deployment, and recommendations to extend interoperability mandates to workforce management systems. Each integration requires separate security reviews, BAAs, and technical work. This excludes clinical data interoperability. This matters because workforce AI requires integrated data.",
    "comment_count": 1,
    "direct_count": 1,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses the lack of data standards and integration for workforce management systems across healthcare organizations. It includes concerns about fragmented technology stacks across scheduling, time/attendance, credentialing, and payroll, absence of standard APIs delaying deployment, and recommendations to extend interoperability mandates to workforce management systems. Each integration requires separate security reviews, BAAs, and technical work. This excludes clinical data interoperability. This matters because workforce AI requires integrated data.",
    "children": []
  },
  {
    "code": "6.8",
    "description": "Clinician Trust and Acceptance Factors. This sub-theme covers factors affecting whether clinicians trust and adopt AI tools in their practice. || It includes concerns about \"black box\" nature of proprietary algorithms preventing trust, clinician hesitation to rely on outputs not tied to observable signals, and the need for explainability and human override capability",
    "level": 2,
    "parent_code": "6",
    "detailed_guidelines": "Clinician acceptance is essential for AI to deliver value. This excludes patient trust. This matters because distrust leads to non-adoption or inappropriate use.",
    "comment_count": 11,
    "direct_count": 11,
    "touch_count": 0,
    "detailedDescription": "Clinician acceptance is essential for AI to deliver value. This excludes patient trust. This matters because distrust leads to non-adoption or inappropriate use.",
    "children": []
  },
  {
    "code": "7",
    "description": "Patient Trust, Transparency, and Engagement. This theme encompasses all concerns about patient-facing aspects of clinical AI, including transparency requirements, patient involvement in AI development, and patient concerns about AI use in their care. It includes concerns about opacity, bias, loss of human oversight, data privacy, and recommendations for patient engagement throughout the AI lifecycle",
    "level": 1,
    "parent_code": null,
    "detailed_guidelines": "This theme encompasses all concerns about patient-facing aspects of clinical AI, including transparency requirements, patient involvement in AI development, and patient concerns about AI use in their care. It includes concerns about opacity, bias, loss of human oversight, data privacy, and recommendations for patient engagement throughout the AI lifecycle. This theme covers the patient perspective on AI in their care. It does NOT include clinician concerns, technical transparency requirements, or data infrastructure. This matters because patient trust is essential for durable AI adoption and patients are experts on their conditions whose input must inform AI development.",
    "comment_count": 16,
    "direct_count": 16,
    "touch_count": 0,
    "detailedDescription": "This theme encompasses all concerns about patient-facing aspects of clinical AI, including transparency requirements, patient involvement in AI development, and patient concerns about AI use in their care. It includes concerns about opacity, bias, loss of human oversight, data privacy, and recommendations for patient engagement throughout the AI lifecycle. This theme covers the patient perspective on AI in their care. It does NOT include clinician concerns, technical transparency requirements, or data infrastructure. This matters because patient trust is essential for durable AI adoption and patients are experts on their conditions whose input must inform AI development.",
    "children": [
      "7.1",
      "7.2",
      "7.3",
      "7.4",
      "7.5"
    ]
  },
  {
    "code": "7.1",
    "description": "Transparency and Disclosure Requirements for AI Use",
    "level": 2,
    "parent_code": "7",
    "detailed_guidelines": "This sub-theme addresses what patients should be told about AI use in their care and how that information should be communicated. It includes recommendations for clear disclosure when AI is involved and for what purpose, \"AI labels\" similar to nutrition labels explaining capabilities and limitations, and requirements that clinicians be transparent about when and how AI assists care. Patients need to understand why AI is being used, what it can and cannot do, and how their data is handled. This excludes patient involvement in development. This matters because informed patients can make better decisions about their care.",
    "comment_count": 10,
    "direct_count": 10,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses what patients should be told about AI use in their care and how that information should be communicated. It includes recommendations for clear disclosure when AI is involved and for what purpose, \"AI labels\" similar to nutrition labels explaining capabilities and limitations, and requirements that clinicians be transparent about when and how AI assists care. Patients need to understand why AI is being used, what it can and cannot do, and how their data is handled. This excludes patient involvement in development. This matters because informed patients can make better decisions about their care.",
    "children": []
  },
  {
    "code": "7.2",
    "description": "Patient Involvement in AI Development and Governance",
    "level": 2,
    "parent_code": "7",
    "detailed_guidelines": "This sub-theme covers recommendations for engaging patients throughout the AI lifecycle from design through post-market evaluation. It includes recommendations for patient-focused product development meetings, capturing lived experiences of diverse individuals, integrating patient and caregiver input from development through post-market evaluation, and advisory panel review processes with individuals with lived experience. Technologies developed without patient input may not reflect patient needs. This excludes disclosure requirements. This matters because patient involvement improves AI design and builds trust.",
    "comment_count": 3,
    "direct_count": 3,
    "touch_count": 0,
    "detailedDescription": "This sub-theme covers recommendations for engaging patients throughout the AI lifecycle from design through post-market evaluation. It includes recommendations for patient-focused product development meetings, capturing lived experiences of diverse individuals, integrating patient and caregiver input from development through post-market evaluation, and advisory panel review processes with individuals with lived experience. Technologies developed without patient input may not reflect patient needs. This excludes disclosure requirements. This matters because patient involvement improves AI design and builds trust.",
    "children": []
  },
  {
    "code": "7.3",
    "description": "Patient Data Ownership and Control Rights. This sub-theme addresses patient rights over their health data when AI tools are deployed, including who benefits from derived insights. || It includes recommendations that patients retain ownership of personal health data, clear protections for how data are collected, used, shared, and stored, patient-mediated data aggregation, and concerns about who benefits from derived learning",
    "level": 2,
    "parent_code": "7",
    "detailed_guidelines": "Patients want clarity not just on who sees their data but who benefits from it. This excludes general privacy. This matters because data rights affect patient willingness to participate.",
    "comment_count": 7,
    "direct_count": 7,
    "touch_count": 0,
    "detailedDescription": "Patients want clarity not just on who sees their data but who benefits from it. This excludes general privacy. This matters because data rights affect patient willingness to participate.",
    "children": []
  },
  {
    "code": "7.4",
    "description": "Patient Concerns About AI in Healthcare",
    "level": 2,
    "parent_code": "7",
    "detailed_guidelines": "This sub-theme captures the specific worries patients have about AI affecting their healthcare experience and outcomes. It includes concerns about privacy and lack of transparency, biased or incorrect outputs, difficulty correcting errors, losing the human element in healthcare, discriminatory care delivery, and ambient listening technology. Understanding patient concerns is essential for designing AI that patients will accept and trust. This excludes provider concerns. This matters because unaddressed concerns undermine adoption.",
    "comment_count": 10,
    "direct_count": 10,
    "touch_count": 0,
    "detailedDescription": "This sub-theme captures the specific worries patients have about AI affecting their healthcare experience and outcomes. It includes concerns about privacy and lack of transparency, biased or incorrect outputs, difficulty correcting errors, losing the human element in healthcare, discriminatory care delivery, and ambient listening technology. Understanding patient concerns is essential for designing AI that patients will accept and trust. This excludes provider concerns. This matters because unaddressed concerns undermine adoption.",
    "children": []
  },
  {
    "code": "7.5",
    "description": "Empathy and Human Connection in AI-Enabled Care",
    "level": 2,
    "parent_code": "7",
    "detailed_guidelines": "This sub-theme addresses concerns about AI affecting the human relationship that is central to healthcare and healing. It includes concerns that AI can simulate but cannot genuinely experience empathy or moral responsibility, that older patients may find AI impersonal or unsettling, and that patients want empathy, shared experience, and understanding from physicians. The human element in healthcare cannot be replicated by AI. This excludes technical concerns. This matters because healthcare is fundamentally relational.",
    "comment_count": 5,
    "direct_count": 5,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses concerns about AI affecting the human relationship that is central to healthcare and healing. It includes concerns that AI can simulate but cannot genuinely experience empathy or moral responsibility, that older patients may find AI impersonal or unsettling, and that patients want empathy, shared experience, and understanding from physicians. The human element in healthcare cannot be replicated by AI. This excludes technical concerns. This matters because healthcare is fundamentally relational.",
    "children": []
  },
  {
    "code": "8",
    "description": "Equity, Bias, and Disparate Impact. This theme encompasses all concerns about how AI may affect different populations differently, including algorithmic bias, health equity risks, and impacts on underserved communities and settings. It includes concerns about non-representative training data, disparities emerging after implementation, disproportionate burden on rural and safety-net providers, and recommendations for equity safeguards",
    "level": 1,
    "parent_code": null,
    "detailed_guidelines": "This theme encompasses all concerns about how AI may affect different populations differently, including algorithmic bias, health equity risks, and impacts on underserved communities and settings. It includes concerns about non-representative training data, disparities emerging after implementation, disproportionate burden on rural and safety-net providers, and recommendations for equity safeguards. This theme covers fairness and distributional effects of AI. It does NOT include general data quality, individual patient concerns, or workforce equity. This matters because without deliberate measures, AI risks widening existing healthcare disparities rather than closing them.",
    "comment_count": 19,
    "direct_count": 19,
    "touch_count": 0,
    "detailedDescription": "This theme encompasses all concerns about how AI may affect different populations differently, including algorithmic bias, health equity risks, and impacts on underserved communities and settings. It includes concerns about non-representative training data, disparities emerging after implementation, disproportionate burden on rural and safety-net providers, and recommendations for equity safeguards. This theme covers fairness and distributional effects of AI. It does NOT include general data quality, individual patient concerns, or workforce equity. This matters because without deliberate measures, AI risks widening existing healthcare disparities rather than closing them.",
    "children": [
      "8.1",
      "8.2",
      "8.3",
      "8.4",
      "8.5",
      "8.6"
    ]
  },
  {
    "code": "8.1",
    "description": "Algorithmic Bias and Training Data Representativeness. This sub-theme addresses bias embedded in AI systems through non-representative or historically biased training data. || It includes concerns that algorithms amplify errors and preexisting biases in source data, that many tools are not validated against sufficiently diverse patient populations, and recommendations for using representative, high-quality data reflecting diverse populations. Examples include algorithms that under-identified Black patients for high-risk care management by using spending as proxy for need",
    "level": 2,
    "parent_code": "8",
    "detailed_guidelines": "This excludes implementation-related disparities. This matters because biased AI perpetuates and amplifies existing inequities.",
    "comment_count": 11,
    "direct_count": 11,
    "touch_count": 0,
    "detailedDescription": "This excludes implementation-related disparities. This matters because biased AI perpetuates and amplifies existing inequities.",
    "children": []
  },
  {
    "code": "8.2",
    "description": "Implementation and Adoption Disparities Across Organizations. This sub-theme covers how AI benefits may be unequally distributed based on organizational resources, geography, and infrastructure. || It includes concerns about disproportionate burden on smaller organizations and rural communities, risk of replicating HITECH Act disparities, and recommendations for targeted support for under-resourced institutions. Well-resourced institutions can adopt advanced AI more easily, creating unequal benefits and skewed data representation",
    "level": 2,
    "parent_code": "8",
    "detailed_guidelines": "This excludes algorithmic bias. This matters because adoption disparities create two-tiered healthcare.",
    "comment_count": 6,
    "direct_count": 6,
    "touch_count": 0,
    "detailedDescription": "This excludes algorithmic bias. This matters because adoption disparities create two-tiered healthcare.",
    "children": []
  },
  {
    "code": "8.3",
    "description": "Rural and Safety-Net Provider Challenges. This sub-theme specifically addresses barriers facing rural and safety-net healthcare organizations in adopting and benefiting from AI. || It includes concerns about infrastructure gaps, limited capital budgets, lack of purchasing power, and recommendations for targeted funding, tax credits, and group purchasing",
    "level": 2,
    "parent_code": "8",
    "detailed_guidelines": "These settings face the greatest risk of being left behind or harmed by AI adoption. This excludes urban academic medical centers. This matters because underserved populations depend on these providers.",
    "comment_count": 6,
    "direct_count": 6,
    "touch_count": 0,
    "detailedDescription": "These settings face the greatest risk of being left behind or harmed by AI adoption. This excludes urban academic medical centers. This matters because underserved populations depend on these providers.",
    "children": []
  },
  {
    "code": "8.4",
    "description": "Small Practice and Solo Practitioner Impact",
    "level": 2,
    "parent_code": "8",
    "detailed_guidelines": "This sub-theme addresses how AI adoption requirements and opportunities affect small medical practices and solo practitioners. It includes concerns about lack of dedicated compliance personnel, inability to afford new systems, risk of closure due to administrative overload, and disproportionate impact of governance requirements. Small practices serve vulnerable populations who may lose access to care if practices cannot adopt AI or are burdened by requirements. This excludes large health systems. This matters because small practices provide essential access.",
    "comment_count": 1,
    "direct_count": 1,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses how AI adoption requirements and opportunities affect small medical practices and solo practitioners. It includes concerns about lack of dedicated compliance personnel, inability to afford new systems, risk of closure due to administrative overload, and disproportionate impact of governance requirements. Small practices serve vulnerable populations who may lose access to care if practices cannot adopt AI or are burdened by requirements. This excludes large health systems. This matters because small practices provide essential access.",
    "children": []
  },
  {
    "code": "8.5",
    "description": "Post-Implementation Equity Monitoring Requirements. This sub-theme covers the need to track equity impacts after AI deployment rather than assuming equity can be ensured through design alone. || It includes recommendations for equity-specific monitoring requirements, routine audits to detect and mitigate bias, measuring equity continuously, and concerns that disparities may not become apparent for months. Equity disparities emerge after implementation through uneven adoption, differential trust, and context-specific failures. This excludes pre-deployment bias assessment",
    "level": 2,
    "parent_code": "8",
    "detailed_guidelines": "This matters because ongoing monitoring catches problems design misses",
    "comment_count": 9,
    "direct_count": 9,
    "touch_count": 0,
    "detailedDescription": "This matters because ongoing monitoring catches problems design misses",
    "children": []
  },
  {
    "code": "8.6",
    "description": "Equity in AI-Driven Administrative and Coverage Decisions",
    "level": 2,
    "parent_code": "8",
    "detailed_guidelines": "This sub-theme addresses how AI in prior authorization and coverage decisions may perpetuate or amplify existing inequities. It includes concerns that algorithms trained on historical spending patterns embed past inequities, that lower spending is misinterpreted as lower need affecting low-income, rural, and minority patients, and that cost-effectiveness measures like QALYs devalue people with disabilities. Commenters document patient harm from AI-driven denials that disproportionately affect vulnerable populations. This excludes clinical AI bias. This matters because administrative AI affects care access.",
    "comment_count": 2,
    "direct_count": 2,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses how AI in prior authorization and coverage decisions may perpetuate or amplify existing inequities. It includes concerns that algorithms trained on historical spending patterns embed past inequities, that lower spending is misinterpreted as lower need affecting low-income, rural, and minority patients, and that cost-effectiveness measures like QALYs devalue people with disabilities. Commenters document patient harm from AI-driven denials that disproportionately affect vulnerable populations. This excludes clinical AI bias. This matters because administrative AI affects care access.",
    "children": []
  },
  {
    "code": "9",
    "description": "Post-Deployment Monitoring and Performance Surveillance",
    "level": 1,
    "parent_code": null,
    "detailed_guidelines": "This theme encompasses all concerns about ongoing oversight of AI systems after they enter clinical use, including performance monitoring, drift detection, and safety surveillance. It includes concerns that pre-deployment validation is insufficient, that performance degrades over time, and recommendations for continuous monitoring frameworks. This theme covers the operational phase of AI deployment. It does NOT include pre-deployment evaluation, governance structures, or technical auditability requirements. This matters because the majority of AI risk emerges downstream where no systematic oversight currently exists.",
    "comment_count": 21,
    "direct_count": 21,
    "touch_count": 0,
    "detailedDescription": "This theme encompasses all concerns about ongoing oversight of AI systems after they enter clinical use, including performance monitoring, drift detection, and safety surveillance. It includes concerns that pre-deployment validation is insufficient, that performance degrades over time, and recommendations for continuous monitoring frameworks. This theme covers the operational phase of AI deployment. It does NOT include pre-deployment evaluation, governance structures, or technical auditability requirements. This matters because the majority of AI risk emerges downstream where no systematic oversight currently exists.",
    "children": [
      "9.1",
      "9.2",
      "9.3"
    ]
  },
  {
    "code": "9.1",
    "description": "Performance Drift Detection and Management",
    "level": 2,
    "parent_code": "9",
    "detailed_guidelines": "This sub-theme addresses the problem of AI performance degrading over time due to changing clinical conditions, coding practices, or population shifts. It includes concerns about performance drift going undetected without routine monitoring, changes in EHR coding practices degrading model performance, and recommendations for drift detection mechanisms and rollback criteria. Disparities can widen quietly over extended periods without detection. This excludes initial deployment issues. This matters because AI that worked at deployment may fail later.",
    "comment_count": 17,
    "direct_count": 17,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses the problem of AI performance degrading over time due to changing clinical conditions, coding practices, or population shifts. It includes concerns about performance drift going undetected without routine monitoring, changes in EHR coding practices degrading model performance, and recommendations for drift detection mechanisms and rollback criteria. Disparities can widen quietly over extended periods without detection. This excludes initial deployment issues. This matters because AI that worked at deployment may fail later.",
    "children": []
  },
  {
    "code": "9.2",
    "description": "Post-Deployment Algorithmic Surveillance Framework",
    "level": 2,
    "parent_code": "9",
    "detailed_guidelines": "This sub-theme covers comprehensive proposals for population-level AI monitoring after deployment using systematic surveillance approaches. It includes the PDAS framework with five core elements: exposure characterization, continuous population-level performance monitoring, outcome-linked evaluation, structured reporting of algorithmic harms, and predefined regulatory response pathways. AI is a population-level intervention requiring the same rigor applied to other such interventions. This excludes individual tool monitoring. This matters because systematic surveillance catches population-level harms.",
    "comment_count": 5,
    "direct_count": 5,
    "touch_count": 0,
    "detailedDescription": "This sub-theme covers comprehensive proposals for population-level AI monitoring after deployment using systematic surveillance approaches. It includes the PDAS framework with five core elements: exposure characterization, continuous population-level performance monitoring, outcome-linked evaluation, structured reporting of algorithmic harms, and predefined regulatory response pathways. AI is a population-level intervention requiring the same rigor applied to other such interventions. This excludes individual tool monitoring. This matters because systematic surveillance catches population-level harms.",
    "children": []
  },
  {
    "code": "9.3",
    "description": "Outcome-Linked Evaluation and Clinical Impact Assessment",
    "level": 2,
    "parent_code": "9",
    "detailed_guidelines": "This sub-theme addresses the need to assess whether AI use improves actual clinical outcomes rather than relying on accuracy metrics alone. It includes recommendations for linking AI performance to patient outcomes, concerns that accuracy metrics don't capture clinical value, and the need for real-world evidence of benefit. AI that is technically accurate may not improve care if it doesn't change clinical decisions appropriately. This excludes process metrics. This matters because outcomes are what ultimately matter for patients.",
    "comment_count": 11,
    "direct_count": 11,
    "touch_count": 0,
    "detailedDescription": "This sub-theme addresses the need to assess whether AI use improves actual clinical outcomes rather than relying on accuracy metrics alone. It includes recommendations for linking AI performance to patient outcomes, concerns that accuracy metrics don't capture clinical value, and the need for real-world evidence of benefit. AI that is technically accurate may not improve care if it doesn't change clinical decisions appropriately. This excludes process metrics. This matters because outcomes are what ultimately matter for patients.",
    "children": []
  }
]